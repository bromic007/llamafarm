# LlamaFarm - Complete Documentation

> Edge AI platform for running LLMs, RAG, classifiers, and document processing locally. No cloud required. Your data never leaves your device.

This file contains the complete LlamaFarm documentation for AI consumption.

---

# Welcome to LlamaFarm

**LlamaFarm brings enterprise AI capabilities to everyone.** Run powerful language models, document processing, and intelligent retrievalâ€”all locally on your hardware. No cloud required. No data leaves your machine.

**INFO: Found a bug or have a feature request?**
[Submit an issue on GitHub â†’](https://github.com/llama-farm/llamafarm/issues)

## Why LlamaFarm?

### ğŸ”’ Edge AI for Everyone

Run sophisticated AI workloads on your own hardware:

- **Complete Privacy** â€” Your documents, queries, and data never leave your device
- **No API Costs** â€” Use open-source models without per-token fees
- **Offline Capable** â€” Works without internet once models are downloaded
- **Hardware Optimized** â€” Automatic GPU/NPU acceleration on Apple Silicon, NVIDIA, and AMD

### ğŸ§  Production-Ready AI Stack

LlamaFarm isn't just a wrapperâ€”it's a complete AI development platform:

| Capability | What It Does |
|-----------|--------------|
| **RAG (Retrieval-Augmented Generation)** | Ingest PDFs, docs, CSVs and query them with AI. Your documents become searchable knowledge. |
| **Multi-Model Runtime** | Switch between Ollama, OpenAI, vLLM, or local GGUF models in one config file. |
| **Custom Classifiers** | Train text classifiers with 8-16 examples using SetFit. No ML expertise required. |
| **Anomaly Detection** | Detect outliers in logs, metrics, or transactions with one API call. |
| **OCR & Document Extraction** | Extract text and structured data from images and PDFs. |
| **Named Entity Recognition** | Find people, organizations, and locations in your text. |
| **Agentic Tools (MCP)** | Give AI models access to filesystems, databases, and APIs. |

### âš¡ Developer Experience

- **Config-Driven** â€” Define your entire AI stack in `llamafarm.yaml`
- **CLI + Web UI** â€” Use the `lf` command line or the Designer visual interface
- **REST API** â€” OpenAI-compatible endpoints for easy integration
- **Extensible** â€” Add custom parsers, embedders, and model providers

---

## Get Started in 60 Seconds

### Option 1: Desktop App (Easiest)

Download the all-in-one desktop application:

The desktop app bundles everything: server, Universal Runtime, and the Designer web UI.

### Option 2: CLI Installation

Install the `lf` command-line tool:

**macOS / Linux:**
```bash
curl -fsSL https://raw.githubusercontent.com/llama-farm/llamafarm/main/install.sh | bash
```

**Windows (PowerShell):**
```powershell
irm https://raw.githubusercontent.com/llama-farm/llamafarm/main/install.ps1 | iex
```

**Or download directly:**
- [Latest Release](https://github.com/llama-farm/llamafarm/releases/latest) â€” Download `lf` binary for your platform

Verify installation:
```bash
lf --help
```

---

## ğŸ“º See It In Action

**Quick Overview (90 seconds):** https://youtu.be/W7MHGyN0MdQ

**Complete Walkthrough (7 minutes):** https://youtu.be/HNnZ4iaOSJ4

---

## What Can You Build?

### Document Q&A
Upload your company's documents and ask questions in natural language:
```bash
lf datasets upload knowledge-base ./contracts/*.pdf
lf datasets process knowledge-base
lf chat "What are our standard payment terms?"
```

### Custom Intent Classification
Train a classifier to route support tickets:
```python
# Train with just 8 examples per category
POST /v1/ml/classifier/fit
{
  "model": "ticket-router",
  "training_data": [
    {"text": "I can't log in", "label": "auth"},
    {"text": "Charge me twice", "label": "billing"},
    ...
  ]
}
```

### Real-Time Anomaly Detection
Monitor API logs for suspicious activity:
```python
# Train on normal traffic
POST /v1/ml/anomaly/fit
{"model": "api-monitor", "data": [...normal_requests...]}

# Detect anomalies in real-time
POST /v1/ml/anomaly/detect
{"model": "api-monitor", "data": [...new_requests...]}
```

### Document Processing Pipeline
Extract structured data from invoices and forms:
```bash
curl -X POST http://localhost:8000/v1/vision/ocr \
  -F "file=@invoice.pdf" \
  -F "model=surya"
```

---

## Choose Your Path

| Get Started | Go Deeper | Build Your Own |
|-------------|-----------|----------------|
| [Quickstart](./quickstart/index.md) â€” Install, init, chat, ingest your first dataset | [Core Concepts](./concepts/index.md) â€” Architecture, sessions, and components | [Extending LlamaFarm](./extending/index.md) â€” Add runtimes, stores, parsers |
| [Designer Web UI](./designer/index.md) â€” Visual interface for project management | [Configuration Guide](./configuration/index.md) â€” Schema-driven project settings | [RAG Guide](./rag/index.md) â€” Strategies, processing pipelines |
| [CLI Reference](./cli/index.md) â€” Command matrix and examples | [Models & Runtime](./models/index.md) â€” Configure AI models and providers | [API Reference](./api/index.md) â€” Full REST API documentation |

---

## Philosophy

- **Local-first, cloud-aware** â€” Everything works offline, yet you can point at remote runtimes when needed
- **Configuration over code** â€” Projects are reproducible because behavior lives in `llamafarm.yaml`
- **Composable modules** â€” RAG, prompts, and runtime selection work independently but integrate cleanly
- **Edge for everyone** â€” Enterprise AI capabilities without enterprise infrastructure
- **Open for extension** â€” Add custom providers, stores, and utilities

---

## Advanced: MCP (Model Context Protocol)

LlamaFarm supports **MCP** for giving AI models access to external tools like filesystems, databases, and APIs.

```yaml
mcp:
  servers:
    - name: filesystem
      transport: stdio
      command: npx
      args: ['-y', '@modelcontextprotocol/server-filesystem', '/path/to/dir']

runtime:
  models:
    - name: assistant
      provider: openai
      model: gpt-4
      mcp_servers: [filesystem]
```

[**Learn more about MCP â†’**](./mcp/index.md)

---

Ready to build? Start with the [Quickstart](./quickstart/index.md).

---

# Quickstart

Get LlamaFarm installed, ingest a dataset, and run your first RAG-powered chat in minutes.

## 1. Prerequisites

- [Ollama](https://ollama.com/download) â€” Local model runtime (or any OpenAI-compatible provider)

## 2. Install LlamaFarm

### Option A: Desktop App (Easiest)

Download the all-in-one desktop application:

| Platform | Download |
|----------|----------|
| **Mac (Universal)** | [â¬‡ï¸ Download](https://github.com/llama-farm/llamafarm/releases/latest/download/LlamaFarm-desktop-app-mac-universal.dmg) |
| **Windows (x86_64)** | [â¬‡ï¸ Download](https://github.com/llama-farm/llamafarm/releases/latest/download/LlamaFarm-desktop-app-windows.exe) |
| **Linux (x86_64)** | [â¬‡ï¸ Download](https://github.com/llama-farm/llamafarm/releases/latest/download/LlamaFarm-desktop-app-linux-x86_64.AppImage) |
| **Linux (arm64)** | [â¬‡ï¸ Download](https://github.com/llama-farm/llamafarm/releases/latest/download/LlamaFarm-desktop-app-linux-arm64.AppImage) |

The desktop app bundles everything you needâ€”no additional installation required.

### Option B: CLI Installation

**macOS / Linux:**
```bash
curl -fsSL https://raw.githubusercontent.com/llama-farm/llamafarm/main/install.sh | bash
```

**Windows (PowerShell):**
```powershell
irm https://raw.githubusercontent.com/llama-farm/llamafarm/main/install.ps1 | iex
```

**Manual Download:**

Download the `lf` binary directly from the [releases page](https://github.com/llama-farm/llamafarm/releases/latest):

| Platform | Binary |
|----------|--------|
| macOS (Apple Silicon) | `lf-darwin-arm64` |
| macOS (Intel) | `lf-darwin-amd64` |
| Linux (x86_64) | `lf-linux-amd64` |
| Linux (arm64) | `lf-linux-arm64` |
| Windows (x86_64) | `lf-windows-amd64.exe` |

After downloading, make it executable and add to your PATH:
```bash
chmod +x lf-darwin-arm64
sudo mv lf-darwin-arm64 /usr/local/bin/lf
```

Verify installation:
```bash
lf --help
```

## 3. Configure Your Runtime (only if you plan to use Ollama)

For best RAG results with longer documents, increase the Ollama context window:

1. Open the Ollama app
2. Navigate to **Settings â†’ Advanced**
3. Adjust the context window size (recommended: 32K+ for documents)

Pull a model if you haven't already:
```bash
ollama pull llama3.2
ollama pull nomic-embed-text  # For embeddings
```

## 4. Create a Project

```bash
lf init my-project
cd my-project
```

This creates `llamafarm.yaml` with default runtime, prompts, and RAG configuration.

## 5. Start LlamaFarm

```bash
lf start
```

This command:
- Starts the API server and Universal Runtime natively
- Opens the interactive chat TUI
- Launches the Designer web UI at `http://localhost:8000`

Hit `Ctrl+C` to exit the chat UI when you're done.

**TIP: Use the Designer Web UI**
Prefer a visual interface? Open `http://localhost:8000` in your browser to access the Designerâ€”manage projects, upload datasets, configure models, and test prompts without touching the command line.

See the [Designer documentation](../designer/index.md) for details.

### Running Services Manually

For development, you can run services individually:

```bash
git clone https://github.com/llama-farm/llamafarm.git
cd llamafarm

npm install -g nx
nx init --useDotNxInstallation --interactive=false

# Start both services
nx dev

# Or in separate terminals:
nx start server           # Terminal 1
nx start universal-runtime # Terminal 2
```

## 6. Chat with Your Project

```bash
# Interactive chat (opens TUI)
lf chat

# One-off message
lf chat "What can you do?"
```

Useful options:
- `--no-rag` â€” Bypass retrieval, hit the model directly
- `--database`, `--retrieval-strategy` â€” Override RAG behavior
- `--curl` â€” Print the equivalent curl command

## 7. Create and Populate a Dataset

```bash
# Create a dataset
lf datasets create -s pdf_ingest -b main_db research-notes

# Upload documents (supports globs/directories)
lf datasets upload research-notes ./examples/fda_rag/files/*.pdf
```

## 8. Process Documents

```bash
lf datasets process research-notes
```

This sends documents through the RAG pipelineâ€”parsing, chunking, embedding, and indexing.

For large PDFs, processing may take a few minutes. The CLI shows progress indicators.

## 9. Query with RAG

```bash
lf rag query --database main_db "What are the key findings?"
```

Useful flags:
- `--top-k 10` â€” Number of results
- `--filter "file_type:pdf"` â€” Metadata filtering
- `--include-metadata` â€” Show document sources
- `--include-score` â€” Show relevance scores

## 10. Next Steps

- [Designer Web UI](../designer/index.md) â€” Visual interface for managing projects
- [Configuration Guide](../configuration/index.md) â€” Deep dive into `llamafarm.yaml`
- [RAG Guide](../rag/index.md) â€” Strategies, parsers, and retrieval
- [ML Models](../models/specialized-ml.md) â€” Classifiers, OCR, anomaly detection
- [API Reference](../api/index.md) â€” Full REST API documentation
- [Examples](../examples/index.md) â€” Run the FDA and Raleigh demos end-to-end

Need help? Chat with us on [Discord](https://discord.gg/RrAUXTCVNF) or open a [discussion](https://github.com/llama-farm/llamafarm/discussions).

---

# Core Concepts

Understand the moving piecesâ€”projects, sessions, runtimes, and the service architectureâ€”before you customize or extend LlamaFarm.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   lf CLI   â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚  LlamaFarm    â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ Runtime Host â”‚
â”‚            â”‚ HTTP  â”‚  Server (API) â”‚       â”‚ (Ollama/vLLM â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â”‚               â”‚       â”‚  OpenAI,...) â”‚
      â”‚              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ Websocket    â”‚  â”‚ Celery  â”‚â—€â”
      â”‚ (streaming)  â”‚  â”‚ Workers â”‚ â”‚ ingest jobs
      â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
      â”‚              â”‚      â–²       â”‚
      â–¼              â”‚      â”‚       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Config     â”‚â—€â”€â”€â”€â”€â”€â”€â”˜  â”‚ RAG     â”‚â—€â”¼â”€â”€â”€â”€â”‚ Vector Store â”‚
â”‚ Watcher    â”‚ updates   â”‚ Worker  â”‚ â”‚    â”‚ (Chroma,...)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚ Dataset    â”‚
                               â”‚ Storage    â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **CLI (`lf`)** orchestrates everything: talking to the API, streaming responses, uploading datasets, and watching config changes.
- **Server** exposes REST endpoints under `/v1/projects/{namespace}/{project}/...` for chat completions, datasets, and RAG queries.
- **Celery workers** handle ingestion tasks asynchronously; the CLI polls and surfaces progress.
- **Runtime hosts** can be local (Ollama) or remote OpenAI-compatible endpoints (vLLM, Together). Configuration controls provider, base URL, API key, and instructor mode.
- **RAG worker** processes documents via configured pipelines and writes to vector databases (default Chroma, configurable).

## Projects & Namespaces

- A **project** is a configuration bundle stored in `llamafarm.yaml` plus server-side metadata.
- Projects live within a **namespace** (defaults to `default`). Namespaces isolate resources, dataset names, and sessions.
- `lf init` creates a project using the serverâ€™s template; you can list existing projects with `lf projects list --namespace my-team`.

## Sessions

- `lf chat` creates or resumes a **session** when you pass a `--session-id` or use the environment variable `LLAMAFARM_SESSION_ID`.
- `lf start` opens a stateful dev session whose history persists under `.llamafarm/projects/<namespace>/<project>/dev/context`.
- `lf chat --no-rag` is stateless by default unless you provide a session identifier.
- API consumers pass `session_id` directly to `/chat/completions` to control continuity.

## Configuration-Driven Behaviour

- `llamafarm.yaml` defines runtime, prompts, and RAG strategies (see [Configuration Guide](../configuration/index.md)).
- Changes to the file trigger the config watcher; the CLI reloads live during dev sessions.
- Missing runtime fields (provider/base_url/api_key) are treated as errors; there are no hidden defaults.

## RAG Strategies

- RAG configuration is composed of **databases** and **data processing strategies**.
- Each dataset references a strategy and database; CLI enforces this relationship when creating datasets.
- Strategies describe parsers, extractors, metadata processors, and embedding choices.

## Extensibility Mindset

Everything in LlamaFarm is intended to be swapped or extended:

- Point `runtime.base_url` to a vLLM or custom OpenAI-compatible gateway.
- Register a new vector store backend, update `rag/schema.yaml`, and regenerate types.
- Add parsers/extractors to support new file formats.
- Create new CLI subcommands under `cli/cmd` to automate workflows.

See [Extending LlamaFarm](../extending/index.md) for detailed instructions.

## Component Health

When commands run, you might see a summary like:

```
âš ï¸ Server is degraded
Summary: server=healthy, storage=healthy, ollama=healthy, celery=degraded, rag-service=healthy, project=healthy
  âš ï¸ celery  degraded   No workers replied to ping (latency: 533ms)
```

- **Degraded** does not always mean failure; ingestion may continue in the background.
- `lf rag health` reports live status of embedder, store, and processing pipeline.
- Address warnings before production deployment (ensure Celery workers are running, Ollama/vLLM accessible, etc.).

## Next Steps

- [Quickstart](../quickstart/index.md) â€“ run through the onboarding flow if you havenâ€™t already.
- [CLI Reference](../cli/index.md) â€“ learn each command in detail.
- [RAG Guide](../rag/index.md) â€“ configure databases, strategies, and retrieval.

---

# Troubleshooting & FAQ

Common symptoms, their causes, and how to resolve them when working with LlamaFarm.

## CLI & Chat

| Issue | Cause | Fix |
| ----- | ----- | --- |
| `Server is degraded` banner | A dependency (Celery, rag-service, Ollama) is slow or offline. | Ensure services are running, restart `lf start`, inspect logs. |
| Service unhealthy (check with `lf services status`) | One or more services failed to start or crashed. | Run `lf services stop`, then `lf services start` to restart. Check logs for errors. |
| `No response received` | Runtime returned an empty stream (model/tool mismatch). | Use `--no-rag`, switch to an agent handler the model supports, or choose a different model. |
| `InstructorRetryException: ... does not support tools` | You selected structured output but the model lacks tool support. | Set `instructor_mode: null` and use simple chat, or choose a capable model. |
| `context deadline exceeded` during queries | Long-running HTTP request or server busy. | Increase timeout, retry after the worker finishes, or scale workers. |

## Dataset Processing

| Issue | Cause | Fix |
| ----- | ----- | --- |
| `Task timed out or failed: PENDING` | Celery worker still ingesting large files. | Wait, then rerun `lf datasets process`; monitor worker logs. |
| Duplicates skipped | File already processed (hash matches). | Remove the file or ingest new content; duplicates are safe to ignore. |
| Metadata mismatch warnings | Strategy name or database not defined in config. | Update `llamafarm.yaml` to include the strategy/database. |

## Configuration Errors

| Error | Fix |
| ----- | --- |
| `runtime.provider is not one of ...` | Update provider enum or choose a supported value. |
| `Missing required property runtime.base_url` | Provide `base_url` when using non-default provider endpoints (vLLM, Together). |
| `llamafarm.yaml not found` | Run `lf init` or set `--cwd` to a directory containing the config. |

## Installation Issues

| Issue | Cause | Fix |
| ----- | ----- | --- |
| `lf` command not found after installation | Binary not in system PATH | Ensure `/usr/local/bin` (or custom install directory) is in your PATH. Run `echo $PATH` to verify. |
| Installation script fails | Permissions issue or unsupported platform | Try with `sudo` or check platform compatibility (macOS/Linux supported). Windows users should download `lf.exe` manually. |

## Sessions & State

- Delete `.llamafarm/projects/<namespace>/<project>/dev/context` to reset dev chat history.
- Use a new namespace (`lf init --namespace new-team`) for isolated experiments.
- Pass explicit `session_id` to API calls or set `LLAMAFARM_SESSION_ID` when testing stateless flows.

## Build & Development Issues

| Issue | Cause | Fix |
| ----- | ----- | --- |
| `nx build docs` sqlite I/O error | Nx cache database lock or permission issue | Remove `.nx/` cache directory or run with `NX_SKIP_NX_CACHE=1`. |
| Docker containers fail to start | Port conflicts or missing dependencies | Check if ports 8000/6379/5432 are available. Run `lf services stop` first, then `lf services start`. |
| Python module import errors | Virtual environment not activated or dependencies not installed | Run `cd server && uv sync` or `cd rag && uv sync` to install dependencies. |

## Extensibility Pitfalls

- Forgot to regenerate types after editing `config/schema.yaml` or `rag/schema.yaml` â†’ run `config/generate_types.py`.
- Added a provider/store without updating docs â†’ document how to configure it so others know it exists.
- CLI command not appearing â†’ ensure you added it via `rootCmd.AddCommand()` and compiled with `go build`.

Still stuck? Ask in [Discord](https://discord.gg/RrAUXTCVNF) or create a [discussion](https://github.com/llama-farm/llamafarm/discussions).

---

# Desktop App

The LlamaFarm Desktop App provides a complete local AI environment with visual project management, dataset uploads, chat interface, and built-in model management â€” no command line required.

**INFO: Found a bug or have a feature request?**
[Submit an issue on GitHub â†’](https://github.com/llama-farm/llamafarm/issues)

## Downloads

---

## Hardware Requirements

### Minimum Requirements

To run the desktop app with small models (1-3B parameters like Qwen 1.7B):

| Component | Mac | Windows | Linux |
|-----------|-----------|---------|-------|
| **CPU** | Apple M1 or newer | Intel i5 / AMD Ryzen 5 (8th gen+) | Intel i5 / AMD Ryzen 5 (8th gen+) |
| **RAM** | 8 GB | 8 GB | 8 GB |
| **Storage** | 10 GB free | 10 GB free | 10 GB free |
| **OS** | macOS 12+ (Monterey) | Windows 10/11 (64-bit) | Ubuntu 22.04+ (tested) |
| **GPU** | Integrated (Metal) | Optional | Optional |

### Recommended Requirements

For larger models (7-8B parameters) and better performance:

| Component | Mac | Windows | Linux |
|-----------|-----------|---------|-------|
| **CPU** | Apple M2 Pro or better | Intel i7 / AMD Ryzen 7 | Intel i7 / AMD Ryzen 7 |
| **RAM** | 32 GB+ | 32 GB+ | 32 GB+ |
| **Storage** | 50 GB+ SSD | 50 GB+ SSD | 50 GB+ SSD |
| **OS** | macOS 13+ (Ventura) | Windows 11 | Ubuntu 22.04+ |
| **GPU** | Unified Memory (Metal) | NVIDIA RTX 3090+ (24GB+ VRAM) | NVIDIA RTX 3090+ (24GB+ VRAM) |

---

## Model Memory Requirements

The default model is **Qwen 1.7B GGUF (Q4_K_M quantization)**, which works well on modest hardware.

| Model | Parameters | RAM Required | VRAM (GPU) | Notes |
|-------|------------|--------------|------------|-------|
| **Qwen 1.7B** (default) | 1.7B | 4 GB | 2 GB | Great for testing, fast responses |
| **Qwen 3B** | 3B | 6 GB | 4 GB | Better quality, still fast |
| **Llama 3.1 8B** | 8B | 10 GB | 6 GB | High quality, needs more resources |
| **Qwen 8B** | 8B | 10 GB | 6 GB | High quality reasoning |

**TIP: Quantization Matters**
GGUF models use quantization (Q4_K_M, Q5_K_M, Q8_0) to reduce memory usage. Q4_K_M offers the best balance of quality and speed for most users.

---

## Platform-Specific Notes

### Mac (Apple Silicon)

- **Tested on**: M1, M1 Pro, M1 Max, M2, M3
- **Acceleration**: Uses Metal for GPU acceleration automatically
- **Memory**: Unified memory is shared between CPU and GPU â€” 16GB+ recommended for 8B models
- **Installation**: Unzip and drag to Applications folder

### Windows

- **Tested on**: Windows 10 (21H2+), Windows 11
- **Acceleration**: NVIDIA CUDA (if available), otherwise CPU
- **GPU Support**: NVIDIA GPUs with CUDA 11.8+ drivers
- **Installation**: Run the `.exe` installer

**NOTE: Windows Defender**
Windows Defender may scan the app on first launch. This is normal and should complete within a minute.

### Linux

- **Tested on**: Ubuntu 22.04 LTS, Ubuntu 24.04 LTS
- **Format**: AppImage (portable, no installation needed)
- **Acceleration**: NVIDIA CUDA or Vulkan (if available)
- **Dependencies**: FUSE required for AppImage

```bash
# Make executable and run
chmod +x LlamaFarm-desktop-app-linux-x86_64.AppImage
./LlamaFarm-desktop-app-linux-x86_64.AppImage

# If FUSE is not installed:
sudo apt install fuse libfuse2
```

**NOTE: Other Distributions**
While Ubuntu is our primary test platform, the AppImage should work on most modern Linux distributions with glibc 2.31+. Community reports for Fedora, Arch, and Debian are welcome!

---

## Features

The desktop app includes:

- **Visual Project Management** â€” Create, configure, and switch between projects
- **Dataset Uploads** â€” Drag-and-drop file uploads with real-time processing status
- **Chat Interface** â€” Test your AI with full RAG context
- **Model Management** â€” Download, switch, and configure models
- **Built-in Services** â€” No need to run Docker or manage background processes

---

## Troubleshooting

### App won't start

1. **Windows**: Allow through Windows Defender/Firewall
2. **Linux**: Ensure FUSE is installed, check AppImage is executable

### Out of memory

- Close other applications
- Use a smaller model (Qwen 1.7B instead of 8B)
- Use higher quantization (Q4_K_M instead of Q8_0)

### Model download fails

- Check internet connection
- Ensure sufficient disk space
- Try downloading again â€” downloads resume automatically

### Need help?

- [Submit an issue on GitHub](https://github.com/llama-farm/llamafarm/issues)
- [Join our Discord](https://discord.gg/RrAUXTCVNF)

---

## Next Steps

- [Quickstart Guide](../quickstart/index.md) â€” Get started with your first project
- [Configuration Guide](../configuration/index.md) â€” Customize your setup
- [CLI Reference](../cli/index.md) â€” For power users who want command-line access

---

# Designer Web UI

The LlamaFarm Designer is a modern, browser-based interface for building and managing AI projects. It provides a visual way to configure projects, manage datasets, test prompts, and interact with your AI systemsâ€”all without touching the command line.

## What is the Designer?

The Designer is a React-based web application that complements the `lf` CLI. While the CLI excels at automation and scripting, the Designer offers:

- **Visual project management**: Create and configure projects through an intuitive interface
- **Interactive dataset management**: Upload files via drag-and-drop, configure processing strategies visually
- **Real-time configuration**: Edit your `llamafarm.yaml` with live validation and helpful hints
- **Integrated chat interface**: Test your AI project with full RAG context
- **Dual-mode editing**: Switch between visual designer and direct YAML editing

## When to Use the Designer vs CLI

Use the **Designer** when you want to:

- Quickly prototype and experiment with configurations
- Visualize your project structure and relationships
- Upload and manage datasets interactively
- Test prompts and see results in real-time
- Get immediate feedback on configuration errors

Use the **CLI** when you want to:

- Automate workflows and integrate with CI/CD
- Process large batches of data
- Script repetitive tasks
- Work in headless/remote environments

## Accessing the Designer

The easiest way to start the Designer is through the `lf start` command:

```bash
lf start
```

This automatically launches:

- The FastAPI server (port 8000)
- The RAG worker
- The Designer web UI (port **8000**)

Once started, open your browser to:

```
http://localhost:8000
```

The Designer is served by the same FastAPI server, so it shares port 8000 with the API.

![Designer Home Page](./screenshots/designer-home.png)
_The Designer home page where you can create and select projects_

## Main Sections

The Designer is organized into several key sections:

### Projects

Switch between projects or create new ones. Each project has its own configuration, datasets, and chat history.

### Dashboard

Get an overview of your active project, including:

- Project configuration summary
- Dataset statistics
- Quick actions for common tasks
- Version history

### Data

Manage your datasets and processing strategies:

- Upload files via drag-and-drop or file picker
- Create and configure data processing strategies
- Monitor ingestion progress
- View dataset details and file lists

### Models

Configure runtime providers and models:

- Select from available models (Ollama, OpenAI, etc.)
- Configure model parameters (temperature, max tokens, etc.)
- Test model connections
- Download models for local use

### Databases

Set up RAG (Retrieval-Augmented Generation):

- Configure vector databases
- Choose embedding models
- Select retrieval strategies
- Test queries against your data

### Prompts

Design and test system prompts:

- Create prompt templates
- Test prompts with different inputs
- Compare outputs across models
- Save effective prompts to your configuration

### Chat

Interactive testing with your AI project:

- Chat with full project context
- Toggle RAG on/off
- View retrieved documents
- Debug responses

### Config Editor Mode

Every section includes a toggle to switch between Designer (visual) mode and Config Editor (YAML) mode. In Config Editor mode, you can:

- Directly edit the `llamafarm.yaml` file
- See real-time validation errors
- Search and navigate large configurations
- Copy/paste configuration snippets

## Next Steps

- Explore the [Features Guide](./features.md) for detailed walkthroughs of each section
- Learn about [Development Setup](./development.md) if you want to contribute or customize the Designer
- Check out the [Quickstart Guide](../quickstart/index.md) to create your first project

## Troubleshooting

**Designer won't load:**

- Ensure the server is running (`lf start` or check Docker containers)
- Verify the server is accessible at `http://localhost:8000/health/liveness`
- Check browser console for connection errors

**Can't connect to API:**

- The Designer expects the server at `localhost:8000` by default
- If using a custom setup, set the `VITE_APP_API_URL` environment variable

**Changes not saving:**

- Check for validation errors (red highlights in Config Editor)
- Ensure you have write permissions to your project directory
- Look for error messages in the top-right toast notifications

---

# Designer Features

This guide walks through each section of the Designer web UI, explaining what you can do and how to use the various features.

## Projects

The Projects section is your starting point in the Designer. From the home page, you can:

### Creating a New Project

1. Click the "Create new project" button on the home page
2. Enter a project name (alphanumeric and hyphens only)
3. Optionally fill in the project brief:
   - **What**: Describe what your project does
   - **Goals**: Define what you want to achieve
   - **Audience**: Specify who will use it
4. Click "Create" to initialize the project

The Designer will automatically create a `llamafarm.yaml` configuration file with sensible defaults.

### Switching Projects

Click on any project card on the home page to switch to that project. Your current project is highlighted in the header.

### Editing Project Details

From any project page, click the project name dropdown in the header and select "Edit project" to update the project brief or rename the project.

## Dashboard

The Dashboard provides an at-a-glance view of your active project.

![Designer Dashboard](./screenshots/dashboard.png)
*The Dashboard showing project overview and key metrics*

### What You'll See

- **Project Configuration Summary**: Key settings like runtime provider, model, and enabled features
- **Dataset Statistics**: Number of datasets, total files, and processing status
- **Quick Actions**: Common tasks like creating datasets or editing prompts
- **Recent Activity**: Latest changes to your project configuration

### Using the Dashboard

The Dashboard is read-only and designed for quick reference. Use the action buttons to jump directly to relevant sections when you need to make changes.

## Data Management

The Data section is where you manage datasets and configure how files are processed.

![Data Management Interface](./screenshots/data-management.png)
*Managing datasets and processing strategies*

### Processing Strategies

Processing strategies define how your data is parsed, chunked, and embedded. You can:

- **View existing strategies**: See all configured data processing strategies for your project
- **Create new strategies**: Click "Create new" to define a strategy from scratch
- **Copy strategies**: Duplicate an existing strategy as a starting point
- **Edit strategies**: Click on any strategy card to view and modify its configuration

Each strategy includes:
- **Parsers**: How to extract text from different file types (PDF, DOCX, TXT, CSV, Markdown)
- **Chunking**: How to split documents into manageable pieces
- **Embedding**: Which model to use for creating vector embeddings
- **Storage**: Which vector database to use

### Datasets

Below the processing strategies, you'll see your datasets:

- **Create Dataset**: Click "+ Create dataset" to add a new dataset
  - Choose a name
  - Select a processing strategy
  - Select a database
- **Upload Files**: Drag and drop files onto a dataset card or click to browse
- **Process Dataset**: After uploading, click "Process" to start ingestion
- **View Details**: Click a dataset name to see files, metadata, and processing status
- **Delete Dataset**: Use the dropdown menu on each card

### Supported File Types

The Designer supports uploading and processing:
- PDF documents (`.pdf`)
- Word documents (`.docx`)
- Text files (`.txt`)
- CSV files (`.csv`)
- Markdown files (`.md`, `.markdown`)

### Config Editor Mode

Toggle to Config Editor mode to see the raw YAML configuration for datasets and strategies. This is useful for:
- Copying configuration between projects
- Making bulk changes
- Understanding the underlying structure

## Models

The Models section lets you configure which AI models your project uses.

### Runtime Configuration

Select your runtime provider:
- **Ollama** (default): Local models on your machine
- **OpenAI**: GPT models via API
- **OpenAI-compatible**: Custom endpoints (vLLM, LM Studio, etc.)

### Model Selection

Depending on your provider:
- **Ollama**: Choose from downloaded models or pull new ones
- **OpenAI**: Enter your API key and select a model
- **Custom**: Configure the base URL and model name

### Model Parameters

Adjust runtime behavior:
- **Temperature**: Control randomness (0.0 = deterministic, 1.0+ = creative)
- **Max Tokens**: Limit response length
- **Top P**: Nucleus sampling threshold
- **Frequency Penalty**: Discourage repetition
- **Presence Penalty**: Encourage topic diversity

### Testing Connections

Use the "Test Connection" button to verify your model configuration works before saving.

## Databases (RAG Configuration)

The Databases section is where you configure Retrieval-Augmented Generation.

![RAG Configuration](./screenshots/rag-config.png)
*Configuring vector databases and retrieval strategies*

### Vector Databases

Create and manage vector databases:
- **ChromaDB** (default): Local vector storage
- **Pinecone**, **Weaviate**, etc.: Cloud-hosted options

Each database needs:
- A unique name
- Storage configuration (connection details for hosted options)

### Embedding Strategies

Configure how text is converted to vectors:
- **Embedding Model**: Choose from available models (e.g., `nomic-embed-text`)
- **Chunk Size**: How large each embedded piece should be
- **Overlap**: How much chunks should overlap to preserve context

### Retrieval Methods

Define how relevant documents are found:
- **Similarity Search**: Pure vector similarity (default)
- **Hybrid Search**: Combines vector and keyword matching
- **MMR (Maximal Marginal Relevance)**: Balances relevance with diversity
- **Metadata Filtering**: Add filters based on document properties

### Testing Queries

Use the built-in query tester to:
1. Enter a test question
2. See which documents are retrieved
3. Verify relevance and ranking
4. Adjust retrieval parameters if needed

## Prompts

The Prompts section helps you design and test system prompts.

### Creating Prompts

1. Click "Add Prompt" to create a new prompt template
2. Give it a descriptive name
3. Write your system prompt (supports variables like `{context}`, `{question}`)
4. Add any user message templates

### Prompt Sets

Organize related prompts into sets:
- Default prompts that always run
- Conditional prompts based on context
- Specialized prompts for different tasks

### Testing Prompts

Use the test panel to:
- Enter sample inputs
- See the rendered prompt (with variables filled in)
- Get actual model responses
- Compare multiple prompt versions side-by-side

### Prompt Library

Browse example prompts for common use cases:
- RAG-enhanced Q&A
- Document summarization
- Code generation
- Data extraction

## Chat Interface

The Chat section provides an interactive way to test your AI project.

### Starting a Chat

Simply type a message and press Enter or click Send. The chat uses:
- Your configured runtime and model
- Active system prompts
- RAG context if enabled

### RAG Toggle

Use the RAG toggle to:
- **On**: Retrieve relevant documents before answering (default for RAG projects)
- **Off**: Use only the model's training data

When RAG is on, you'll see retrieved document snippets alongside responses.

### Session Management

Each project maintains its own chat session:
- History persists between visits
- Clear chat to start fresh
- Sessions are stored locally in your project directory

### Debugging Responses

The chat interface shows:
- Retrieved context (when RAG is enabled)
- Retrieval scores
- Processing time
- Token usage (if available)

Use this information to tune your RAG configuration for better results.

## Config Editor

Every section in the Designer includes a toggle between Designer mode (visual) and Config Editor mode (raw YAML).

### When to Use Config Editor

- Making precise changes to nested configuration
- Copying configuration between projects
- Bulk editing (e.g., updating all chunk sizes)
- Learning the YAML structure
- Troubleshooting validation errors

### Editor Features

The Config Editor is powered by CodeMirror and includes:
- **Syntax highlighting**: YAML with schema awareness
- **Real-time validation**: Errors highlighted as you type
- **Auto-completion**: Suggestions based on schema
- **Search and replace**: Find and modify across the entire config
- **Line numbers**: Easy navigation in large files

### Validation

The editor validates your changes against the LlamaFarm schema:
- Red underlines indicate errors
- Hover over errors for explanations
- Changes won't save until validation passes

### Formatting

The editor automatically:
- Indents correctly
- Maintains YAML syntax
- Preserves comments (if you add them manually)

## Switching Between Modes

In any section, look for the toggle button in the top-right corner to switch between Designer and Config Editor modes. Your place in the document is preserved when switching.

## Tips and Best Practices

### For New Users
- Start with Designer mode to understand available options
- Switch to Config Editor once you're comfortable with the structure
- Use the Dashboard to verify your changes took effect

### For Advanced Users
- Config Editor is faster for repetitive changes
- Copy example configurations and modify them
- Use comments in YAML mode to document your choices

### For Troubleshooting
- Check Config Editor for validation errors that might not be obvious in Designer mode
- Compare your config with working examples
- Use the browser console (F12) for detailed error messages

## Next Steps

- Return to the [Designer Overview](./index.md)
- Learn how to [develop and customize the Designer](./development.md)
- Explore [example projects](../examples/index.md) to see these features in action

---

# Designer Development Guide

This guide covers how to run, build, and customize the LlamaFarm Designer for local development or contribution.

## Prerequisites

Before you begin, ensure you have:

- **Node.js 20+**: Download from [nodejs.org](https://nodejs.org/) or use `nvm`
- **npm**: Comes with Node.js
- **LlamaFarm Server**: The Designer requires a running backend server

## Local Development Setup

### 1. Clone the Repository

```bash
git clone https://github.com/llama-farm/llamafarm.git
cd llamafarm/designer
```

### 2. Install Dependencies

The Designer uses `--legacy-peer-deps` due to some dependency resolution quirks:

```bash
npm install --legacy-peer-deps
```

This installs all required packages including:
- React 18
- Vite (build tool)
- TanStack Query (data fetching)
- Radix UI (component library)
- CodeMirror (YAML editor)
- Axios (HTTP client)

### 3. Start the Backend Server

The Designer needs the LlamaFarm server running. In a separate terminal:

```bash
# Option 1: Using the CLI
lf start

# Option 2: Manual server start
cd ../server
uv sync
uv run uvicorn server.main:app --reload --host 0.0.0.0 --port 8000
```

Verify the server is running:
```bash
curl http://localhost:8000/health/liveness
```

### 4. Start the Development Server

Back in the designer directory:

```bash
npm run dev
```

The Designer will start on `http://localhost:5173` (Vite's default port).

Vite will automatically:
- Proxy API requests to `http://localhost:8000`
- Hot-reload when you edit source files
- Display build errors in the browser

### 5. Open in Browser

Navigate to `http://localhost:5173` to see the Designer.

## Development Workflow

### Project Structure

```
designer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/              # API client and service functions
â”‚   â”œâ”€â”€ components/       # React components (organized by feature)
â”‚   â”œâ”€â”€ hooks/            # Custom React hooks
â”‚   â”œâ”€â”€ contexts/         # React context providers
â”‚   â”œâ”€â”€ types/            # TypeScript type definitions
â”‚   â”œâ”€â”€ utils/            # Helper functions
â”‚   â”œâ”€â”€ App.tsx           # Main app component with routing
â”‚   â””â”€â”€ main.tsx          # Application entry point
â”œâ”€â”€ public/               # Static assets
â”œâ”€â”€ nginx/                # Nginx config for production
â”œâ”€â”€ package.json          # Dependencies and scripts
â”œâ”€â”€ vite.config.ts        # Vite configuration
â”œâ”€â”€ tsconfig.json         # TypeScript configuration
â”œâ”€â”€ Dockerfile            # Multi-stage build for production
â””â”€â”€ docker-entrypoint.sh  # Environment variable injection
```

### API Proxy Configuration

During development, Vite proxies API requests to avoid CORS issues. This is configured in `vite.config.ts`:

```typescript
server: {
  proxy: {
    '/api': {
      target: process.env.API_URL || 'http://localhost:8000',
      changeOrigin: true,
      rewrite: path => path.replace(/^\/api/, ''),
    },
  },
}
```

The client code (`src/api/client.ts`) automatically detects localhost and connects directly to `http://localhost:8000`.

### Environment Variables

You can customize the API URL using environment variables:

```bash
# .env.local
VITE_APP_API_URL=http://localhost:8000
```

Available variables:
- `VITE_APP_API_URL`: Backend API base URL
- `VITE_API_VERSION`: API version (default: `v1`)
- `VITE_APP_ENV`: Environment name (development/production)

### Making Changes

1. **Edit source files** in `src/`
2. **See changes instantly** via hot module replacement
3. **Check the browser console** (F12) for errors
4. **Test API calls** using the Network tab

### Code Style

The project uses ESLint and Prettier:

```bash
# Check for linting errors
npm run lint

# Auto-fix linting errors
npm run lint:fix
```

Follow these conventions:
- Use functional components with hooks
- Prefer TypeScript interfaces over types
- Keep components focused and small
- Use TanStack Query for all API calls
- Handle loading and error states

## Building for Production

### Local Build

To create a production build locally:

```bash
npm run build
```

This:
1. Runs TypeScript compiler (`tsc`) to check types
2. Bundles and minifies with Vite
3. Outputs to `dist/` directory

Preview the production build:

```bash
npm run preview
```

### Build Output

The production build includes:
- Minified JavaScript bundles
- Optimized CSS
- Static assets
- `index.html` entry point

Build artifacts are code-split for efficient loading:
- `codemirror-core`: Editor core functionality
- `codemirror-features`: Language support and themes
- `react-vendor`: React and React Router
- `ui-vendor`: Radix UI components
- `utils-vendor`: TanStack Query and Axios

## Docker Build

The Designer uses a multi-stage Docker build for production deployment.

### Building the Image

```bash
cd designer
docker build -t llamafarm-designer .
```

The Dockerfile:
1. **Build stage** (Node 20 Alpine): Installs deps and builds
2. **Production stage** (Nginx Alpine): Serves static files

### Running the Container

```bash
docker run -d \
  --name llamafarm-designer \
  -p 3123:80 \
  -e VITE_APP_API_URL=http://localhost:8000 \
  llamafarm-designer
```

### Environment Variable Injection

The container uses a custom entrypoint (`docker-entrypoint.sh`) that:
1. Captures `VITE_APP_*` environment variables
2. Injects them into the built `index.html` as a global `ENV` object
3. Starts Nginx

This allows runtime configuration without rebuilding the image.

## Architecture Overview

### Tech Stack

- **React 18**: UI framework with hooks and concurrent features
- **TypeScript**: Type safety and better DX
- **Vite**: Fast build tool with HMR
- **TanStack Query**: Server state management and caching
- **React Router**: Client-side routing
- **Radix UI**: Accessible component primitives
- **Tailwind CSS**: Utility-first styling
- **CodeMirror 6**: Advanced code editor
- **Axios**: HTTP client with interceptors

### State Management

The Designer uses multiple state management strategies:

1. **Server State** (TanStack Query):
   - Projects, datasets, models
   - Cached with automatic revalidation
   - See `src/hooks/` for query definitions

2. **Local State** (React hooks):
   - UI state (modals, panels, forms)
   - Transient data not persisted

3. **Context** (React Context):
   - Theme (light/dark)
   - Active project
   - Modal state

### Data Flow

1. User interacts with component
2. Component calls hook (e.g., `useCreateDataset`)
3. Hook triggers TanStack Query mutation
4. Query sends request via `src/api/` service
5. Server responds
6. Query updates cache and UI re-renders

### Key Components

- **Header**: Global navigation and project switcher
- **Dashboard**: Project overview and quick stats
- **Data**: Dataset and strategy management
- **Models**: Runtime configuration
- **Databases**: RAG setup
- **Prompts**: Prompt engineering interface
- **Chat**: Interactive testing with chatbox
- **ConfigEditor**: YAML editor with validation

### Routing

Routes are defined in `src/App.tsx`:

```
/                          â†’ Home (project selection)
/samples                   â†’ Sample projects
/chat
  /dashboard              â†’ Project dashboard
  /data                   â†’ Dataset management
  /data/:datasetId        â†’ Dataset details
  /models                 â†’ Model configuration
  /databases              â†’ RAG configuration
  /prompt                 â†’ Prompt management
  /test                   â†’ Testing interface
```

## Testing

### Manual Testing

1. Start the dev server
2. Test each section:
   - Create/edit projects
   - Upload datasets
   - Configure models
   - Test RAG queries
   - Edit configuration

### API Testing

Use the browser Network tab to inspect:
- Request/response payloads
- Status codes
- Response times

### Console Logging

The Designer logs proxy requests during development:
```
[PROXY] GET /api/v1/projects/default/my-project -> http://localhost:8000/v1/projects/default/my-project
```

## Common Development Tasks

### Adding a New Component

1. Create component file in `src/components/`
2. Define props interface
3. Implement component
4. Export from `src/components/index.ts` (if needed)

### Adding a New API Endpoint

1. Add function to appropriate `src/api/*.ts` file
2. Define request/response types in `src/types/`
3. Create React Query hook in `src/hooks/`
4. Use hook in component

### Adding a New Route

1. Add route to `src/App.tsx`
2. Create page component
3. Update navigation links if needed

### Updating Types from Schema

When the backend schema changes:

```bash
cd ../config
uv run python generate_types.py
```

This regenerates both backend and frontend types to keep them in sync.

## Troubleshooting

### Port Already in Use

If port 5173 is occupied:
```bash
# Kill the process using the port
lsof -ti:5173 | xargs kill -9

# Or specify a different port
npm run dev -- --port 3000
```

### API Connection Errors

- Verify server is running: `curl http://localhost:8000/health/liveness`
- Check `VITE_APP_API_URL` in environment
- Look at browser console for CORS errors

### Build Failures

- Clear node_modules and reinstall: `rm -rf node_modules && npm install --legacy-peer-deps`
- Check TypeScript errors: `npx tsc --noEmit`
- Verify all imports resolve correctly

### Hot Reload Not Working

- Check file watchers limit on Linux: `echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf && sudo sysctl -p`
- Restart the dev server
- Clear browser cache

## Contributing

### Before Submitting a PR

1. Run linter: `npm run lint:fix`
2. Test your changes manually
3. Build successfully: `npm run build`
4. Update types if schema changed
5. Document new features in this guide

### Code Review Checklist

- [ ] TypeScript types are correct
- [ ] Loading and error states handled
- [ ] Responsive design works on mobile
- [ ] Dark mode looks correct
- [ ] No console errors or warnings
- [ ] API calls use TanStack Query
- [ ] Components are accessible (keyboard navigation, ARIA labels)

## Resources

- [React Documentation](https://react.dev/)
- [Vite Documentation](https://vitejs.dev/)
- [TanStack Query Documentation](https://tanstack.com/query/latest)
- [Radix UI Documentation](https://www.radix-ui.com/)
- [Tailwind CSS Documentation](https://tailwindcss.com/)
- [CodeMirror 6 Documentation](https://codemirror.net/)

## Next Steps

- Return to the [Designer Overview](./index.md)
- Explore [Designer Features](./features.md)
- Check the [Contributing Guide](../contributing/index.md) for general contribution guidelines

---

# CLI Reference

The `lf` CLI is your control center for LlamaFarm projects. This reference captures global flags, command behaviours, and examples you can copy into your shell. Each subcommand shares the same auto-start logic: if the server or RAG worker is not running locally, the CLI will launch them (unless you override `--server-url`).

## Global Flags

```
lf [command] [flags]
```

| Flag | Description |
| ---- | ----------- |
| `--debug`, `-d` | Enable verbose logging. |
| `--server-url` | Override the server endpoint (default `http://localhost:8000`). |
| `--server-start-timeout` | How long to wait for local server startup (default 45s). |
| `--cwd` | Treat another directory as the working project root. |
| `--auto-start` | Automatically start services when needed (default: true). Use `--auto-start=false` to disable. |

Environment helpers:
- `LLAMAFARM_SESSION_ID` â€“ reuse a session for `lf chat`.
- `OLLAMA_HOST` â€“ point `lf start` to a different Ollama endpoint.
- `LF_VERSION_REF` â€“ override the source code version/ref downloaded by the CLI (useful for testing feature branches or specific versions).
- `LF_DATA_DIR` â€“ override the data directory (default: `~/.llamafarm`).

## Command Matrix

| Command | Description |
| ------- | ----------- |
| [`lf init`](./lf-init.md) | Scaffold a project and generate `llamafarm.yaml`. |
| [`lf start`](./lf-start.md) | Launch server + RAG services and open the dev chat UI. |
| [`lf chat`](./lf-chat.md) | Send single prompts, preview REST calls, manage sessions. |
| [`lf models`](./lf-models.md) | List available models and manage multi-model configurations. |
| [`lf datasets`](./lf-datasets.md) | Create, upload, process, and delete datasets. |
| [`lf rag`](./lf-rag.md) | Query documents and access RAG maintenance tools. |
| [`lf projects`](./lf-projects.md) | List projects by namespace. |
| [`lf services`](./lf-services.md) | Manage LlamaFarm services (server, RAG worker, universal runtime). |
| [`lf version`](./lf-version.md) | Print CLI version/build info and check for updates. |

## Service Management with --auto-start

The `--auto-start` flag (default: `true`) gives you control over when services start. By default, the CLI automatically starts the server and RAG worker when needed. Use `--auto-start=false` to:

- **CI/CD pipelines**: Connect to pre-started services without triggering restarts
- **Manual service management**: Keep services running between commands
- **Debugging**: Separate service startup from command execution

### Usage Examples

**Two-terminal development workflow:**
```bash
# Terminal 1: Start and monitor services
lf start

# Terminal 2: Run commands without triggering restarts
lf chat --auto-start=false "What is LlamaFarm?"
lf datasets list --auto-start=false
lf rag stats --auto-start=false
```

**CI/CD integration:**
```yaml
- name: Start services
  run: lf start &

- name: Wait for health
  run: timeout 60 bash -c 'until curl -f http://localhost:8000/health; do sleep 2; done'

- name: Run tests
  run: |
    lf datasets create --auto-start=false -s pdf_ingest -b main_db test-data
    lf rag query --auto-start=false "test query"
```

**Error handling:**
If services are not running and you use `--auto-start=false`, you'll see:
```
services not running and auto-start is disabled: server, celery (use --auto-start to enable automatic startup)
```

To start services manually, run:
```bash
lf start
```

Or remove the `--auto-start=false` flag to allow automatic startup.

## Troubleshooting CLI Output

- **"Server is degraded"** â€“ At least one dependency (Celery, RAG worker, Ollama) is slow or offline. Commands may still succeed; check logs if they hang.
- **"No response received"** â€“ The runtime streamed nothing; run with `--no-rag` or change models if the provider struggles with tools output.
- **Dataset processing timeouts** â€“ The CLI times out after waiting for Celery. Re-run once ingestion finishes or increase worker availability.
- **Authorization redaction** â€“ `lf chat --curl` hides API keys automatically; replace `<redacted>` before running.

Looking to add a new command? See [Extending LlamaFarm](../extending/index.md#extend-the-cli) for a Cobra walkthrough.

## Services Management

The `lf services` command provides control over LlamaFarm's backend services.

### Check Service Status

```bash
lf services status
lf services status --json  # Machine-readable output
```

Shows the status of all services (server, RAG worker, universal runtime).

### Start Services

```bash
lf services start              # Start all services
lf services start server       # Start specific service
lf services start rag          # Start RAG worker
lf services start universal-runtime  # Start Universal Runtime
```

**Orchestration Modes:**
- `native` (default): Native processes managed by CLI
- `docker`: Docker containers
- `auto`: Auto-detect best mode

Set via environment variable:
```bash
LF_ORCHESTRATION_MODE=docker lf services start
```

### Stop Services

```bash
lf services stop               # Stop all services
lf services stop server        # Stop specific service
```

### Available Services

| Service | Description | Default Port |
|---------|-------------|--------------|
| `server` | Main FastAPI server | 8000 |
| `rag` | RAG/Celery worker | N/A |
| `universal-runtime` | Universal Runtime server | 11540 |

---

# lf init

Create a new LlamaFarm project and generate a validated `llamafarm.yaml` based on server templates.

## Synopsis

```
lf init [path] [flags]
```

- `path` (optional) â€“ directory where the project should be created (default: current directory).

## Flags

| Flag | Description |
| ---- | ----------- |
| `--namespace` | Namespace to create the project in (defaults to `default`). |
| `--template` | Server-side template name (optional). |
| Global flags | `--server-url`, `--debug`, etc. |

## Behaviour

- Ensures the API server is reachable (auto-starts locally if needed).
- Creates the target directory (if missing) and writes `llamafarm.yaml` using the serverâ€™s `project.config` response.
- Fails if a LlamaFarm config already exists in the directory.

## Example

```bash
lf init customer-support
cd customer-support
cat llamafarm.yaml
```

**Output**

```
Initializing a new LlamaFarm project in customer-support
Created project default/customer-support in /path/to/customer-support
```

If you see `Error: Project already exists`, remove or rename the directory before re-running.

## See Also

- [Configuration Guide](../configuration/index.md)
- [`lf start`](./lf-start.md)

---

# lf start

Launch the local development stack (API server + RAG worker) and open an interactive chat UI with live config watching.

## Synopsis

```
lf start [flags]
```

## Flags

| Flag            | Description                                                      |
| --------------- | ---------------------------------------------------------------- |
| `--ollama-host` | Override the Ollama endpoint (default `http://localhost:11434`). |
| Global flags    | `--server-url`, `--debug`, `--cwd`, etc.                         |

Environment variables:

- `OLLAMA_HOST` â€“ fallback if the flag is not provided.

## Behaviour

- Ensures the server and RAG worker containers are running (via the container orchestrator/Nx).
- Starts a config watcher so edits to `llamafarm.yaml` refresh automatically.
- Launches a text UI for chatting with the current project; use `Ctrl+C` to exit.
- Shows health diagnostics for server, storage, Ollama/vLLM, Celery, rag-service, and project state.
- Starts the Designer web UI at `http://localhost:8000` for visual project management.

## Example

```bash
lf start --ollama-host http://localhost:11434
```

**Typical health banner**

```
âš ï¸  Server is degraded
Summary: server=healthy, storage=healthy, ollama=healthy, celery=degraded, rag-service=healthy, project=healthy
  âš ï¸ celery  degraded   No workers replied to ping (latency: 533ms)
```

Investigate degraded components before shipping to productionâ€”Celery workers might be offline or overloaded.

## See Also

- [`lf chat`](./lf-chat.md)
- [Designer Web UI](../designer/index.md) â€“ Visual interface available after starting
- [Troubleshooting](../troubleshooting/index.md)

---

# lf chat

Send a single prompt to your projectâ€™s chat endpoint. By default, responses include RAG context defined in `llamafarm.yaml`.

## Synopsis

```
lf chat                        # Interactive TUI using project from llamafarm.yaml
lf chat [namespace/project]    # Interactive TUI for explicit project
lf chat [namespace/project] "message" [flags]  # One-off request
```

If you omit `namespace/project`, the CLI resolves them from `llamafarm.yaml`.

## Useful Flags

| Flag | Description |
| ---- | ----------- |
| `--model` | Select a specific model from your multi-model configuration. |
| `--file`, `-f` | Read prompt content from a file. |
| `--no-rag` | Skip retrievalâ€”direct LLM call. |
| `--database` | Target a specific RAG database. |
| `--retrieval-strategy` | Override the retrieval strategy. |
| `--rag-top-k` | Adjust the number of results (default 5). |
| `--rag-score-threshold` | Minimum similarity score for results. |
| `--curl` | Print the sanitized `curl` request instead of executing. |

## Behaviour

- Automatically starts the server if needed.
- Filters client/error messages from the transcript before sending.
- Streams responses; exit code is non-zero if the API returns an error.
- Redacts authorization headers when using `--curl`.

## Examples

```bash
# Interactive project chat (auto-detect project)
lf chat

# Basic one-off chat (RAG enabled)
lf chat "Summarize the latest FDA letters."

# Use a specific model
lf chat --model powerful "Complex reasoning question"

# Explicit project with file input
lf chat company/legal -f prompt.txt

# Pure LLM request with curl preview
lf chat --no-rag --curl "Explain RAG in 2 sentences"

# Override strategy for targeted retrieval
lf chat --database main_db --retrieval-strategy hybrid_search "Find biologics references"

# Combine model selection with RAG
lf chat --model lemon --database main_db "Query with specific model and database"
```

## Sessions

- Set `LLAMAFARM_SESSION_ID=abc123` to keep context between calls.
- `lf start` manages its own session history in `.llamafarm/projects/.../dev/context`.
- Delete session files to reset state or start a new namespace/project for isolation.

## See Also

- [`lf rag query`](./lf-rag.md)
- [Configuration Guide](../configuration/index.md)

---

# lf models

Manage and interact with models configured in your project. The models command provides subcommands to list available models and switch between them during chat sessions.

## Synopsis

```
lf models list [namespace/project] [flags]
```

If you omit `namespace/project`, the CLI resolves them from `llamafarm.yaml`.

## Subcommands

### `lf models list`

List all models configured in your project with their descriptions and providers.

```bash
lf models list                    # List models from current project
lf models list company/project    # List models from specific project
```

**Output includes:**
- Model name (used for `--model` flag)
- Description
- Provider (ollama, lemonade, openai, etc.)
- Default status

## Using Models

After listing available models, use them in chat commands:

```bash
# Use a specific model
lf chat --model powerful "Complex reasoning question"

# Use the default model (no flag needed)
lf chat "Regular question"
```

## Multi-Model Configuration

Configure multiple models in `llamafarm.yaml`:

```yaml
runtime:
  default_model: fast

  models:
    fast:
      description: "Fast Ollama model"
      provider: ollama
      model: gemma3:1b

    powerful:
      description: "More capable model"
      provider: ollama
      model: qwen3:8b

    lemon:
      description: "Lemonade local model"
      provider: lemonade
      model: user.Qwen3-4B
      base_url: "http://127.0.0.1:11534/v1"
      lemonade:
        backend: llamacpp
        port: 11534
```

## Examples

```bash
# List all models
lf models list

# Use a specific model for chat
lf chat --model lemon "What is the capital of France?"

# Compare responses from different models
lf chat --model fast "Quick answer needed"
lf chat --model powerful "Complex reasoning task"
```

## See Also

- [`lf chat`](./lf-chat.md)
- [Models & Runtime Guide](../models/index.md)
- [Configuration Guide](../configuration/index.md)

---

# lf datasets

Manage datasets defined for your projectâ€”create, upload, process, list, and delete ingestion targets used by RAG.

## Subcommands

| Command | Description |
| ------- | ----------- |
| `lf datasets list` | Show datasets for the current project. |
| `lf datasets create` | Create a dataset (optionally upload files immediately). |
| `lf datasets upload` | Upload files to an existing dataset. |
| `lf datasets process` | Run the ingestion pipeline (parsers, extractors, embeddings). |
| `lf datasets delete` | Remove a dataset. |

Each subcommand accepts the global flags for server resolution and config watching.

## `lf datasets list`

```
lf datasets list
```

Prints a table containing dataset name, strategy, database, and file count.

## `lf datasets create`

```
lf datasets create -s <strategy> -b <database> <name> [files...]
```

- Validates that the strategy and database exist in your `rag` configuration.
- Accepts globs and directories for optional initial upload (same semantics as `upload`).

## `lf datasets upload`

```
lf datasets upload <name> <path> [paths...]
```

- Supports directories, globs, and individual files.
- Displays per-file success/failure alongside a summary.

## `lf datasets process`

```
lf datasets process <name>
```

- Sends an ingestion job to Celery.
- Shows progress dots every 2 seconds when stdout is a TTY.
- Reads results and prints chunk counts, parser/extractor usage, and skip reasons (duplicates, errors).

If you see a timeout, the worker may still be processingâ€”wait and re-run. Consult server logs for detailed status.

## `lf datasets delete`

```
lf datasets delete <name>
```

Deletes the dataset and its metadata from the server (stored files remain unless back-end removes them).

## Tips

- Place shared data under `datasets` in `llamafarm.yaml` if you want the CLI to sync metadata.
- Run `lf rag health` after processing to ensure embedder/store are healthy.
- Combine with `lf rag query` to verify retrieval quality immediately after ingestion.

## See Also

- [RAG Guide](../rag/index.md)
- [Troubleshooting](../troubleshooting/index.md)

---

# lf rag

Query your knowledge base and access RAG maintenance utilities.

## Querying Documents

```
lf rag query "question" [flags]
```

| Flag | Purpose |
| ---- | ------- |
| `--database` | Select a database (defaults to config default). |
| `--data-processing-strategy` | Filter results to a strategy. |
| `--retrieval-strategy` | Override retrieval behaviour (vector, hybrid, metadata filtered, etc.). |
| `--top-k` | Number of chunks to return. |
| `--score-threshold` | Minimum similarity score. |
| `--filter` | Apply metadata filters (`key:value`). Repeatable. |
| `--include-metadata`, `--include-score` | Show metadata/score columns. |
| `--distance-metric`, `--hybrid-alpha`, `--rerank-model`, `--query-expansion`, `--max-tokens` | Advanced knobs matching server capabilities. |

Example:

```bash
lf rag query --database main_db --filter "doc_type:letter" --include-metadata \
  "Which letters mention additional clinical trials?"
```

## Maintenance Commands

Some subcommands are hidden from `--help` but available for operators:

| Command | Description |
| ------- | ----------- |
| `lf rag stats` | Vector/document counts, storage usage (JSON or table). |
| `lf rag health` | Embedder/store health summary. |
| `lf rag list` | List ingested documents and metadata. |
| `lf rag compact` | Compact/optimize the vector store. |
| `lf rag reindex` | Reindex all documents using a given strategy. |
| `lf rag clear` | Delete **all** documents from a database (dangerous). |
| `lf rag delete` | Remove documents by ID, filename, or metadata filter. |
| `lf rag export/import` | Move datasets between environments. |

> âš ï¸ Destructive commands (`clear`, `delete`) prompt for confirmation unless you pass `--force`.

## Troubleshooting

- **Empty results** â€“ confirm dataset processing succeeded and the retrieval strategy matches your query type.
- **Timeouts** â€“ large datasets can take time to process; check Celery logs or increase `--server-start-timeout` before retrying.
- **Hybrid/Tool Errors** â€“ some smaller models donâ€™t support tool calls; switch to a basic agent handler via configuration.

## See Also

- [RAG Guide](../rag/index.md)
- [Extending RAG](../extending/index.md#extend-rag-components)

---

# lf projects

List the projects available within a namespace on the server.

## Synopsis

```
lf projects list [flags]
```

## Flags

| Flag | Description |
| ---- | ----------- |
| `--namespace` | Namespace to list (required if not set in config). |
| Global flags | `--server-url`, `--debug`, etc. |

## Example

```bash
lf projects list --namespace default
```

Output:

```
default/llamafarm-1
default/fda-assistant
default/raleigh-udo-demo
```

Use this to discover project IDs when switching between namespaces or debugging server state.

## See Also

- [`lf init`](./lf-init.md)
- [Configuration Guide](../configuration/index.md)

---

# lf services

Manage LlamaFarm backend services (server, RAG worker, universal runtime).

## Synopsis

```bash
lf services <subcommand> [service-name] [flags]
```

## Subcommands

| Command | Description |
|---------|-------------|
| `status` | Check status of all services |
| `start` | Start services |
| `stop` | Stop services |

## Available Services

| Service | Description | Default Port |
|---------|-------------|--------------|
| `server` | Main FastAPI server | 8000 |
| `rag` | RAG/Celery worker | N/A |
| `universal-runtime` | Universal Runtime server for HuggingFace models | 11540 |

## Orchestration Modes

LlamaFarm supports multiple orchestration modes, controlled by the `LF_ORCHESTRATION_MODE` environment variable:

| Mode | Description |
|------|-------------|
| `native` (default) | Run services as native processes |
| `docker` | Run services as Docker containers |
| `auto` | Auto-detect best mode (prefers native) |

## lf services status

Check the current status of all LlamaFarm services without starting them.

```bash
lf services status [flags]
```

### Flags

| Flag | Description |
|------|-------------|
| `--json` | Output status in JSON format |

### Output Information

The status command shows:
- Process/container running state
- PID (native) or container ID (Docker)
- Port mappings (Docker)
- Health status (if service is running)
- Log file location (native) or image information (Docker)
- Uptime

### Examples

```bash
# Check status of all services
lf services status

# Get machine-readable JSON output
lf services status --json
```

### Sample Output

```
LlamaFarm Services Status
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Service             State      PID      Health    Uptime
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  server              running    12345    healthy   2h 15m
  rag                 running    12346    healthy   2h 15m
  universal-runtime   stopped    -        -         -
```

## lf services start

Start LlamaFarm services.

```bash
lf services start [service-name] [flags]
```

### Arguments

- `service-name` (optional): Specific service to start. If omitted, starts all services.

### Examples

```bash
# Start all services
lf services start

# Start only the main server
lf services start server

# Start only the RAG worker
lf services start rag

# Start only the Universal Runtime
lf services start universal-runtime

# Start all services using Docker
LF_ORCHESTRATION_MODE=docker lf services start
```

## lf services stop

Stop LlamaFarm services.

```bash
lf services stop [service-name] [flags]
```

### Arguments

- `service-name` (optional): Specific service to stop. If omitted, stops all services.

### Examples

```bash
# Stop all services
lf services stop

# Stop only the main server
lf services stop server

# Stop only the RAG worker
lf services stop rag

# Stop all Docker containers
LF_ORCHESTRATION_MODE=docker lf services stop
```

## Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `LF_ORCHESTRATION_MODE` | Orchestration mode (`native`, `docker`, `auto`) | `native` |

## Use Cases

### Development Workflow

```bash
# Start all services for development
lf services start

# Check everything is running
lf services status

# When done, stop everything
lf services stop
```

### Debugging Service Issues

```bash
# Check which services are running
lf services status --json | jq '.services[] | select(.state == "running")'

# Restart a specific service
lf services stop server
lf services start server
```

### Docker Deployment

```bash
# Start services in Docker mode
LF_ORCHESTRATION_MODE=docker lf services start

# Check container status
lf services status

# Stop Docker containers
LF_ORCHESTRATION_MODE=docker lf services stop
```

## Troubleshooting

### Service Won't Start

1. Check if the port is already in use:
   ```bash
   lsof -i :8000  # For server
   lsof -i :11540 # For universal-runtime
   ```

2. Kill conflicting processes:
   ```bash
   kill <PID>
   ```

3. Try starting the service again:
   ```bash
   lf services start server
   ```

### Service Shows "unhealthy"

1. Check the service logs (native mode):
   ```bash
   # Log files are shown in status output
   tail -f ~/.llamafarm/logs/server.log
   ```

2. Verify dependencies are running:
   - Server needs Redis for Celery
   - RAG worker needs ChromaDB
   - Universal Runtime needs sufficient memory for models

### Docker Mode Issues

1. Ensure Docker is running:
   ```bash
   docker ps
   ```

2. Check Docker container logs:
   ```bash
   docker logs llamafarm-server
   ```

## See Also

- [lf start](./lf-start.md) - Start server and open interactive chat
- [lf init](./lf-init.md) - Initialize a new project
- [Configuration Guide](../configuration/index.md) - Configure services

---

# lf version

Display the CLI build information.

## Synopsis

```
lf version
```

## Output

```
LlamaFarm CLI v0.0.0-dev
  commit: abcdef1234
  built: 2025-09-29T12:34:56Z
```

Use this command when reporting issues or validating which release youâ€™re running.

## See Also

- [Contributing](../contributing/index.md)

---

# Configuration Guide

Every LlamaFarm project is defined by a single file: `llamafarm.yaml`. The server validates it against JSON Schema, so missing fields surface as errors instead of hidden defaults. This guide explains each section and shows how to extend the schema responsibly.

## File Layout

```yaml
version: v1
name: my-project
namespace: default
runtime: { ... }
prompts: [...]
rag: { ... }
datasets: [...]
```

### Metadata

| Field       | Type   | Required  | Notes                                              |
| ----------- | ------ | --------- | -------------------------------------------------- |
| `version`   | string | âœ… (`v1`) | Schema version.                                    |
| `name`      | string | âœ…        | Project identifier.                                |
| `namespace` | string | âœ…        | Grouping for isolation (matches server namespace). |

### Runtime

Controls how chat completions are executed. LlamaFarm supports both **multi-model** (recommended) and **legacy single-model** configurations.

#### Multi-Model Configuration (Recommended)

Configure multiple models and switch between them via CLI or API:

```yaml
runtime:
  default_model: fast  # Which model to use by default

  models:
    fast:
      description: "Fast Ollama model"
      provider: ollama
      model: gemma3:1b
      prompt_format: unstructured

    powerful:
      description: "More capable model"
      provider: ollama
      model: qwen3:8b
```

**Using multi-model:**
- CLI: `lf chat --model powerful "your question"`
- CLI: `lf models list`
- API: `POST /v1/projects/{ns}/{id}/chat/completions` with `{"model": "powerful", ...}`

#### Legacy Single-Model Configuration (Still Supported)

The original flat runtime configuration is automatically converted internally:

```yaml
runtime:
  provider: openai
  model: qwen2.5:7b
  base_url: http://localhost:8000/v1
  api_key: sk-local-placeholder
  instructor_mode: tools
  model_api_parameters:
    temperature: 0.2
```

#### Runtime Fields

**Multi-model format:**

| Field           | Type   | Required | Description                                   |
| --------------- | ------ | -------- | --------------------------------------------- |
| `default_model` | string | âœ…       | Name of the default model to use              |
| `models`        | array  | âœ…       | List of model configurations (see below)      |

**Per-model fields:**

| Field                  | Type                                  | Required                                                                   | Description                                                                                                            |
| ---------------------- | ------------------------------------- | -------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| `name`                 | string                                | âœ…                                                                         | Unique identifier for this model                                                                                       |
| `provider`             | enum (`openai`, `ollama`, `lemonade`, `universal`) | âœ…                                                                         | `openai` for OpenAI-compatible APIs, `ollama` for local Ollama, `lemonade` for local GGUF models with NPU/GPU support, `universal` for the Universal Runtime |
| `model`                | string                                | âœ…                                                                         | Model identifier understood by the provider                                                                            |
| `description`          | string                                | Optional                                                                   | Human-readable description of the model                                                                                |
| `default`              | boolean                               | Optional                                                                   | Set to `true` to make this the default model (alternative to `default_model`)                                         |
| `base_url`             | string or null                        | âš ï¸ Required for non-default hosts (vLLM, Together, Lemonade)              | API endpoint URL                                                                                                       |
| `api_key`              | string or null                        | âš ï¸ Required for most hosted providers. Use `.env` + environment variables | Authentication key                                                                                                     |
| `instructor_mode`      | string or null                        | Optional                                                                   | `json`, `md_json`, `tools` for structured output modes                                                                 |
| `prompt_format`        | string                                | Optional                                                                   | `unstructured` or other format                                                                                         |
| `model_api_parameters` | object                                | Optional                                                                   | Passthrough parameters (temperature, top_p, etc.)                                                                      |
| `lemonade`             | object                                | âš ï¸ Required for `provider: lemonade`                                      | Lemonade-specific configuration (see below)                                                                            |
| `extra_body`           | object                                | Optional                                                                   | Provider-specific parameters (see `n_ctx` below)                                                                       |
| `encoder_config`       | object                                | Optional                                                                   | Configuration for BERT-style encoder models (Universal runtime only)                                                   |
| `tool_call_strategy`   | enum                                  | `native_api`                                                               | `native_api` or `prompt_based` for tool calling strategy                                                               |
| `mcp_servers`          | array                                 | Optional                                                                   | List of MCP server names to use (omit for all, empty for none)                                                         |

**extra_body fields (Universal runtime):**

| Field   | Type    | Default | Description                                              |
| ------- | ------- | ------- | -------------------------------------------------------- |
| `n_ctx` | integer | auto    | Context window size for GGUF models. Auto-detected if not specified. |

**encoder_config fields (Universal runtime):**

| Field                | Type    | Default     | Description                                              |
| -------------------- | ------- | ----------- | -------------------------------------------------------- |
| `max_length`         | integer | auto        | Maximum sequence length (ModernBERT: 8192, classic: 512) |
| `use_flash_attention`| boolean | `true`      | Enable Flash Attention 2 for faster inference            |
| `task`               | enum    | `embedding` | `embedding`, `classification`, `reranking`, `ner`        |

**Lemonade-specific fields:**

| Field          | Type   | Required | Description                                  |
| -------------- | ------ | -------- | -------------------------------------------- |
| `backend`      | string | âœ…       | `llamacpp`, `onnx`, or `transformers`        |
| `port`         | number | âœ…       | Port number (default: 11534)                 |
| `context_size` | number | Optional | Context window size (default: 32768)         |

> **Extending providers:** To add a new provider enum, update `config/schema.yaml`, regenerate types via `config/generate_types.py`, and implement routing in the server/CLI. See [Extending runtimes](../extending/index.md#extend-runtimes).

### Prompts

Prompts are named sets of messages that seed instructions for each session.

```yaml
prompts:
  - name: default
    messages:
      - role: system
        content: >-
          You are a supportive assistant. Cite documents when relevant.
```

- Each prompt has a `name` and a list of `messages` with `role` and `content`.
- Roles can be `system`, `user`, or `assistant` (anything supported by the runtime).
- Models can select which prompt sets to use via `prompts: [list of names]`; if omitted, all prompts stack in definition order.
- Prompts are appended before user input; combine with RAG context via the RAG guide.

### RAG Configuration

The `rag` section mirrors [`rag/schema.yaml`](/rag/schema.yaml). It defines databases and data-processing strategies.

```yaml
rag:
  databases:
    - name: main_db
      type: ChromaStore
      default_embedding_strategy: default_embeddings
      default_retrieval_strategy: semantic_search
      embedding_strategies:
        - name: default_embeddings
          type: OllamaEmbedder
          config:
            model: nomic-embed-text:latest
      retrieval_strategies:
        - name: semantic_search
          type: VectorRetriever
          config:
            top_k: 5
  data_processing_strategies:
    - name: pdf_ingest
      parsers:
        - type: PDFParser_LlamaIndex
          config:
            chunk_size: 1500
            chunk_overlap: 200
      extractors:
        - type: HeadingExtractor
        - type: ContentStatisticsExtractor
```

Key points:

- `databases` map to vector stores; choose from `ChromaStore` or `QdrantStore` by default.
- `embedding_strategies` and `retrieval_strategies` let you define hybrid or metadata-aware search.
- `data_processing_strategies` describe parser/extractor pipelines applied during ingestion.
- For a complete field reference, see the [RAG Guide](../rag/index.md).

### Memory Configuration

The `memory` section configures optional memory stores for working memory, time-series data, spatial data, and graph relationships.

```yaml
memory:
  default_store: main_memory
  stores:
    - name: main_memory
      working_memory:
        enabled: true
        ttl_seconds: 3600
        max_records: 10000
      timeseries:
        enabled: true
        retention_days: 30
      spatial:
        enabled: false
      graph:
        enabled: true
        max_path_depth: 10
        entity_extraction: true
        relationship_extraction: false
      consolidation:
        enabled: true
        interval_seconds: 300
        min_records: 10
        batch_size: 100
```

#### Memory Store Fields

| Field | Type | Description |
|-------|------|-------------|
| `name` | string | Unique identifier for this memory store |
| `working_memory` | object | Short-term memory buffer configuration |
| `timeseries` | object | Time-series store configuration |
| `spatial` | object | Geo-spatial store configuration |
| `graph` | object | Graph store configuration |
| `consolidation` | object | Memory consolidation settings |

#### Working Memory

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `enabled` | boolean | `true` | Enable working memory |
| `ttl_seconds` | integer | `3600` | Time-to-live for records |
| `max_records` | integer | `10000` | Maximum records before auto-prune |

#### Time-series

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `enabled` | boolean | `true` | Enable time-series store |
| `retention_days` | integer | `30` | Days to retain data |

#### Spatial

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `enabled` | boolean | `true` | Enable spatial store |
| `retention_days` | integer | `30` | Days to retain data |
| `index_type` | string | `rtree` | `rtree` or `geohash` |

#### Graph

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `enabled` | boolean | `true` | Enable graph store |
| `max_path_depth` | integer | `10` | Maximum depth for path finding |
| `entity_extraction` | boolean | `true` | Extract entities using NER |
| `relationship_extraction` | boolean | `false` | Extract relationships via LLM |

#### Consolidation

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `enabled` | boolean | `true` | Enable consolidation |
| `interval_seconds` | integer | `300` | Consolidation interval |
| `min_records` | integer | `10` | Minimum records before consolidation |
| `batch_size` | integer | `100` | Batch size for consolidation |
| `prune_after_consolidate` | boolean | `true` | Prune after consolidation |
| `extract_summaries` | boolean | `false` | Create embeddings from consolidated data |

---

### Datasets

`datasets` keep metadata about datasets you manage via the CLI.

```yaml
datasets:
  - name: research-notes
    data_processing_strategy: pdf_ingest
    database: main_db
    files:
      - 2d5fd8424e62c56cad39864fac9ecff7af9639cf211deb936a16dc05aca5b3ea
```

- `files` are SHA256 hashes tracked by the server.
- Not required, but useful for syncing dataset metadata across environments.

## Validation & Errors

- The CLI enforces schema validation when loading configs. Missing runtime fields raise `Error: runtime.provider is required`.
- Use `lf chat --curl` to inspect the raw request if responses look wrong (verify prompts and RAG toggles).
- The server logs include full validation errors if API calls fail due to config mismatches.

## Extending the Schema

1. Edit `config/schema.yaml` or `rag/schema.yaml` to add new enums/properties.
2. Run `config/generate_types.py` to regenerate Pydantic/Go datamodels.
3. Update server/CLI logic to accept the new fields.
4. Document the addition in this guide and the Extending section.

Example: To support a new provider `together`, add it to the `provider` enum, regenerate types, and update runtime selection to issue HTTP requests to Togetherâ€™s API.

## Best Practices

- Keep secrets out of YAML; use environment variables and reference them at runtime.
- Version control your config; treat `llamafarm.yaml` like application code.
- Use separate namespaces or configs for dev/staging/prod to avoid cross-talk.
- Document uncommon parser/extractor choices for future maintainers.

Need concrete samples? Check the [Example configs](./example-configs.md) and the examples in the repo (`examples/fda_rag/llamafarm-example.yaml`).

---

# Example Configs

Use these snippets as starting points for real projects. Every example validates against the current schema.

## Quick Start (Minimal Config)

The simplest working configuration:

```yaml
version: v1
name: quickstart
namespace: default

runtime:
  default_model: default
  models:
    - name: default
      provider: ollama
      model: gemma3:1b
      default: true
```

## Simple RAG Setup

Basic RAG with PDF processing:

```yaml
version: v1
name: simple-rag
namespace: default

runtime:
  default_model: default
  models:
    - name: default
      provider: ollama
      model: llama3.1:8b
      default: true

rag:
  databases:
    - name: docs_db
      type: ChromaStore
      embedding_strategies:
        - name: default
          type: OllamaEmbedder
          config:
            model: nomic-embed-text
      retrieval_strategies:
        - name: search
          type: BasicSimilarityStrategy
          config:
            top_k: 5
          default: true
      default_embedding_strategy: default
      default_retrieval_strategy: search

  data_processing_strategies:
    - name: basic
      description: "Basic PDF and text processing"
      parsers:
        - type: PDFParser_PyPDF2
          file_include_patterns: ["*.pdf"]
          config:
            chunk_size: 1000
            chunk_overlap: 100
        - type: TextParser_Python
          file_include_patterns: ["*.txt", "*.md"]
          config:
            chunk_size: 800
```

## Local RAG with Ollama (Multi-Model)

```yaml
version: v1
name: local-rag
namespace: default

runtime:
  default_model: default

  models:
    - name: default
      description: "Primary Ollama model"
      provider: ollama
      model: llama3:8b
      default: true

prompts:
  - name: default
    messages:
      - role: system
        content: >-
          You are a friendly assistant. Reference document titles when possible.

rag:
  databases:
    - name: main_db
      type: ChromaStore
      default_embedding_strategy: default_embeddings
      default_retrieval_strategy: semantic_search
      embedding_strategies:
        - name: default_embeddings
          type: OllamaEmbedder
          config:
            model: nomic-embed-text:latest
      retrieval_strategies:
        - name: semantic_search
          type: VectorRetriever
          config:
            top_k: 5
  data_processing_strategies:
    - name: pdf_ingest
      parsers:
        - type: PDFParser_LlamaIndex
          config:
            chunk_size: 1200
            chunk_overlap: 150
      extractors:
        - type: HeadingExtractor
        - type: ContentStatisticsExtractor

datasets:
  - name: policies
    data_processing_strategy: pdf_ingest
    database: main_db
```

## Lemonade Local Runtime (Multi-Model)

```yaml
version: v1
name: lemonade-local
namespace: default

runtime:
  default_model: balanced

  models:
    - name: fast
      description: "Fast 0.6B model for quick responses"
      provider: lemonade
      model: user.Qwen3-0.6B
      base_url: "http://127.0.0.1:11534/v1"
      lemonade:
        backend: llamacpp
        port: 11534
        context_size: 32768

    - name: balanced
      description: "Balanced 4B model - recommended"
      provider: lemonade
      model: user.Qwen3-4B
      base_url: "http://127.0.0.1:11535/v1"
      default: true
      lemonade:
        backend: llamacpp
        port: 11535
        context_size: 32768

    - name: powerful
      description: "Powerful 8B reasoning model"
      provider: lemonade
      model: user.Qwen3-8B
      base_url: "http://127.0.0.1:11536/v1"
      lemonade:
        backend: llamacpp
        port: 11536
        context_size: 65536

prompts:
  - name: default
    messages:
      - role: system
        content: >-
          You are a helpful assistant with access to local models.
```

**Setup:**
1. Download models: `uv run lemonade-server-dev pull user.Qwen3-4B --checkpoint unsloth/Qwen3-4B-GGUF:Q4_K_M --recipe llamacpp`
2. Start instances (from llamafarm project root):
   - `LEMONADE_MODEL=user.Qwen3-0.6B LEMONADE_PORT=11534 nx start lemonade`
   - `LEMONADE_MODEL=user.Qwen3-4B LEMONADE_PORT=11535 nx start lemonade`
   - `LEMONADE_MODEL=user.Qwen3-8B LEMONADE_PORT=11536 nx start lemonade`

> **Note:** Currently, Lemonade must be manually started. In the future, it will run as a container and be auto-started by the LlamaFarm server.

See [Lemonade Quickstart](../models#quick-setup) for detailed setup.

## vLLM Gateway with Structured Output

```yaml
version: v1
name: llm-gateway
namespace: enterprise

runtime:
  default_model: vllm-model

  models:
    - name: vllm-model
      description: "vLLM gateway model"
      provider: openai
      model: qwen2.5:7b
      base_url: https://llm.company.internal/v1
      api_key: ${VLLM_API_KEY}
      instructor_mode: tools
      default: true
      model_api_parameters:
        temperature: 0.1

prompts:
  - name: default
    messages:
      - role: system
        content: >-
          You are a compliance assistant returning JSON with fields: `summary`, `citations`.

rag:
  databases:
    - name: compliance_db
      type: QdrantStore
      default_embedding_strategy: openai_embeddings
      default_retrieval_strategy: hybrid_search
      embedding_strategies:
        - name: openai_embeddings
          type: OpenAIEmbedder
          config:
            model: text-embedding-3-small
      retrieval_strategies:
        - name: hybrid_search
          type: HybridUniversalStrategy
          config:
            dense_weight: 0.7
            sparse_weight: 0.3
  data_processing_strategies:
    - name: docx_ingest
      parsers:
        - type: DocxParser_LlamaIndex
          config:
            chunk_size: 1000
            chunk_overlap: 100
      extractors:
        - type: EntityExtractor
          config:
            include_types: [ORGANIZATION, LAW]
```

## Multi-Strategy Retrieval

```yaml
rag:
  databases:
    - name: research_db
      type: ChromaStore
      default_embedding_strategy: dense_embeddings
      default_retrieval_strategy: reranked_search
      embedding_strategies:
        - name: dense_embeddings
          type: SentenceTransformerEmbedder
          config:
            model: all-MiniLM-L6-v2
      retrieval_strategies:
        - name: keyword_search
          type: BM25Retriever
          config:
            stop_words: ["the", "a", "and"]
        - name: reranked_search
          type: RerankedStrategy
          config:
            candidate_strategy: keyword_search
            reranker: bm25+embedding
```

## Mixed Providers (Ollama + Lemonade)

```yaml
version: v1
name: mixed-providers
namespace: default

runtime:
  default_model: ollama-default

  models:
    - name: ollama-default
      description: "Primary Ollama model"
      provider: ollama
      model: llama3:8b
      default: true

    - name: ollama-small
      description: "Small Ollama model"
      provider: ollama
      model: gemma3:1b

    - name: lemon-fast
      description: "Lemonade fast model with NPU/GPU"
      provider: lemonade
      model: user.Qwen3-0.6B
      base_url: "http://127.0.0.1:11534/v1"
      lemonade:
        backend: llamacpp
        port: 11534
        context_size: 32768

prompts:
  - name: default
    messages:
      - role: system
        content: >-
          You are a helpful assistant. Use the appropriate model for the task.
```

**Usage:**
- Fast responses: `lf chat --model lemon-fast "Quick question"`
- Default model: `lf chat "Normal question"`
- Small model: `lf chat --model ollama-small "Simple task"`

## Universal Runtime with Reranking

Universal Runtime enables cross-encoder reranking for high-precision search:

```yaml
version: v1
name: reranking-demo
namespace: default

runtime:
  default_model: default
  models:
    - name: default
      provider: ollama
      model: llama3.1:8b
      default: true

    # Cross-encoder reranker via Universal Runtime
    - name: reranker
      description: "Fast cross-encoder for document reranking"
      provider: universal
      model: cross-encoder/ms-marco-MiniLM-L-6-v2
      base_url: http://127.0.0.1:11540

rag:
  databases:
    - name: main_db
      type: ChromaStore
      config:
        collection_name: documents
        distance_function: cosine

      embedding_strategies:
        - name: default
          type: OllamaEmbedder
          config:
            model: nomic-embed-text
            dimension: 768

      retrieval_strategies:
        - name: fast
          type: BasicSimilarityStrategy
          config:
            top_k: 10
          default: false

        - name: accurate
          type: CrossEncoderRerankedStrategy
          config:
            model_name: reranker
            initial_k: 30
            final_k: 5
            relevance_threshold: 0.3
          default: true

      default_embedding_strategy: default
      default_retrieval_strategy: accurate
```

## Complex Multi-Format RAG

Full-featured RAG with multiple file types, extractors, and retrieval strategies:

```yaml
version: v1
name: enterprise-rag
namespace: production

runtime:
  default_model: primary
  models:
    - name: primary
      description: "Primary production model"
      provider: ollama
      model: llama3.1:8b
      default: true

    - name: fast
      description: "Fast model for quick queries"
      provider: ollama
      model: gemma3:1b

    - name: query_decomposer
      description: "Model for query decomposition"
      provider: openai
      model: gemma3:1b
      base_url: http://localhost:11434/v1

    - name: reranker
      description: "Cross-encoder reranker"
      provider: universal
      model: BAAI/bge-reranker-v2-m3
      base_url: http://127.0.0.1:11540

prompts:
  - name: default
    messages:
      - role: system
        content: |
          You are an expert document analyst. Always cite sources by filename.
          If information is not in the provided context, say so clearly.

rag:
  default_database: main_db

  databases:
    - name: main_db
      type: ChromaStore
      config:
        collection_name: enterprise_docs
        distance_function: cosine
        persist_directory: ./data/main_db

      embedding_strategies:
        - name: default
          type: UniversalEmbedder
          config:
            model: nomic-ai/nomic-embed-text-v2-moe
            dimension: 768
            batch_size: 16
          priority: 0

      retrieval_strategies:
        # Fast basic search
        - name: fast
          type: BasicSimilarityStrategy
          config:
            top_k: 10
            distance_metric: cosine

        # Filtered by metadata
        - name: filtered
          type: MetadataFilteredStrategy
          config:
            top_k: 10
            filter_mode: pre

        # High-accuracy reranking
        - name: accurate
          type: CrossEncoderRerankedStrategy
          config:
            model_name: reranker
            initial_k: 30
            final_k: 10
          default: true

        # Complex query handling
        - name: complex
          type: MultiTurnRAGStrategy
          config:
            model_name: query_decomposer
            max_sub_queries: 3
            complexity_threshold: 50
            enable_reranking: true
            reranker_config:
              model_name: reranker
              initial_k: 20
              final_k: 10

      default_embedding_strategy: default
      default_retrieval_strategy: accurate

  data_processing_strategies:
    - name: universal_processor
      description: "Process PDFs, Word docs, spreadsheets, and text"
      parsers:
        # PDFs with LlamaIndex (semantic chunking)
        - type: PDFParser_LlamaIndex
          file_include_patterns: ["*.pdf", "*.PDF"]
          priority: 100
          config:
            chunk_size: 1200
            chunk_overlap: 150
            chunk_strategy: semantic
            extract_metadata: true
            extract_tables: true

        # PDF fallback
        - type: PDFParser_PyPDF2
          file_include_patterns: ["*.pdf"]
          priority: 50
          config:
            chunk_size: 1000
            chunk_overlap: 100

        # Word documents
        - type: DocxParser_LlamaIndex
          file_include_patterns: ["*.docx"]
          priority: 100
          config:
            chunk_size: 1000
            chunk_overlap: 100
            extract_tables: true

        # Excel files
        - type: ExcelParser_LlamaIndex
          file_include_patterns: ["*.xlsx", "*.xls"]
          priority: 100
          config:
            chunk_size: 500
            chunk_strategy: rows

        # CSV files
        - type: CSVParser_Pandas
          file_include_patterns: ["*.csv"]
          priority: 100
          config:
            chunk_size: 500
            extract_metadata: true

        # Markdown
        - type: MarkdownParser_LlamaIndex
          file_include_patterns: ["*.md", "*.markdown"]
          priority: 100
          config:
            chunk_size: 800
            chunk_strategy: headings
            extract_code_blocks: true

        # Plain text and code
        - type: TextParser_LlamaIndex
          file_include_patterns: ["*.txt", "*.py", "*.js", "*.html"]
          priority: 80
          config:
            chunk_size: 800
            chunk_strategy: semantic
            preserve_code_structure: true

      extractors:
        # Entity extraction
        - type: EntityExtractor
          priority: 100
          config:
            entity_types: [PERSON, ORG, DATE, PRODUCT, EMAIL, PHONE]
            use_fallback: true

        # Keyword extraction
        - type: KeywordExtractor
          priority: 90
          config:
            algorithm: yake
            max_keywords: 15

        # Content statistics
        - type: ContentStatisticsExtractor
          priority: 80
          config:
            include_readability: true
            include_structure: true

        # Pattern matching
        - type: PatternExtractor
          priority: 70
          file_include_patterns: ["*.pdf"]
          config:
            predefined_patterns: [email, phone, date, version]

datasets:
  - name: documents
    database: main_db
    data_processing_strategy: universal_processor
```

## Qdrant Production Setup

Production-ready configuration with Qdrant vector database:

```yaml
version: v1
name: qdrant-production
namespace: production

runtime:
  default_model: default
  models:
    - name: default
      provider: openai
      model: gpt-4o-mini
      api_key: ${OPENAI_API_KEY}
      default: true

rag:
  databases:
    - name: production_db
      type: QdrantStore
      config:
        host: qdrant.internal.company.com
        port: 6333
        api_key: ${QDRANT_API_KEY}
        collection_name: documents
        vector_size: 1536
        distance: Cosine

      embedding_strategies:
        - name: openai
          type: OpenAIEmbedder
          config:
            model: text-embedding-3-small
            api_key: ${OPENAI_API_KEY}
            batch_size: 100

      retrieval_strategies:
        - name: production
          type: HybridUniversalStrategy
          config:
            combination_method: weighted_average
            final_k: 10
            strategies:
              - type: BasicSimilarityStrategy
                weight: 0.7
                config:
                  top_k: 20
              - type: MetadataFilteredStrategy
                weight: 0.3
                config:
                  top_k: 20
          default: true

      default_embedding_strategy: openai
      default_retrieval_strategy: production
```

---

Mix and match these patterns to suit your project. For detailed component configuration, see the [RAG documentation](../rag/index.md):

- [Parsers Reference](../rag/parsers.md) - All parser types and options
- [Embedders Reference](../rag/embedders.md) - Embedding configurations
- [Extractors Reference](../rag/extractors.md) - Metadata extraction
- [Databases Reference](../rag/databases.md) - Vector store options
- [Retrieval Strategies](../rag/retrieval-strategies.md) - Search configurations

---

# API Reference

LlamaFarm provides a comprehensive REST API for managing projects, datasets, chat interactions, and RAG (Retrieval-Augmented Generation) operations. The API follows RESTful conventions and is compatible with OpenAI's chat completion format.

## Base URL

The API is served at: `http://localhost:8000`

All versioned endpoints use the `/v1` prefix:

```text
http://localhost:8000/v1
```

## Finding Your Namespace and Project Name

### Understanding Namespaces and Projects

LlamaFarm organizes your work into **namespaces** (organizational containers) and **projects** (individual configurations):

- **Namespace**: A top-level organizational unit (e.g., your username, team name, or organization)
- **Project Name**: The unique identifier for a specific LlamaFarm project within a namespace.

### From Your llamafarm.yaml

The easiest way to find your namespace and project name is to check your `llamafarm.yaml` configuration file:

```yaml
version: v1
name: my-project # This is your project name
namespace: my-org # This is your namespace
```

### From the File System

Projects are stored in:

```
~/.llamafarm/projects/{namespace}/{project_name}/
```

For example, if you see:

```
~/.llamafarm/projects/acme-corp/chatbot/
```

Then:

- Namespace: `acme-corp`
- Project name: `chatbot`

### Using the API

You can also list projects programmatically:

```bash
# List all projects in a namespace
curl http://localhost:8000/v1/projects/my-org
```

### Custom Data Directory

If you've set a custom data directory using the `LF_DATA_DIR` environment variable, check:

```
$LF_DATA_DIR/projects/{namespace}/{project_name}/
```

## Authentication

Currently, the API does not require authentication. This is designed for local development environments. For production deployments, implement authentication at the reverse proxy or load balancer level.

## Response Format

### Success Responses

Successful requests return JSON with appropriate HTTP status codes (200, 201, etc.):

```json
{
  "field": "value",
  ...
}
```

### Error Responses

Error responses follow a consistent format with appropriate HTTP status codes (400, 404, 500, etc.):

```json
{
  "detail": "Error message description"
}
```

Common HTTP status codes:

- `200 OK` - Request succeeded
- `201 Created` - Resource created successfully
- `400 Bad Request` - Invalid request parameters
- `404 Not Found` - Resource not found
- `422 Unprocessable Entity` - Validation error
- `500 Internal Server Error` - Server error

## API Endpoints Overview

### Projects

- `GET /v1/projects/{namespace}` - List projects
- `POST /v1/projects/{namespace}` - Create project
- `GET /v1/projects/{namespace}/{project}` - Get project details
- `PUT /v1/projects/{namespace}/{project}` - Update project configuration
- `DELETE /v1/projects/{namespace}/{project}` - Delete project

### Chat

- `POST /v1/projects/{namespace}/{project}/chat/completions` - Send chat message (OpenAI-compatible)
- `GET /v1/projects/{namespace}/{project}/chat/sessions/{session_id}/history` - Get chat history
- `DELETE /v1/projects/{namespace}/{project}/chat/sessions/{session_id}` - Delete chat session
- `DELETE /v1/projects/{namespace}/{project}/chat/sessions` - Delete all sessions
- `GET /v1/projects/{namespace}/{project}/models` - List available models

### Datasets

- `GET /v1/projects/{namespace}/{project}/datasets` - List datasets
- `POST /v1/projects/{namespace}/{project}/datasets` - Create dataset
- `DELETE /v1/projects/{namespace}/{project}/datasets/{dataset}` - Delete dataset
- `POST /v1/projects/{namespace}/{project}/datasets/{dataset}/data` - Upload file to dataset
- `POST /v1/projects/{namespace}/{project}/datasets/{dataset}/actions` - Trigger dataset actions (ingest/process) via Celery tasks
- `DELETE /v1/projects/{namespace}/{project}/datasets/{dataset}/data/{file_hash}` - Remove file from dataset

### RAG (Retrieval-Augmented Generation)

- `POST /v1/projects/{namespace}/{project}/rag/query` - Query RAG system
- `GET /v1/projects/{namespace}/{project}/rag/health` - Check RAG health
- `GET /v1/projects/{namespace}/{project}/rag/stats` - Get RAG statistics
- `GET /v1/projects/{namespace}/{project}/rag/databases` - List databases
- `GET /v1/projects/{namespace}/{project}/rag/databases/{database}` - Get database details
- `GET /v1/projects/{namespace}/{project}/rag/databases/{database}/documents` - List documents in database
- `POST /v1/projects/{namespace}/{project}/rag/databases` - Create database
- `PATCH /v1/projects/{namespace}/{project}/rag/databases/{database}` - Update database
- `DELETE /v1/projects/{namespace}/{project}/rag/databases/{database}` - Delete database

### Tasks

- `GET /v1/projects/{namespace}/{project}/tasks/{task_id}` - Get async task status
- `DELETE /v1/projects/{namespace}/{project}/tasks/{task_id}` - Cancel running task

### Event Logs

- `GET /v1/projects/{namespace}/{project}/event_logs` - List event logs
- `GET /v1/projects/{namespace}/{project}/event_logs/{event_id}` - Get event details

### Examples

- `GET /v1/examples` - List available examples
- `GET /v1/examples/{example_id}/datasets` - List example datasets
- `POST /v1/examples/{example_id}/import-project` - Import example as new project
- `POST /v1/examples/{example_id}/import-data` - Import example data into existing project
- `POST /v1/examples/{example_id}/import-dataset` - Import specific dataset from example

### Models Cache

- `GET /v1/models` - List cached models
- `POST /v1/models/download` - Download/cache a model
- `POST /v1/models/validate-download` - Check disk space before download
- `GET /v1/models/{model_id}/quantizations` - List GGUF quantization options
- `DELETE /v1/models/{model_name}` - Delete cached model

### Vision (OCR & Document Extraction)

- `POST /v1/vision/ocr` - OCR text extraction (accepts file upload or base64)
- `POST /v1/vision/documents/extract` - Document extraction/VQA (accepts file upload or base64)

### ML (Custom Classifiers & Anomaly Detection)

- `POST /v1/ml/classifier/fit` - Train custom text classifier (SetFit few-shot)
- `POST /v1/ml/classifier/predict` - Classify texts using trained model
- `POST /v1/ml/classifier/save` - Save trained classifier to disk
- `POST /v1/ml/classifier/load` - Load classifier from disk
- `GET /v1/ml/classifier/models` - List saved classifiers
- `DELETE /v1/ml/classifier/models/{name}` - Delete saved classifier
- `POST /v1/ml/anomaly/fit` - Train anomaly detector
- `POST /v1/ml/anomaly/score` - Score data points for anomalies
- `POST /v1/ml/anomaly/detect` - Detect anomalies (returns only anomalous points)
- `POST /v1/ml/anomaly/save` - Save trained anomaly model
- `POST /v1/ml/anomaly/load` - Load anomaly model from disk
- `GET /v1/ml/anomaly/models` - List saved anomaly models
- `DELETE /v1/ml/anomaly/models/{filename}` - Delete saved anomaly model

### Health

- `GET /health` - Overall health check
- `GET /health/liveness` - Liveness probe

### System

- `GET /` - Basic hello endpoint
- `GET /info` - System information
- `GET /v1/system/version-check` - Check for CLI updates
- `GET /v1/system/disk` - Get disk space information

## Chat API

### Send Chat Message (OpenAI-Compatible)

Send a chat message to the LLM. This endpoint is compatible with OpenAI's chat completion API.

**Endpoint:** `POST /v1/projects/{namespace}/{project}/chat/completions`

**Headers:**

- `X-Session-ID` (optional): Session ID for stateful conversations. If not provided, a new session is created.
- `X-No-Session` (optional): Set to any value for stateless mode (no conversation history)

**Request Body:**

```json
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "What is LlamaFarm?"
    }
  ],
  "stream": false,
  "temperature": 0.7,
  "max_tokens": 1000,
  "rag_enabled": true,
  "database": "main_db",
  "rag_top_k": 5,
  "rag_score_threshold": 0.7
}
```

**Request Fields:**

- `messages` (required): Array of chat messages with `role` and `content`
- `model` (optional): Select which model to use (OpenAI-compatible, added in PR #263 multi-model support)
- `stream` (optional): Enable streaming responses (Server-Sent Events)
- `temperature` (optional): Sampling temperature (0.0-2.0)
- `max_tokens` (optional): Maximum tokens to generate for the **answer** (thinking tokens are separate)
- `top_p` (optional): Nucleus sampling parameter
- `top_k` (optional): Top-k sampling parameter
- `rag_enabled` (optional): Enable/disable RAG (uses config default if not specified)
- `database` (optional): Database to use for RAG queries
- `rag_top_k` (optional): Number of RAG results to retrieve
- `rag_score_threshold` (optional): Minimum similarity score for RAG results
- `rag_queries` (optional): Array of custom queries for RAG retrieval, overriding the user message. Can be a single query `["my query"]` or multiple queries `["query1", "query2"]` - results from multiple queries are executed concurrently, merged, and deduplicated
- `think` (optional): Enable thinking/reasoning mode for supported models like Qwen3 (default: `false`)
- `thinking_budget` (optional): Maximum tokens for thinking process when `think: true` (default: `1024`)

**Response (Non-Streaming):**

```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "llama3.2:3b",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "LlamaFarm is a framework for building AI applications..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 20,
    "completion_tokens": 100,
    "total_tokens": 120
  }
}
```

**Response Headers:**

- `X-Session-ID`: The session ID (only in stateful mode)

**Streaming Response:**

When `stream: true`, the response is sent as Server-Sent Events:

```
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"llama3.2:3b","choices":[{"index":0,"delta":{"role":"assistant","content":"Llama"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"llama3.2:3b","choices":[{"index":0,"delta":{"content":"Farm"},"finish_reason":null}]}

data: [DONE]
```

**Example (Non-Streaming):**

```bash
curl -X POST http://localhost:8000/v1/projects/my-org/chatbot/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'
```

**Example (Streaming):**

```bash
curl -X POST http://localhost:8000/v1/projects/my-org/chatbot/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "stream": true
  }'
```

**Example (Stateless):**

```bash
curl -X POST http://localhost:8000/v1/projects/my-org/chatbot/chat/completions \
  -H "Content-Type: application/json" \
  -H "X-No-Session: true" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'
```

**Example (With RAG):**

```bash
curl -X POST http://localhost:8000/v1/projects/my-org/chatbot/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "What are the FDA regulations?"}
    ],
    "rag_enabled": true,
    "database": "fda_db",
    "rag_top_k": 10
  }'
```

**Example (With Thinking/Reasoning):**

For models that support chain-of-thought reasoning (like Qwen3), enable thinking mode to see the model's reasoning process:

```bash
curl -X POST http://localhost:8000/v1/projects/my-org/chatbot/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "What is 15% of 85?"}
    ],
    "think": true,
    "thinking_budget": 512,
    "max_tokens": 200
  }'
```

**Response with Thinking:**

```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "Qwen3-1.7B",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "15% of 85 is **12.75**."
      },
      "finish_reason": "stop"
    }
  ],
  "thinking": {
    "content": "To find 15% of 85, I need to multiply 85 by 0.15. Let me calculate: 85 Ã— 0.15 = 12.75.",
    "tokens": null
  }
}
```

**Token Allocation with Thinking:**

- `max_tokens`: Controls the **answer** length (default: 512)
- `thinking_budget`: Controls the **thinking** length (default: 1024 when enabled)
- Total generation = `thinking_budget` + `max_tokens`

This ensures your answer isn't cut short by the thinking process.

**Example (Custom RAG Query):**

Override the default RAG query (which uses the user message) with a custom search query. This is useful when the user's question is conversational but you want specific technical retrieval:

```bash
curl -X POST http://localhost:8000/v1/projects/my-org/chatbot/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Can you summarize the key findings?"}
    ],
    "rag_enabled": true,
    "database": "research_db",
    "rag_queries": ["clinical trial results primary endpoints efficacy safety"]
  }'
```

**Example (Multiple Custom RAG Queries):**

Execute multiple search queries concurrently and merge the results. This is useful for comparative analysis or comprehensive retrieval:

```bash
curl -X POST http://localhost:8000/v1/projects/my-org/chatbot/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Compare the two approaches"}
    ],
    "rag_enabled": true,
    "database": "research_db",
    "rag_queries": [
      "machine learning neural network methodology",
      "traditional statistical analysis regression"
    ],
    "rag_top_k": 10
  }'
```

Results from multiple queries are automatically executed concurrently, merged, deduplicated by content, sorted by relevance score, and limited to `rag_top_k` total results.

### Get Chat History

Retrieve conversation history for a session.

**Endpoint:** `GET /v1/projects/{namespace}/{project}/chat/sessions/{session_id}/history`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `session_id` (path, required): Session ID

**Response:**

```json
{
  "messages": [
    {
      "role": "user",
      "content": "Hello!"
    },
    {
      "role": "assistant",
      "content": "Hi! How can I help you today?"
    }
  ]
}
```

**Example:**

```bash
curl http://localhost:8000/v1/projects/my-org/chatbot/chat/sessions/abc-123/history
```

### Delete Chat Session

Delete a specific chat session and its history.

**Endpoint:** `DELETE /v1/projects/{namespace}/{project}/chat/sessions/{session_id}`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `session_id` (path, required): Session ID

**Response:**

```json
{
  "message": "Session abc-123 deleted"
}
```

**Example:**

```bash
curl -X DELETE http://localhost:8000/v1/projects/my-org/chatbot/chat/sessions/abc-123
```

### Delete All Chat Sessions

Delete all chat sessions for a project.

**Endpoint:** `DELETE /v1/projects/{namespace}/{project}/chat/sessions`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name

**Response:**

```json
{
  "message": "Deleted 5 session(s)",
  "count": 5
}
```

**Example:**

```bash
curl -X DELETE http://localhost:8000/v1/projects/my-org/chatbot/chat/sessions
```

### List Available Models

List all configured models for a project.

**Endpoint:** `GET /v1/projects/{namespace}/{project}/models`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name

**Response:**

```json
{
  "total": 2,
  "models": [
    {
      "name": "fast-model",
      "provider": "ollama",
      "model": "llama3.2:3b",
      "base_url": "http://localhost:11434/v1",
      "default": true,
      "description": "Fast model for quick responses"
    },
    {
      "name": "smart-model",
      "provider": "ollama",
      "model": "llama3.2:70b",
      "base_url": "http://localhost:11434/v1",
      "default": false,
      "description": "Larger model for complex tasks"
    }
  ]
}
```

**Example:**

```bash
curl http://localhost:8000/v1/projects/my-org/chatbot/models
```

---

## Datasets API

### List Datasets

List all datasets in a project.

**Endpoint:** `GET /v1/projects/{namespace}/{project}/datasets`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `include_extra_details` (query, optional): Include detailed file information (default: true)

**Response:**

```json
{
  "total": 2,
  "datasets": [
    {
      "name": "research_papers",
      "database": "main_db",
      "data_processing_strategy": "universal_processor",
      "files": ["abc123", "def456"],
      "details": {
        "total_files": 2,
        "total_size_bytes": 1048576,
        "file_details": [
          {
            "hash": "abc123",
            "original_filename": "paper1.pdf",
            "size": 524288,
            "timestamp": 1677652288.0
          }
        ]
      }
    }
  ]
}
```

**Example:**

```bash
curl http://localhost:8000/v1/projects/my-org/chatbot/datasets
```

### Get Available Strategies

Get available data processing strategies and databases for a project.

**Endpoint:** `GET /v1/projects/{namespace}/{project}/datasets/strategies`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name

**Response:**

```json
{
  "data_processing_strategies": ["universal_processor", "custom_strategy"],
  "databases": ["main_db", "research_db"]
}
```

**Example:**

```bash
curl http://localhost:8000/v1/projects/my-org/chatbot/datasets/strategies
```

### Create Dataset

Create a new dataset.

**Endpoint:** `POST /v1/projects/{namespace}/{project}/datasets`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name

**Request Body:**

```json
{
  "name": "research_papers",
  "data_processing_strategy": "universal_processor",
  "database": "main_db"
}
```

**Response:**

```json
{
  "dataset": {
    "name": "research_papers",
    "database": "main_db",
    "data_processing_strategy": "universal_processor",
    "files": []
  }
}
```

**Example:**

```bash
curl -X POST http://localhost:8000/v1/projects/my-org/chatbot/datasets \
  -H "Content-Type: application/json" \
  -d '{
    "name": "research_papers",
    "data_processing_strategy": "universal_processor",
    "database": "main_db"
  }'
```

### Delete Dataset

Delete a dataset.

**Endpoint:** `DELETE /v1/projects/{namespace}/{project}/datasets/{dataset}`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `dataset` (path, required): Dataset name

**Response:**

```json
{
  "dataset": {
    "name": "research_papers",
    "database": "main_db",
    "data_processing_strategy": "universal_processor",
    "files": []
  }
}
```

**Example:**

```bash
curl -X DELETE http://localhost:8000/v1/projects/my-org/chatbot/datasets/research_papers
```

### Upload File to Dataset

Upload a file to a dataset (stores the file but does not process it).

**Endpoint:** `POST /v1/projects/{namespace}/{project}/datasets/{dataset}/data`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `dataset` (path, required): Dataset name

**Request:**

- Content-Type: `multipart/form-data`
- Body: File upload with field name `file`

**Response:**

```json
{
  "filename": "paper1.pdf",
  "hash": "abc123def456",
  "processed": false
}
```

**Example:**

```bash
curl -X POST http://localhost:8000/v1/projects/my-org/chatbot/datasets/research_papers/data \
  -F "file=@paper1.pdf"
```

### Process Dataset

Processing is now driven exclusively through the dataset actions endpoint, which queues Celery tasks and returns a task ID you can poll later.

**Endpoint:** `POST /v1/projects/{namespace}/{project}/datasets/{dataset}/actions`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `dataset` (path, required): Dataset name
- `action_type` (body, required): `"process"` (alias `"ingest"`)

**Request Body:**

```json
{
  "action_type": "process"
}
```

**Response:**

```json
{
  "message": "Accepted",
  "task_uri": "http://localhost:8000/v1/projects/my-org/chatbot/tasks/8f6f9c2a",
  "task_id": "8f6f9c2a"
}
```

Use `task_uri`/`task_id` with `GET /v1/projects/{namespace}/{project}/tasks/{task_id}` to monitor progress. When the Celery task finishes, the `result` payload matches the historical `ProcessDatasetResponse` structure (processed/skipped/failed counts plus per-file details).

**Example:**

```bash
curl -X POST http://localhost:8000/v1/projects/my-org/chatbot/datasets/research_papers/actions \
  -H "Content-Type: application/json" \
  -d '{"action_type":"process"}'
```

### Remove File from Dataset

Remove a file from a dataset.

**Endpoint:** `DELETE /v1/projects/{namespace}/{project}/datasets/{dataset}/data/{file_hash}`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `dataset` (path, required): Dataset name
- `file_hash` (path, required): Hash of the file to remove
- `remove_from_disk` (query, optional): Also delete the file from disk (default: false)

**Response:**

```json
{
  "file_hash": "abc123"
}
```

**Example:**

```bash
curl -X DELETE http://localhost:8000/v1/projects/my-org/chatbot/datasets/research_papers/data/abc123
```

**Example (Remove from disk):**

```bash
curl -X DELETE "http://localhost:8000/v1/projects/my-org/chatbot/datasets/research_papers/data/abc123?remove_from_disk=true"
```

---

## RAG API

### Query RAG System

Perform a semantic search query against a RAG database.

**Endpoint:** `POST /v1/projects/{namespace}/{project}/rag/query`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name

**Request Body:**

```json
{
  "query": "What are the clinical trial requirements?",
  "database": "fda_db",
  "top_k": 5,
  "score_threshold": 0.7,
  "retrieval_strategy": "hybrid",
  "metadata_filters": {
    "document_type": "regulation"
  }
}
```

**Request Fields:**

- `query` (required): The search query text
- `database` (optional): Database name (uses default if not specified)
- `top_k` (optional): Number of results to return (default: 5)
- `score_threshold` (optional): Minimum similarity score
- `retrieval_strategy` (optional): Strategy to use for retrieval
- `metadata_filters` (optional): Filter results by metadata
- `distance_metric` (optional): Distance metric to use
- `hybrid_alpha` (optional): Alpha parameter for hybrid search
- `rerank_model` (optional): Model to use for reranking
- `query_expansion` (optional): Enable query expansion
- `max_tokens` (optional): Maximum tokens in results

**Response:**

```json
{
  "query": "What are the clinical trial requirements?",
  "results": [
    {
      "content": "Clinical trials must follow FDA regulations...",
      "score": 0.92,
      "metadata": {
        "document_id": "fda_21cfr312",
        "page": 5,
        "document_type": "regulation"
      },
      "chunk_id": "chunk_123",
      "document_id": "fda_21cfr312"
    }
  ],
  "total_results": 5,
  "processing_time_ms": 45.2,
  "retrieval_strategy_used": "hybrid",
  "database_used": "fda_db"
}
```

**Example:**

```bash
curl -X POST http://localhost:8000/v1/projects/my-org/chatbot/rag/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are the clinical trial requirements?",
    "database": "fda_db",
    "top_k": 5
  }'
```

### List RAG Databases

List all configured RAG databases and their associated strategies for a project.

**Endpoint:** `GET /v1/projects/{namespace}/{project}/rag/databases`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name

**Response:**

```json
{
  "databases": [
    {
      "name": "main_db",
      "type": "ChromaStore",
      "is_default": true,
      "embedding_strategies": [
        {
          "name": "default_embeddings",
          "type": "OllamaEmbedder",
          "priority": 0,
          "is_default": true
        }
      ],
      "retrieval_strategies": [
        {
          "name": "basic_search",
          "type": "BasicSimilarityStrategy",
          "is_default": true
        }
      ]
    }
  ],
  "default_database": "main_db"
}
```

**Example:**

```bash
curl http://localhost:8000/v1/projects/my-org/chatbot/rag/databases
```

### Get Database Details

Get detailed information about a specific RAG database including its configuration and dependent datasets.

**Endpoint:** `GET /v1/projects/{namespace}/{project}/rag/databases/{database_name}`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `database_name` (path, required): Name of the database

**Response:**

```json
{
  "name": "main_db",
  "type": "ChromaStore",
  "config": {
    "collection_name": "documents",
    "distance_function": "cosine"
  },
  "embedding_strategies": [
    {
      "name": "default_embeddings",
      "type": "OllamaEmbedder",
      "config": {
        "model": "nomic-embed-text",
        "dimension": 768
      },
      "priority": 0
    }
  ],
  "retrieval_strategies": [
    {
      "name": "basic_search",
      "type": "BasicSimilarityStrategy",
      "config": { "top_k": 10 },
      "default": true
    }
  ],
  "default_embedding_strategy": "default_embeddings",
  "default_retrieval_strategy": "basic_search",
  "dependent_datasets": ["research_papers", "documentation"]
}
```

**Example:**

```bash
curl http://localhost:8000/v1/projects/my-org/chatbot/rag/databases/main_db
```

### Create Database

Create a new RAG database in the project configuration.

**Endpoint:** `POST /v1/projects/{namespace}/{project}/rag/databases`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name

**Request Body:**

```json
{
  "name": "new_database",
  "type": "ChromaStore",
  "config": {
    "collection_name": "my_collection",
    "distance_function": "cosine"
  },
  "embedding_strategies": [
    {
      "name": "embeddings",
      "type": "OllamaEmbedder",
      "config": {
        "model": "nomic-embed-text",
        "dimension": 768
      }
    }
  ],
  "retrieval_strategies": [
    {
      "name": "basic_search",
      "type": "BasicSimilarityStrategy",
      "config": { "top_k": 10 },
      "default": true
    }
  ]
}
```

**Response (201 Created):**

```json
{
  "database": {
    "name": "new_database",
    "type": "ChromaStore",
    "is_default": false,
    "embedding_strategies": [...],
    "retrieval_strategies": [...]
  }
}
```

**Example:**

```bash
curl -X POST http://localhost:8000/v1/projects/my-org/chatbot/rag/databases \
  -H "Content-Type: application/json" \
  -d '{
    "name": "new_database",
    "type": "ChromaStore",
    "embedding_strategies": [
      {"name": "embeddings", "type": "OllamaEmbedder", "config": {"model": "nomic-embed-text"}}
    ],
    "retrieval_strategies": [
      {"name": "basic", "type": "BasicSimilarityStrategy", "config": {}, "default": true}
    ]
  }'
```

### Update Database

Update a RAG database's mutable fields. Note: `name` and `type` are immutable.

**Endpoint:** `PATCH /v1/projects/{namespace}/{project}/rag/databases/{database_name}`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `database_name` (path, required): Name of the database

**Request Body (all fields optional):**

```json
{
  "config": {
    "distance_function": "euclidean"
  },
  "embedding_strategies": [...],
  "retrieval_strategies": [...],
  "default_embedding_strategy": "new_default",
  "default_retrieval_strategy": "reranked_search"
}
```

**Response:**

```json
{
  "name": "main_db",
  "type": "ChromaStore",
  "config": {...},
  "embedding_strategies": [...],
  "retrieval_strategies": [...],
  "default_embedding_strategy": "new_default",
  "default_retrieval_strategy": "reranked_search",
  "dependent_datasets": []
}
```

**Example - Add a reranking strategy:**

```bash
curl -X PATCH http://localhost:8000/v1/projects/my-org/chatbot/rag/databases/main_db \
  -H "Content-Type: application/json" \
  -d '{
    "retrieval_strategies": [
      {"name": "basic_search", "type": "BasicSimilarityStrategy", "config": {"top_k": 10}},
      {"name": "reranked_search", "type": "CrossEncoderRerankedStrategy", "config": {"model_name": "reranker", "initial_k": 30}}
    ],
    "default_retrieval_strategy": "reranked_search"
  }'
```

### Delete Database

Delete a RAG database from the project. Fails if any datasets depend on this database.

**Endpoint:** `DELETE /v1/projects/{namespace}/{project}/rag/databases/{database_name}`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `database_name` (path, required): Name of the database
- `delete_collection` (query, optional): Whether to delete the underlying vector store collection. Set to `false` to only remove from config. Default: `true`

**Response (200 OK):**

```json
{
  "message": "Database 'old_db' deleted successfully",
  "database": {
    "name": "old_db",
    "type": "ChromaStore",
    ...
  },
  "collection_deleted": true
}
```

**Error Response (409 Conflict - has dependent datasets):**

```json
{
  "detail": "Cannot delete database 'main_db': 2 dataset(s) depend on it. Delete or reassign these datasets first: ['dataset1', 'dataset2']"
}
```

**Example:**

```bash
# Delete database and its collection
curl -X DELETE http://localhost:8000/v1/projects/my-org/chatbot/rag/databases/old_db

# Only remove from config, keep the vector store data
curl -X DELETE "http://localhost:8000/v1/projects/my-org/chatbot/rag/databases/old_db?delete_collection=false"
```

### Check RAG Health

Get health status of the RAG system and databases.

**Endpoint:** `GET /v1/projects/{namespace}/{project}/rag/health`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `database` (query, optional): Specific database to check (uses default if not specified)

**Response:**

```json
{
  "status": "healthy",
  "database": "main_db",
  "components": {
    "vector_store": {
      "name": "vector_store",
      "status": "healthy",
      "latency": 15.2,
      "message": "Vector store operational"
    },
    "embeddings": {
      "name": "embeddings",
      "status": "healthy",
      "latency": 8.5,
      "message": "Embedding service operational"
    }
  },
  "last_check": "2024-01-15T10:30:00Z",
  "issues": null
}
```

**Response Fields:**

- `status`: Overall health status (`healthy`, `degraded`, `unhealthy`)
- `database`: Database that was checked
- `components`: Individual component health checks
- `last_check`: Timestamp of the health check
- `issues`: Array of issues if any problems detected

**Component Health:**

- `name`: Component identifier
- `status`: Component status (`healthy`, `degraded`, `unhealthy`)
- `latency`: Response time in milliseconds
- `message`: Optional status message

**Example:**

```bash
curl http://localhost:8000/v1/projects/my-org/chatbot/rag/health
```

**Example (Specific database):**

```bash
curl "http://localhost:8000/v1/projects/my-org/chatbot/rag/health?database=main_db"
```

### Get RAG Statistics

Get statistics for a RAG database including vector counts and storage usage.

**Endpoint:** `GET /v1/projects/{namespace}/{project}/rag/stats`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `database` (query, optional): Specific database to get stats for (uses default if not specified)

**Response:**

```json
{
  "database": "main_db",
  "vector_count": 1250,
  "storage_bytes": 52428800,
  "storage_human": "50 MB",
  "embedding_dimension": 768,
  "collection_name": "documents",
  "last_updated": "2024-01-15T10:30:00Z"
}
```

**Example:**

```bash
curl http://localhost:8000/v1/projects/my-org/chatbot/rag/stats
```

**Example (Specific database):**

```bash
curl "http://localhost:8000/v1/projects/my-org/chatbot/rag/stats?database=research_db"
```

### List Documents in Database

List all documents stored in a RAG database with their metadata.

**Endpoint:** `GET /v1/projects/{namespace}/{project}/rag/databases/{database_name}/documents`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `database_name` (path, required): Name of the database
- `limit` (query, optional): Maximum documents to return (1-1000, default: 50)

**Response:**

```json
[
  {
    "id": "abc123def456",
    "filename": "clinical_trial_report.pdf",
    "chunk_count": 45,
    "size_bytes": 1048576,
    "parser_used": "PDFParser_LlamaIndex",
    "date_ingested": "2024-01-15T10:30:00Z",
    "metadata": {
      "document_type": "report",
      "source_dataset": "research_papers"
    }
  },
  {
    "id": "789xyz",
    "filename": "fda_guidelines.pdf",
    "chunk_count": 128,
    "size_bytes": 2097152,
    "parser_used": "PDFParser_PyPDF2",
    "date_ingested": "2024-01-14T15:20:00Z",
    "metadata": null
  }
]
```

**Example:**

```bash
curl http://localhost:8000/v1/projects/my-org/chatbot/rag/databases/main_db/documents
```

**Example (With limit):**

```bash
curl "http://localhost:8000/v1/projects/my-org/chatbot/rag/databases/main_db/documents?limit=100"
```

---

## Tasks API

### Get Task Status

Get the status of an asynchronous task.

**Endpoint:** `GET /v1/projects/{namespace}/{project}/tasks/{task_id}`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `task_id` (path, required): Task ID returned from async operations

**Response:**

```json
{
  "task_id": "task-123-abc",
  "state": "SUCCESS",
  "meta": {
    "current": 2,
    "total": 2
  },
  "result": {
    "processed_files": 2,
    "failed_files": 0
  },
  "error": null,
  "traceback": null
}
```

**Task States:**

- `PENDING` - Task is queued
- `STARTED` - Task is running
- `SUCCESS` - Task completed successfully
- `FAILURE` - Task failed with error
- `RETRY` - Task is being retried

**Example:**

```bash
curl http://localhost:8000/v1/projects/my-org/chatbot/tasks/task-123-abc
```

### Cancel Task

Cancel a running task and revert any files that were successfully processed.

**Endpoint:** `DELETE /v1/projects/{namespace}/{project}/tasks/{task_id}`

**Parameters:**
- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `task_id` (path, required): Task ID to cancel

**Description:**

This endpoint is primarily used for cancelling dataset processing operations. When a task is cancelled:
1. All pending Celery tasks are revoked (prevented from starting)
2. Running tasks are gracefully stopped (current work finishes)
3. The task is marked as cancelled in the backend
4. Any files that were successfully processed have their chunks removed from the vector store

**Response:**
```json
{
  "message": "Task cancelled and 3 file(s) reverted",
  "task_id": "task-123-abc",
  "cancelled": true,
  "pending_tasks_cancelled": 5,
  "running_tasks_at_cancel": 2,
  "files_reverted": 3,
  "files_failed_to_revert": 0,
  "errors": null,
  "already_completed": false,
  "already_cancelled": false
}
```

**Response Fields:**
- `message` - Human-readable status message
- `task_id` - The ID of the cancelled task
- `cancelled` - Whether the task was successfully cancelled
- `pending_tasks_cancelled` - Number of queued tasks that were stopped
- `running_tasks_at_cancel` - Number of tasks that were running when cancelled
- `files_reverted` - Number of files whose chunks were successfully removed
- `files_failed_to_revert` - Number of files that failed to clean up
- `errors` - Array of cleanup errors (if any)
- `already_completed` - True if task had already completed before cancellation
- `already_cancelled` - True if task was already cancelled

**Edge Cases:**

**Task Already Completed:**
```json
{
  "message": "Task already success",
  "task_id": "task-123-abc",
  "cancelled": false,
  "already_completed": true,
  ...
}
```

**Task Already Cancelled:**
```json
{
  "message": "Task already cancelled",
  "task_id": "task-123-abc",
  "cancelled": true,
  "already_cancelled": true,
  "files_reverted": 3,
  ...
}
```

**Cleanup Failures:**
```json
{
  "message": "Task cancelled with cleanup issues: 3 reverted, 1 failed",
  "task_id": "task-123-abc",
  "cancelled": true,
  "files_reverted": 3,
  "files_failed_to_revert": 1,
  "errors": [
    {
      "file_hash": "abc123def456",
      "error": "Vector store connection timeout"
    }
  ],
  ...
}
```

**Example:**
```bash
# Cancel a running dataset processing task
curl -X DELETE http://localhost:8000/v1/projects/my-org/chatbot/tasks/task-123-abc
```

**HTTP Status Codes:**
- `200 OK` - Task cancellation succeeded (or task already completed/cancelled)
- `404 Not Found` - Task not found or not a cancellable task type
- `500 Internal Server Error` - Server error during cancellation

**Notes:**
- Only group tasks (dataset processing) can be cancelled
- **Security:** Tasks can only be cancelled by the namespace/project they belong to
- Cancellation is idempotent (safe to call multiple times)
- Cleanup failures don't prevent cancellation from succeeding
- Successfully processed files are automatically reverted
- Manual cleanup is available via `POST /v1/projects/{namespace}/{project}/datasets/{dataset}/cleanup/{file_hash}` if automatic cleanup fails

---

## Examples API

### List Examples

List all available example projects.

**Endpoint:** `GET /v1/examples`

**Response:**

```json
{
  "examples": [
    {
      "id": "fda_rag",
      "slug": "fda_rag",
      "title": "FDA RAG Example",
      "description": "Example using FDA warning letters",
      "primaryModel": "llama3.2:3b",
      "tags": ["rag", "healthcare"],
      "dataset_count": 1,
      "data_size_bytes": 2097152,
      "data_size_human": "2.0MB",
      "project_size_bytes": 4096,
      "project_size_human": "4.0KB",
      "updated_at": "2024-01-15T10:30:00"
    }
  ]
}
```

**Example:**

```bash
curl http://localhost:8000/v1/examples
```

### Import Example as Project

Import an example as a new project.

**Endpoint:** `POST /v1/examples/{example_id}/import-project`

**Parameters:**

- `example_id` (path, required): Example ID to import

**Request Body:**

```json
{
  "namespace": "my-org",
  "name": "my-fda-project",
  "process": true
}
```

**Response:**

```json
{
  "project": "my-fda-project",
  "namespace": "my-org",
  "datasets": ["fda_letters"],
  "task_ids": ["task-123-abc"]
}
```

**Example:**

```bash
curl -X POST http://localhost:8000/v1/examples/fda_rag/import-project \
  -H "Content-Type: application/json" \
  -d '{
    "namespace": "my-org",
    "name": "my-fda-project",
    "process": true
  }'
```

### Import Example Data

Import example data into an existing project.

**Endpoint:** `POST /v1/examples/{example_id}/import-data`

**Parameters:**

- `example_id` (path, required): Example ID to import data from

**Request Body:**

```json
{
  "namespace": "my-org",
  "project": "my-project",
  "include_strategies": true,
  "process": true
}
```

**Response:**

```json
{
  "project": "my-project",
  "namespace": "my-org",
  "datasets": ["fda_letters"],
  "task_ids": ["task-456-def"]
}
```

**Example:**

```bash
curl -X POST http://localhost:8000/v1/examples/fda_rag/import-data \
  -H "Content-Type: application/json" \
  -d '{
    "namespace": "my-org",
    "project": "my-project",
    "include_strategies": true,
    "process": true
  }'
```

---

## Health API

### Overall Health Check

Check overall system health.

**Endpoint:** `GET /health`

**Response:**

```json
{
  "status": "healthy",
  "services": {
    "api": "healthy",
    "celery": "healthy",
    "redis": "healthy"
  }
}
```

**Example:**

```bash
curl http://localhost:8000/health
```

### Liveness Probe

Simple liveness check for container orchestration.

**Endpoint:** `GET /health/liveness`

**Response:**

```json
{
  "status": "alive"
}
```

**Example:**

```bash
curl http://localhost:8000/health/liveness
```

---

## System Info

### Root Endpoint

Basic hello endpoint.

**Endpoint:** `GET /`

**Response:**

```json
{
  "message": "Hello, World!"
}
```

### System Information

Get system version and configuration info.

**Endpoint:** `GET /info`

**Response:**

```json
{
  "version": "0.1.0",
  "data_directory": "/Users/username/.llamafarm"
}
```

**Example:**

```bash
curl http://localhost:8000/info
```

### Check for CLI Updates

Check if a newer version of the CLI is available.

**Endpoint:** `GET /v1/system/version-check`

**Response:**

```json
{
  "current_version": "0.0.17",
  "latest_version": "0.0.18",
  "name": "v0.0.18",
  "release_notes": "### Features\n- New feature X\n- Improved Y",
  "release_url": "https://github.com/llama-farm/llamafarm/releases/tag/v0.0.18",
  "published_at": "2024-01-15T10:30:00Z",
  "from_cache": false,
  "install": {
    "mac_linux": "curl -fsSL https://raw.githubusercontent.com/llama-farm/llamafarm/main/install.sh | bash",
    "windows": "winget install LlamaFarm.CLI"
  }
}
```

**Example:**

```bash
curl http://localhost:8000/v1/system/version-check
```

### Get Disk Space

Get disk space information for the HuggingFace cache and system disk.

**Endpoint:** `GET /v1/system/disk`

**Response:**

```json
{
  "cache": {
    "total_bytes": 500000000000,
    "used_bytes": 200000000000,
    "free_bytes": 300000000000,
    "path": "/Users/username/.cache/huggingface",
    "percent_free": 60.0
  },
  "system": {
    "total_bytes": 1000000000000,
    "used_bytes": 400000000000,
    "free_bytes": 600000000000,
    "path": "/",
    "percent_free": 60.0
  }
}
```

**Response Fields:**

- `cache`: Disk info for the HuggingFace cache directory (where models are stored)
- `system`: Disk info for the system root directory
- Each contains: `total_bytes`, `used_bytes`, `free_bytes`, `path`, `percent_free`

**Example:**

```bash
curl http://localhost:8000/v1/system/disk
```

---

## Event Logs API

The Event Logs API provides observability into project operations including inference calls, RAG processing, and other events.

### List Event Logs

List event logs for a project with optional filtering.

**Endpoint:** `GET /v1/projects/{namespace}/{project}/event_logs`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `type` (query, optional): Filter by event type (e.g., "inference", "rag_processing")
- `start_time` (query, optional): Filter events after this timestamp (ISO 8601 format)
- `end_time` (query, optional): Filter events before this timestamp (ISO 8601 format)
- `limit` (query, optional): Maximum number of events to return (1-100, default: 10)
- `offset` (query, optional): Number of events to skip for pagination

**Response:**

```json
{
  "total": 42,
  "events": [
    {
      "event_id": "evt_20240115_103000_inference_abc123",
      "type": "inference",
      "timestamp": "2024-01-15T10:30:00Z",
      "summary": {
        "model": "llama3.2:3b",
        "tokens": 150,
        "duration_ms": 1200
      }
    }
  ],
  "limit": 10,
  "offset": 0
}
```

**Example:**

```bash
# List recent events
curl http://localhost:8000/v1/projects/my-org/chatbot/event_logs

# Filter by type with pagination
curl "http://localhost:8000/v1/projects/my-org/chatbot/event_logs?type=inference&limit=20"

# Filter by time range
curl "http://localhost:8000/v1/projects/my-org/chatbot/event_logs?start_time=2024-01-15T00:00:00Z"
```

### Get Event Details

Get full details of a specific event including all sub-events.

**Endpoint:** `GET /v1/projects/{namespace}/{project}/event_logs/{event_id}`

**Parameters:**

- `namespace` (path, required): Project namespace
- `project` (path, required): Project name
- `event_id` (path, required): Event ID

**Response:**

```json
{
  "event_id": "evt_20240115_103000_inference_abc123",
  "type": "inference",
  "timestamp": "2024-01-15T10:30:00Z",
  "data": {
    "model": "llama3.2:3b",
    "messages": [...],
    "response": {...},
    "tokens": {
      "prompt": 50,
      "completion": 100,
      "total": 150
    },
    "duration_ms": 1200
  },
  "sub_events": [...]
}
```

**Example:**

```bash
curl http://localhost:8000/v1/projects/my-org/chatbot/event_logs/evt_20240115_103000_inference_abc123
```

---

## Models Cache API

The Models Cache API allows you to manage locally cached models (primarily HuggingFace models used by Universal Runtime).

### List Cached Models

List all models cached on disk.

**Endpoint:** `GET /v1/models`

**Parameters:**

- `provider` (query, optional): Model provider (default: "universal")

**Response:**

```json
{
  "data": [
    {
      "model_id": "cross-encoder/ms-marco-MiniLM-L-6-v2",
      "size_bytes": 90000000,
      "size_human": "90MB",
      "last_modified": "2024-01-15T10:30:00Z",
      "revisions": ["main"]
    }
  ]
}
```

**Example:**

```bash
curl http://localhost:8000/v1/models
```

### Download/Cache Model

Download and cache a model. Returns a streaming response with progress events.

**Endpoint:** `POST /v1/models/download`

**Request Body:**

```json
{
  "provider": "universal",
  "model_name": "cross-encoder/ms-marco-MiniLM-L-6-v2"
}
```

**Response:** Server-Sent Events stream with progress updates:

```
data: {"event": "progress", "downloaded": 45000000, "total": 90000000, "percent": 50}

data: {"event": "complete", "model_name": "cross-encoder/ms-marco-MiniLM-L-6-v2"}
```

**Example:**

```bash
curl -X POST http://localhost:8000/v1/models/download \
  -H "Content-Type: application/json" \
  -d '{"model_name": "cross-encoder/ms-marco-MiniLM-L-6-v2"}'
```

### Validate Download

Check if there's sufficient disk space for a model download before starting.

**Endpoint:** `POST /v1/models/validate-download`

**Request Body:**

```json
{
  "model_name": "unsloth/Qwen3-1.7B-GGUF"
}
```

**Response:**

```json
{
  "can_download": true,
  "warning": false,
  "message": "Sufficient disk space available",
  "available_bytes": 107374182400,
  "required_bytes": 1073741824,
  "cache_info": {
    "total_bytes": 500000000000,
    "used_bytes": 200000000000,
    "free_bytes": 300000000000,
    "path": "/Users/username/.cache/huggingface",
    "percent_free": 60.0
  },
  "system_info": {
    "total_bytes": 1000000000000,
    "used_bytes": 400000000000,
    "free_bytes": 600000000000,
    "path": "/",
    "percent_free": 60.0
  }
}
```

**Response Fields:**

- `can_download`: Whether download can proceed (false if critically low space)
- `warning`: Whether space is low but download can proceed
- `message`: Human-readable status message
- `available_bytes`: Available disk space in bytes
- `required_bytes`: Estimated space required for download
- `cache_info`: Disk info for HuggingFace cache location
- `system_info`: Disk info for system root

**Example:**

```bash
curl -X POST http://localhost:8000/v1/models/validate-download \
  -H "Content-Type: application/json" \
  -d '{"model_name": "unsloth/Qwen3-1.7B-GGUF"}'
```

### Get GGUF Quantization Options

List all available GGUF quantization options for a model with file sizes.

**Endpoint:** `GET /v1/models/{model_id}/quantizations`

**Parameters:**

- `model_id` (path, required): HuggingFace model identifier (e.g., "unsloth/Qwen3-1.7B-GGUF")

**Response:**

```json
{
  "options": [
    {
      "filename": "Qwen3-1.7B-Q4_K_M.gguf",
      "quantization": "Q4_K_M",
      "size_bytes": 1073741824,
      "size_human": "1.0 GB"
    },
    {
      "filename": "Qwen3-1.7B-Q8_0.gguf",
      "quantization": "Q8_0",
      "size_bytes": 1879048192,
      "size_human": "1.75 GB"
    },
    {
      "filename": "Qwen3-1.7B-F16.gguf",
      "quantization": "F16",
      "size_bytes": 3489660928,
      "size_human": "3.25 GB"
    }
  ]
}
```

**Example:**

```bash
curl http://localhost:8000/v1/models/unsloth/Qwen3-1.7B-GGUF/quantizations
```

### Delete Cached Model

Delete a cached model from disk.

**Endpoint:** `DELETE /v1/models/{model_name}`

**Parameters:**

- `model_name` (path, required): The model identifier to delete
- `provider` (query, optional): Model provider (default: "universal")

**Response:**

```json
{
  "model_name": "cross-encoder/ms-marco-MiniLM-L-6-v2",
  "revisions_deleted": 1,
  "size_freed": 90000000,
  "path": "/Users/username/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L-6-v2"
}
```

**Example:**

```bash
curl -X DELETE "http://localhost:8000/v1/models/cross-encoder/ms-marco-MiniLM-L-6-v2"
```

---

## Multi-Model Support

As of PR #263, LlamaFarm supports multiple models per project with OpenAI-compatible `model` field in chat requests.

### Configuration

In your `llamafarm.yaml`:

```yaml
runtime:
  models:
    - name: fast-model
      provider: ollama
      model: llama3.2:3b
    - name: smart-model
      provider: ollama
      model: llama3.2:70b
  default_model: fast-model
```

### Using Different Models

The `model` field in chat completions requests is OpenAI-compatible and allows you to select which configured model to use:

```bash
curl -X POST http://localhost:8000/v1/projects/my-org/chatbot/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "smart-model",
    "messages": [
      {"role": "user", "content": "Explain quantum computing"}
    ]
  }'
```

If `model` is not specified, the `default_model` from your configuration is used. This makes LlamaFarm a drop-in replacement for OpenAI's API with the added benefit of model selection.

### Get Available Models

```bash
curl http://localhost:8000/v1/projects/my-org/chatbot/models
```

---

## Rate Limiting and Performance

### Concurrent Requests

The API handles concurrent requests efficiently:

- Chat sessions are thread-safe with internal locking
- Dataset processing can run asynchronously with Celery
- Multiple chat sessions can be active simultaneously

### Session Management

- Sessions expire after 30 minutes of inactivity
- Use `X-No-Session` header for stateless requests
- Session cleanup happens automatically

### Async Processing

For long-running operations (dataset processing):

1. POST to the dataset `actions` endpoint (`{"action_type":"process"}`) to queue a Celery task
2. Poll the task endpoint to check status
3. Retrieve final results when `state` is `SUCCESS`

---

## MCP (Model Context Protocol) Compatible Endpoints

LlamaFarm's API is compatible with the Model Context Protocol (MCP), allowing AI agents to interact with LlamaFarm programmatically. The following endpoints are tagged with `mcp` for easy discovery by MCP clients:

### MCP-Compatible Operations

**Project Management:**

- `GET /v1/projects/{namespace}` - List projects (operation ID: `projects_list`)
- `POST /v1/projects/{namespace}` - Create project (operation ID: `project_create`)
- `GET /v1/projects/{namespace}/{project}` - Get project (operation ID: `project_get`)
- `PUT /v1/projects/{namespace}/{project}` - Update project (operation ID: `project_update`)
- `DELETE /v1/projects/{namespace}/{project}` - Delete project (operation ID: `project_delete`)

**Model Management:**

- `GET /v1/projects/{namespace}/{project}/models` - List models (operation ID: `models_list`)

**Dataset Operations:**

- `GET /v1/projects/{namespace}/{project}/datasets` - List datasets (operation ID: `dataset_list`)
- `GET /v1/projects/{namespace}/{project}/datasets/strategies` - List strategies (operation ID: `dataset_strategies_list`)
- `POST /v1/projects/{namespace}/{project}/datasets` - Create dataset (operation ID: `dataset_create`)
- `DELETE /v1/projects/{namespace}/{project}/datasets/{dataset}` - Delete dataset (operation ID: `dataset_delete`)
- `POST /v1/projects/{namespace}/{project}/datasets/{dataset}/actions` - Dataset actions (operation ID: `dataset_actions`)
- `POST /v1/projects/{namespace}/{project}/datasets/{dataset}/data` - Upload data (operation ID: `dataset_data_upload`)

**RAG Operations:**

- `POST /v1/projects/{namespace}/{project}/rag/query` - Query RAG (operation ID: `rag_query`)
- `POST /v1/projects/{namespace}/{project}/rag/databases` - Create database (operation ID: `database_create`)
- `GET /v1/projects/{namespace}/{project}/rag/databases/{database}` - Get database (operation ID: `database_get`)
- `PATCH /v1/projects/{namespace}/{project}/rag/databases/{database}` - Update database (operation ID: `database_update`)
- `DELETE /v1/projects/{namespace}/{project}/rag/databases/{database}` - Delete database (operation ID: `database_delete`)

**Task Management:**

- `GET /v1/projects/{namespace}/{project}/tasks/{task_id}` - Get task status (operation ID: `task_get`)

### Using LlamaFarm with MCP Servers

You can configure LlamaFarm projects to expose tools through MCP servers, giving AI agents access to filesystems, databases, APIs, and custom business logic. See the [MCP documentation](../mcp/index.md) for configuration examples.

**Example: AI Agent with LlamaFarm MCP Tools**

```python
# AI agent can use LlamaFarm endpoints as tools
# through MCP protocol
{
  "tool": "llamafarm_rag_query",
  "arguments": {
    "query": "What are the clinical trial requirements?",
    "database": "fda_db",
    "top_k": 5
  }
}
```

---

## Best Practices

### Error Handling

Always check HTTP status codes and handle errors:

```bash
response=$(curl -s -w "\n%{http_code}" http://localhost:8000/v1/projects/my-org/chatbot)
http_code=$(echo "$response" | tail -n 1)
body=$(echo "$response" | head -n -1)

if [ "$http_code" -eq 200 ]; then
  echo "Success: $body"
else
  echo "Error ($http_code): $body"
fi
```

### Using RAG Effectively

1. **Create datasets first**: Upload and process files before querying
2. **Use appropriate top_k**: Start with 5-10 results
3. **Set score thresholds**: Filter low-quality results (e.g., 0.7)
4. **Test queries**: Use the RAG query endpoint to test retrieval before chat

### Chat Sessions

1. **Stateful conversations**: Reuse session IDs for context
2. **Stateless queries**: Use `X-No-Session` for one-off questions
3. **Clean up**: Delete old sessions to free resources
4. **History access**: Use history endpoint to debug conversations

### Dataset Processing

1. **Upload first, process later**: Separate ingestion from processing
2. **Use async for large datasets**: Enable async processing for >10 files
3. **Monitor task status**: Poll task endpoint for progress
4. **Handle duplicates**: System automatically skips duplicate files

---

## API Clients

### Python Example

```python

class LlamaFarmClient:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.session = requests.Session()

    def chat(self, namespace, project, message, session_id=None):
        url = f"{self.base_url}/v1/projects/{namespace}/{project}/chat/completions"
        headers = {}
        if session_id:
            headers["X-Session-ID"] = session_id

        response = self.session.post(
            url,
            headers=headers,
            json={
                "messages": [{"role": "user", "content": message}]
            }
        )
        response.raise_for_status()
        return response.json()

    def query_rag(self, namespace, project, query, database=None, top_k=5):
        url = f"{self.base_url}/v1/projects/{namespace}/{project}/rag/query"
        response = self.session.post(
            url,
            json={
                "query": query,
                "database": database,
                "top_k": top_k
            }
        )
        response.raise_for_status()
        return response.json()

# Usage
client = LlamaFarmClient()
result = client.chat("my-org", "chatbot", "Hello!")
print(result["choices"][0]["message"]["content"])
```

### JavaScript/TypeScript Example

```typescript
interface ChatMessage {
  role: "system" | "user" | "assistant";
  content: string;
}

interface ChatRequest {
  messages: ChatMessage[];
  stream?: boolean;
  rag_enabled?: boolean;
  database?: string;
}

class LlamaFarmClient {
  constructor(private baseUrl: string = "http://localhost:8000") {}

  async chat(
    namespace: string,
    project: string,
    messages: ChatMessage[],
    sessionId?: string
  ): Promise<any> {
    const headers: Record<string, string> = {
      "Content-Type": "application/json",
    };

    if (sessionId) {
      headers["X-Session-ID"] = sessionId;
    }

    const response = await fetch(
      `${this.baseUrl}/v1/projects/${namespace}/${project}/chat/completions`,
      {
        method: "POST",
        headers,
        body: JSON.stringify({ messages }),
      }
    );

    if (!response.ok) {
      throw new Error(`API error: ${response.status}`);
    }

    return response.json();
  }

  async queryRAG(
    namespace: string,
    project: string,
    query: string,
    database?: string,
    topK: number = 5
  ): Promise<any> {
    const response = await fetch(
      `${this.baseUrl}/v1/projects/${namespace}/${project}/rag/query`,
      {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          query,
          database,
          top_k: topK,
        }),
      }
    );

    if (!response.ok) {
      throw new Error(`API error: ${response.status}`);
    }

    return response.json();
  }
}

// Usage
const client = new LlamaFarmClient();
const result = await client.chat("my-org", "chatbot", [
  { role: "user", content: "Hello!" },
]);
console.log(result.choices[0].message.content);
```

---

## Troubleshooting

### Common Issues

**Problem:** `404 Not Found` when accessing project

- **Solution**: Verify namespace and project name are correct. List projects to confirm.

**Problem:** Chat returns empty or error

- **Solution**: Check that the model is configured correctly and Ollama is running.

**Problem:** RAG query returns no results

- **Solution**: Ensure dataset is processed and database exists. Check RAG health endpoint.

**Problem:** Dataset processing stuck

- **Solution**: Check Celery worker status. Use async processing and poll task endpoint.

**Problem:** Session not persisting

- **Solution**: Ensure you're passing `X-Session-ID` header and not using `X-No-Session`.

### Debugging Tips

1. **Check logs**: Server logs are written to stdout
2. **Verify configuration**: Use `GET /v1/projects/{namespace}/{project}` to inspect config
3. **Test health**: Use `/health` endpoint to verify services are running
4. **Inspect tasks**: For async operations, poll task endpoint for detailed error info
5. **Use curl verbose**: Add `-v` flag to curl for detailed request/response info

---

## Vision API (OCR & Document Extraction)

The Vision API provides OCR and document extraction capabilities through the main LlamaFarm API server. These endpoints proxy to the Universal Runtime, handling file uploads and base64 image conversion automatically.

**Base URL:** `http://localhost:8000/v1/vision`

### OCR Endpoint

Extract text from images and PDFs using multiple OCR backends.

**Endpoint:** `POST /v1/vision/ocr`

**Content-Type:** `multipart/form-data`

**Parameters:**

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `file` | file | No* | - | PDF or image file to process |
| `images` | string | No* | - | Base64-encoded images as JSON array |
| `model` | string | No | `surya` | OCR backend: `surya`, `easyocr`, `paddleocr`, `tesseract` |
| `languages` | string | No | `en` | Comma-separated language codes (e.g., `en,fr`) |
| `return_boxes` | boolean | No | `false` | Return bounding boxes for detected text |

*Either `file` or `images` must be provided.

**Supported File Types:** PDF, PNG, JPG, JPEG, GIF, WebP, BMP, TIFF

**Response:**

```json
{
  "object": "list",
  "data": [
    {
      "index": 0,
      "text": "Extracted text from the document...",
      "confidence": 0.95
    }
  ],
  "model": "surya",
  "usage": {"images_processed": 1}
}
```

**Example (File Upload):**

```bash
curl -X POST http://localhost:8000/v1/vision/ocr \
  -F "file=@document.pdf" \
  -F "model=easyocr" \
  -F "languages=en"
```

**Example (Base64 Images):**

```bash
curl -X POST http://localhost:8000/v1/vision/ocr \
  -F 'images=["data:image/png;base64,iVBORw0KGgo..."]' \
  -F "model=surya" \
  -F "languages=en"
```

### Document Extraction Endpoint

Extract structured data from documents using vision-language models.

**Endpoint:** `POST /v1/vision/documents/extract`

**Content-Type:** `multipart/form-data`

**Parameters:**

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `file` | file | No* | - | PDF or image file to process |
| `images` | string | No* | - | Base64-encoded images as JSON array |
| `model` | string | Yes | - | HuggingFace model ID (e.g., `naver-clova-ix/donut-base-finetuned-docvqa`) |
| `prompts` | string | No | - | Comma-separated prompts for VQA task |
| `task` | string | No | `extraction` | Task type: `extraction`, `vqa`, `classification` |

*Either `file` or `images` must be provided.

**Supported Models:**

| Model | Description |
|-------|-------------|
| `naver-clova-ix/donut-base-finetuned-cord-v2` | Receipt/invoice extraction |
| `naver-clova-ix/donut-base-finetuned-docvqa` | Document Q&A |
| `microsoft/layoutlmv3-base-finetuned-docvqa` | Document Q&A with layout |

**Response:**

```json
{
  "object": "list",
  "data": [
    {
      "index": 0,
      "confidence": 0.9,
      "text": "<s_docvqa><s_question>What is the total?</s_question><s_answer>$15.99</s_answer>",
      "fields": [
        {"key": "question", "value": "What is the total?", "confidence": 0.9},
        {"key": "answer", "value": "$15.99", "confidence": 0.9}
      ]
    }
  ],
  "model": "naver-clova-ix/donut-base-finetuned-docvqa",
  "task": "vqa",
  "usage": {"documents_processed": 1}
}
```

**Example (Document VQA with File Upload):**

```bash
curl -X POST http://localhost:8000/v1/vision/documents/extract \
  -F "file=@receipt.pdf" \
  -F "model=naver-clova-ix/donut-base-finetuned-docvqa" \
  -F "prompts=What is the store name?,What is the total amount?" \
  -F "task=vqa"
```

**Example (Extraction with Base64):**

```bash
curl -X POST http://localhost:8000/v1/vision/documents/extract \
  -F 'images=["data:image/png;base64,iVBORw0KGgo..."]' \
  -F "model=naver-clova-ix/donut-base-finetuned-cord-v2" \
  -F "task=extraction"
```

---

## ML API (Custom Classifiers & Anomaly Detection)

The ML API provides custom text classification and anomaly detection capabilities through the main LlamaFarm API server. These endpoints proxy to the Universal Runtime with automatic model versioning support.

**Base URL:** `http://localhost:8000/v1/ml`

**TIP: Model Versioning**
When `overwrite: false` (default), models are saved with timestamps like `my-model_20251215_155054`. Use the `-latest` suffix (e.g., `my-model-latest`) to automatically resolve to the newest version.

### Custom Text Classification (SetFit)

Train custom classifiers with as few as 8-16 examples per class using SetFit (Sentence Transformer Fine-tuning).

#### Fit Classifier

Train a new text classifier.

**Endpoint:** `POST /v1/ml/classifier/fit`

**Request Body:**

```json
{
  "model": "intent-classifier",
  "base_model": "sentence-transformers/all-MiniLM-L6-v2",
  "training_data": [
    {"text": "I need to book a flight to NYC", "label": "booking"},
    {"text": "Reserve a hotel room for me", "label": "booking"},
    {"text": "Cancel my reservation please", "label": "cancellation"},
    {"text": "I want to cancel my booking", "label": "cancellation"},
    {"text": "What time does the flight leave?", "label": "inquiry"},
    {"text": "How much does it cost?", "label": "inquiry"}
  ],
  "num_iterations": 20,
  "batch_size": 16,
  "overwrite": false
}
```

**Request Fields:**

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `model` | string | Yes | - | Base name for the classifier |
| `base_model` | string | No | `all-MiniLM-L6-v2` | Sentence transformer to fine-tune |
| `training_data` | array | Yes | - | List of `{"text", "label"}` objects |
| `num_iterations` | int | No | 20 | Contrastive learning iterations |
| `batch_size` | int | No | 16 | Training batch size |
| `overwrite` | bool | No | false | If false, version with timestamp |

**Response:**

```json
{
  "object": "fit_result",
  "model": "intent-classifier-test_20260102_202450",
  "base_model": "sentence-transformers/all-MiniLM-L6-v2",
  "samples_fitted": 40,
  "num_classes": 4,
  "labels": ["booking", "cancellation", "complaint", "inquiry"],
  "training_time_ms": 6751.66,
  "status": "fitted",
  "auto_saved": true,
  "saved_path": "~/.llamafarm/models/classifier/intent-classifier-test_20260102_202450",
  "base_name": "intent-classifier-test",
  "versioned_name": "intent-classifier-test_20260102_202450",
  "overwrite": false
}
```

**Example:**

```bash
curl -X POST http://localhost:8000/v1/ml/classifier/fit \
  -H "Content-Type: application/json" \
  -d '{
    "model": "intent-classifier",
    "training_data": [
      {"text": "I need to book a flight to NYC", "label": "booking"},
      {"text": "Reserve a hotel room for me", "label": "booking"},
      {"text": "Cancel my reservation please", "label": "cancellation"},
      {"text": "I want to cancel my booking", "label": "cancellation"},
      {"text": "What time does the flight leave?", "label": "inquiry"},
      {"text": "How much does it cost?", "label": "inquiry"},
      {"text": "I am very unhappy with the service", "label": "complaint"},
      {"text": "This is unacceptable quality", "label": "complaint"}
    ],
    "num_iterations": 20
  }'
```

#### Predict with Classifier

Classify texts using a trained model.

**Endpoint:** `POST /v1/ml/classifier/predict`

**Request Body:**

```json
{
  "model": "intent-classifier-latest",
  "texts": [
    "Book me a flight to Paris tomorrow",
    "Cancel my upcoming trip",
    "What are the check-in times?",
    "This is absolutely terrible service"
  ]
}
```

**Response:**

```json
{
  "object": "list",
  "data": [
    {
      "text": "Book me a flight to Paris tomorrow",
      "label": "booking",
      "score": 0.66,
      "all_scores": {"booking": 0.66, "cancellation": 0.11, "complaint": 0.11, "inquiry": 0.13}
    },
    {
      "text": "Cancel my upcoming trip",
      "label": "cancellation",
      "score": 0.79,
      "all_scores": {"booking": 0.09, "cancellation": 0.79, "complaint": 0.07, "inquiry": 0.05}
    },
    {
      "text": "What are the check-in times?",
      "label": "inquiry",
      "score": 0.68,
      "all_scores": {"booking": 0.15, "cancellation": 0.06, "complaint": 0.11, "inquiry": 0.68}
    },
    {
      "text": "This is absolutely terrible service",
      "label": "complaint",
      "score": 0.77,
      "all_scores": {"booking": 0.06, "cancellation": 0.08, "complaint": 0.77, "inquiry": 0.10}
    }
  ],
  "total_count": 4,
  "model": "intent-classifier-test_20260102_202450"
}
```

**Example:**

```bash
curl -X POST http://localhost:8000/v1/ml/classifier/predict \
  -H "Content-Type: application/json" \
  -d '{
    "model": "intent-classifier-latest",
    "texts": [
      "Book me a flight to Paris tomorrow",
      "Cancel my upcoming trip",
      "What are the check-in times?",
      "This is absolutely terrible service"
    ]
  }'
```

#### Save Classifier

Save a trained classifier to disk for production use.

**Endpoint:** `POST /v1/ml/classifier/save`

**Request Body:**

```json
{
  "model": "intent-classifier_20251215_155054"
}
```

**Response:**

```json
{
  "object": "save_result",
  "model": "intent-classifier_20251215_155054",
  "path": "~/.llamafarm/models/classifier/intent-classifier_20251215_155054",
  "status": "saved"
}
```

#### Load Classifier

Load a previously saved classifier.

**Endpoint:** `POST /v1/ml/classifier/load`

**Request Body:**

```json
{
  "model": "intent-classifier-latest"
}
```

**Response:**

```json
{
  "object": "load_result",
  "model": "intent-classifier_20251215_155054",
  "status": "loaded"
}
```

#### List Classifier Models

**Endpoint:** `GET /v1/ml/classifier/models`

**Response:**

```json
{
  "object": "list",
  "data": [
    {"name": "intent-classifier_20251215_155054", "labels": ["booking", "cancellation", "inquiry"]}
  ]
}
```

#### Delete Classifier Model

**Endpoint:** `DELETE /v1/ml/classifier/models/{model_name}`

---

### Anomaly Detection

Train anomaly detectors on normal data and detect outliers in new data.

#### Fit Anomaly Detector

Train an anomaly detection model.

**Endpoint:** `POST /v1/ml/anomaly/fit`

**Request Body (Numeric Data):**

```json
{
  "model": "sensor-detector",
  "backend": "isolation_forest",
  "data": [
    [22.1, 1024], [23.5, 1100], [21.8, 980],
    [24.2, 1050], [22.7, 1080], [23.1, 990]
  ],
  "contamination": 0.1,
  "overwrite": false
}
```

**Request Body (Mixed Data with Schema):**

```json
{
  "model": "api-monitor",
  "backend": "isolation_forest",
  "data": [
    {"response_time_ms": 100, "bytes": 1024, "method": "GET", "user_agent": "Mozilla/5.0"},
    {"response_time_ms": 105, "bytes": 1100, "method": "POST", "user_agent": "Chrome/90.0"}
  ],
  "schema": {
    "response_time_ms": "numeric",
    "bytes": "numeric",
    "method": "label",
    "user_agent": "hash"
  },
  "contamination": 0.1
}
```

**Request Fields:**

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `model` | string | No | "default" | Model identifier |
| `backend` | string | No | "isolation_forest" | Algorithm: `isolation_forest`, `one_class_svm`, `local_outlier_factor`, `autoencoder` |
| `data` | array | Yes | - | Training data (numeric arrays or dicts) |
| `schema` | object | No | - | Feature encoding schema (required for dict data) |
| `contamination` | float | No | 0.1 | Expected proportion of anomalies (0-0.5) |
| `overwrite` | bool | No | false | If false, version with timestamp |

**Response:**

```json
{
  "object": "fit_result",
  "model": "sensor_anomaly_detector_20260102_202438",
  "backend": "isolation_forest",
  "samples_fitted": 30,
  "training_time_ms": 85.77,
  "model_params": {
    "backend": "isolation_forest",
    "contamination": 0.05,
    "threshold": 0.894,
    "input_dim": 1
  },
  "status": "fitted",
  "base_name": "sensor_anomaly_detector",
  "versioned_name": "sensor_anomaly_detector_20260102_202438",
  "overwrite": false
}
```

**Example (Temperature Sensor Data):**

```bash
# Train on normal temperature readings (20-25Â°C range)
curl -X POST http://localhost:8000/v1/ml/anomaly/fit \
  -H "Content-Type: application/json" \
  -d '{
    "model": "sensor_anomaly_detector",
    "backend": "isolation_forest",
    "data": [
      [22.1], [23.5], [21.8], [24.2], [22.7],
      [23.1], [21.5], [24.8], [22.3], [23.9],
      [21.2], [24.5], [22.8], [23.2], [21.9],
      [24.1], [22.5], [23.7], [21.6], [24.3]
    ],
    "contamination": 0.05
  }'
```

#### Score Anomalies

Score all data points for anomalies.

**Endpoint:** `POST /v1/ml/anomaly/score`

**Request Body:**

```json
{
  "model": "sensor-detector-latest",
  "backend": "isolation_forest",
  "data": [[22.0], [23.5], [0.0], [100.0], [21.5]],
  "threshold": 0.5
}
```

**Response:**

```json
{
  "object": "list",
  "data": [
    {"index": 0, "score": 0.23, "is_anomaly": false, "raw_score": 0.12},
    {"index": 1, "score": 0.21, "is_anomaly": false, "raw_score": 0.10},
    {"index": 2, "score": 0.89, "is_anomaly": true, "raw_score": -0.45},
    {"index": 3, "score": 0.95, "is_anomaly": true, "raw_score": -0.52},
    {"index": 4, "score": 0.22, "is_anomaly": false, "raw_score": 0.11}
  ],
  "summary": {
    "total_points": 5,
    "anomaly_count": 2,
    "anomaly_rate": 0.4,
    "threshold": 0.5
  }
}
```

#### Detect Anomalies

Detect anomalies (returns only anomalous points).

**Endpoint:** `POST /v1/ml/anomaly/detect`

Same request format as `/score`, but response only includes anomalous points.

**Example (Detecting Temperature Anomalies):**

```bash
# Test with mix of normal and anomalous readings
curl -X POST http://localhost:8000/v1/ml/anomaly/detect \
  -H "Content-Type: application/json" \
  -d '{
    "model": "sensor_anomaly_detector-latest",
    "backend": "isolation_forest",
    "data": [[22.0], [23.5], [0.0], [21.5], [100.0], [24.0], [-10.0], [22.8], [35.0], [23.2]],
    "threshold": 0.5
  }'
```

**Response:**

```json
{
  "object": "list",
  "data": [
    {"index": 2, "score": 0.61, "raw_score": 0.65},
    {"index": 4, "score": 0.60, "raw_score": 0.64},
    {"index": 6, "score": 0.61, "raw_score": 0.65},
    {"index": 8, "score": 0.60, "raw_score": 0.64}
  ],
  "total_count": 4,
  "model": "sensor_anomaly_detector_20260102_202438",
  "backend": "isolation_forest",
  "summary": {
    "anomalies_detected": 4,
    "threshold": 0.5
  }
}
```

The anomalies detected are:
- Index 2: 0.0Â°C (freezing - way below normal)
- Index 4: 100.0Â°C (boiling - way above normal)
- Index 6: -10.0Â°C (sub-freezing)
- Index 8: 35.0Â°C (elevated temperature)

#### Save Anomaly Model

**Endpoint:** `POST /v1/ml/anomaly/save`

**Request Body:**

```json
{
  "model": "sensor-detector_20251215_160000",
  "backend": "isolation_forest"
}
```

#### Load Anomaly Model

**Endpoint:** `POST /v1/ml/anomaly/load`

**Request Body:**

```json
{
  "model": "sensor-detector-latest",
  "backend": "isolation_forest"
}
```

#### List Anomaly Models

**Endpoint:** `GET /v1/ml/anomaly/models`

#### Delete Anomaly Model

**Endpoint:** `DELETE /v1/ml/anomaly/models/{filename}`

---

## Universal Runtime API

The Universal Runtime is a separate service (port 11540) that provides specialized ML endpoints for document processing, text analysis, embeddings, and anomaly detection.

**Base URL:** `http://localhost:11540`

**TIP: Using Vision APIs**
For OCR and document extraction, you can use either:
- **LlamaFarm API** (`/v1/vision/*`) - Accepts file uploads directly, converts PDFs to images automatically
- **Universal Runtime** (`/v1/ocr`, `/v1/documents/extract`) - Accepts base64 images or file IDs

### Starting the Universal Runtime

```bash
nx start universal-runtime
```

### Universal Runtime Endpoints

| Category | Endpoint | Description |
|----------|----------|-------------|
| **Health** | `GET /health` | Runtime health and loaded models |
| **Models** | `GET /v1/models` | List currently loaded models |
| **Chat** | `POST /v1/chat/completions` | OpenAI-compatible chat completions |
| **Embeddings** | `POST /v1/embeddings` | Generate text embeddings |
| **Files** | `POST /v1/files` | Upload files for processing |
| **Files** | `GET /v1/files` | List uploaded files |
| **Files** | `GET /v1/files/{id}` | Get file metadata |
| **Files** | `GET /v1/files/{id}/images` | Get file as base64 images |
| **Files** | `DELETE /v1/files/{id}` | Delete uploaded file |
| **OCR** | `POST /v1/ocr` | Extract text from images (base64) |
| **Documents** | `POST /v1/documents/extract` | Extract structured data (base64) |
| **Classification** | `POST /v1/classify` | Classify text using pre-trained models (sentiment, etc.) |
| **Custom Classifier** | `POST /v1/classifier/fit` | Train custom classifier (SetFit few-shot) |
| **Custom Classifier** | `POST /v1/classifier/predict` | Classify with trained custom model |
| **Custom Classifier** | `POST /v1/classifier/save` | Save trained classifier |
| **Custom Classifier** | `POST /v1/classifier/load` | Load saved classifier |
| **Custom Classifier** | `GET /v1/classifier/models` | List saved classifiers |
| **Custom Classifier** | `DELETE /v1/classifier/models/{name}` | Delete saved classifier |
| **NER** | `POST /v1/ner` | Named entity recognition |
| **Reranking** | `POST /v1/rerank` | Rerank documents by relevance |
| **Anomaly** | `POST /v1/anomaly/fit` | Train anomaly detector |
| **Anomaly** | `POST /v1/anomaly/score` | Score data for anomalies |
| **Anomaly** | `POST /v1/anomaly/detect` | Detect anomalies (filtered) |
| **Anomaly** | `POST /v1/anomaly/save` | Save trained model |
| **Anomaly** | `POST /v1/anomaly/load` | Load saved model |
| **Anomaly** | `GET /v1/anomaly/models` | List saved models |
| **Anomaly** | `DELETE /v1/anomaly/models/{filename}` | Delete saved model |

### Quick Examples

**OCR:**
```bash
curl -X POST http://localhost:11540/v1/ocr \
  -H "Content-Type: application/json" \
  -d '{"model": "surya", "images": ["base64..."], "languages": ["en"]}'
```

**Embeddings:**
```bash
curl -X POST http://localhost:11540/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{"model": "sentence-transformers/all-MiniLM-L6-v2", "input": "Hello world"}'
```

**Custom Classification (SetFit):**
```bash
# Train a custom classifier with few examples
curl -X POST http://localhost:11540/v1/classifier/fit \
  -H "Content-Type: application/json" \
  -d '{
    "model": "intent-classifier",
    "training_data": [
      {"text": "Book a flight", "label": "booking"},
      {"text": "Cancel my order", "label": "cancellation"}
    ]
  }'

# Classify new texts
curl -X POST http://localhost:11540/v1/classifier/predict \
  -H "Content-Type: application/json" \
  -d '{"model": "intent-classifier", "texts": ["I need to reserve a room"]}'
```

**Anomaly Detection:**
```bash
# Train
curl -X POST http://localhost:11540/v1/anomaly/fit \
  -H "Content-Type: application/json" \
  -d '{"model": "my-detector", "backend": "isolation_forest", "data": [[1,2],[3,4]]}'

# Detect
curl -X POST http://localhost:11540/v1/anomaly/detect \
  -H "Content-Type: application/json" \
  -d '{"model": "my-detector", "data": [[1,2],[100,200]]}'
```

For complete documentation, see:
- [Specialized ML Models](../models/specialized-ml.md) - OCR, documents, classification, NER, reranking
- [Anomaly Detection Guide](../models/anomaly-detection.md) - Complete anomaly detection documentation
- [Models & Runtime](../models/index.md) - Runtime configuration

---

## Next Steps

- Learn about [Configuration](../configuration/index.md)
- Explore [RAG concepts](../rag/index.md)
- Review [Examples](../examples/index.md)
- Check [Troubleshooting](../troubleshooting/index.md)

---

# RAG Guide

LlamaFarm treats retrieval-augmented generation as a first-class, configurable pipeline. This guide explains how strategies, databases, and datasets fit togetherâ€”and how to operate and extend them.

## Component Reference

For detailed configuration options, see these reference guides:

| Component | Description | Link |
|-----------|-------------|------|
| **Parsers** | Transform documents into chunks | [Parsers Reference](./parsers.md) |
| **Embedders** | Convert text to vectors | [Embedders Reference](./embedders.md) |
| **Extractors** | Enrich chunks with metadata | [Extractors Reference](./extractors.md) |
| **Databases** | Vector storage backends | [Databases Reference](./databases.md) |
| **Retrieval Strategies** | Search and ranking methods | [Retrieval Strategies](./retrieval-strategies.md) |
| **Advanced Retrieval** | Cross-encoder, multi-turn RAG | [Advanced Retrieval](./advanced-retrieval.md) |

## RAG at a Glance

| Piece | Where it lives | Purpose |
| ----- | -------------- | ------- |
| `rag.databases[]` | `llamafarm.yaml` | Define vector stores and retrieval strategies. |
| `rag.data_processing_strategies[]` | `llamafarm.yaml` | Describe parsers, extractors, and metadata processors for ingestion. |
| `lf datasets create/upload/process` | CLI | Ingest documents according to the chosen strategy/database. |
| `lf rag query` | CLI | Query the store with semantic, hybrid, or metadata-aware retrieval. |
| Celery workers | Server runtime | Perform heavy ingestion tasks. |

## Configure Databases

Each database entry declares a store type (default `ChromaStore` or `QdrantStore`) and the embedding/retrieval strategies available.

```yaml
rag:
  databases:
    - name: main_db
      type: ChromaStore
      default_embedding_strategy: default_embeddings
      default_retrieval_strategy: semantic_search
      embedding_strategies:
        - name: default_embeddings
          type: OllamaEmbedder
          config:
            model: nomic-embed-text:latest
      retrieval_strategies:
        - name: semantic_search
          type: VectorRetriever
          config:
            top_k: 5
        - name: hybrid_search
          type: HybridUniversalStrategy
          config:
            dense_weight: 0.6
            sparse_weight: 0.4
```

- Add multiple strategies for different workloads (semantic, keyword, reranked).
- Set `default_*` fields to control CLI defaults.
- Extend store/types by editing `rag/schema.yaml` and following the [Extending guide](../extending/index.md#extend-rag-components).

## Define Processing Strategies

Processing strategies control how files become chunks in the vector store.

```yaml
rag:
  data_processing_strategies:
    - name: pdf_ingest
      description: Ingest FDA letters with headings & stats.
      parsers:
        - type: PDFParser_LlamaIndex
          config:
            chunk_size: 1500
            chunk_overlap: 200
            preserve_layout: true
      extractors:
        - type: HeadingExtractor
        - type: ContentStatisticsExtractor
      metadata_extractors:
        - type: EntityExtractor
```

- Parsers handle format-aware chunking (PDF, CSV, DOCX, Markdown, text).
- Extractors add metadata (entities, headings, statistics) to each chunk.
- Customize chunk size/overlap per parser type.

## Dataset Lifecycle

1. **Create** a dataset referencing a strategy and database.
   ```bash
   lf datasets create -s pdf_ingest -b main_db research-notes
   ```
2. **Upload** files via `lf datasets upload` (supports globs and directories). The CLI stores file hashes for dedupe.
3. **Process** documents with `lf datasets process research-notes`. The server schedules a Celery job; monitor progress in the CLI output and server logs.
4. **Query** with `lf rag query` to validate retrieval quality.

## Querying & Retrieval Strategies

`lf rag query` exposes several toggles:

- `--retrieval-strategy` to select among those defined in the database.
- `--filter "key:value"` for metadata filtering (e.g., `doc_type:letter`, `year:2024`).
- `--top-k`, `--score-threshold`, `--include-metadata`, `--include-score` for result tuning.
- `--distance-metric`, `--hybrid-alpha`, `--rerank-model`, `--query-expansion` for advanced workflows.

Pair queries with `lf chat` to confirm the runtime consumes retrieved context correctly.

### Advanced Retrieval Strategies

For improved accuracy and handling of complex queries, LlamaFarm supports:

- **Cross-Encoder Reranking** - Rerank initial candidates using specialized models (10-100x faster than LLM reranking)
- **Multi-Turn RAG** - Decompose complex queries into sub-queries, retrieve in parallel, and merge results

See [Advanced Retrieval Strategies](./advanced-retrieval.md) for detailed configuration and usage.

## Monitoring & Maintenance

- `lf rag stats` â€“ view vector counts and storage usage.
- `lf rag health` â€“ check embedder/store health status.
- `lf rag list` â€“ inspect documents and metadata.
- `lf rag compact` / `lf rag reindex` â€“ maintain store performance.
- `lf rag clear` / `lf rag delete` â€“ remove data (dangerous; confirm before use).

## Troubleshooting

| Symptom | Possible Cause | Fix |
| ------- | -------------- | ---- |
| `No response received` after `lf chat` | Runtime returned empty stream (model mismatch, tool support) | Try `--no-rag`, switch models, or adjust agent handler. |
| `Task timed out or failed: PENDING` during processing | Celery worker still ingesting large files | Wait and re-run, check worker logs, ensure enough resources. |
| Query returns 0 results | Incorrect strategy/database, unprocessed dataset, high score threshold | Verify dataset processed successfully, adjust `--score-threshold`. |

## Next Steps

- [Parsers Reference](./parsers.md) â€“ configure document parsing
- [Embedders Reference](./embedders.md) â€“ configure embedding models
- [Extractors Reference](./extractors.md) â€“ add metadata extraction
- [Databases Reference](./databases.md) â€“ configure vector stores
- [Retrieval Strategies](./retrieval-strategies.md) â€“ configure search strategies
- [Advanced Retrieval](./advanced-retrieval.md) â€“ cross-encoder and multi-turn RAG
- [CLI Reference](../cli/index.md) â€“ command usage
- [Extending RAG](../extending/index.md#extend-rag-components) â€“ add custom components
- [Examples](../examples/index.md) â€“ see FDA and Raleigh workflows

---

# Databases Reference

Vector databases store embeddings and enable semantic search. LlamaFarm supports multiple vector store backends for different deployment scenarios.

## Quick Start

Databases are configured in `rag.databases`:

```yaml
rag:
  default_database: main_db
  databases:
    - name: main_db
      type: ChromaStore
      config:
        collection_name: documents
        distance_function: cosine
```

### Common Database Properties

| Property | Required | Description |
|----------|----------|-------------|
| `name` | Yes | Unique identifier (lowercase, underscores) |
| `type` | Yes | Store type (`ChromaStore`, `QdrantStore`, etc.) |
| `config` | No | Store-specific configuration |
| `embedding_strategies` | No | List of embedding configurations |
| `retrieval_strategies` | No | List of retrieval configurations |
| `default_embedding_strategy` | No | Default embedder name |
| `default_retrieval_strategy` | No | Default retrieval strategy name |

---

## ChromaStore

Chroma is a lightweight, embedded vector database. Great for development and small-to-medium deployments.

**Best for:** Local development, prototyping, embedded use

```yaml
- name: main_db
  type: ChromaStore
  config:
    collection_name: documents
    distance_function: cosine
    persist_directory: ./data/chroma_db
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `collection_name` | string | `documents` | Collection name (alphanumeric, hyphens, underscores) |
| `host` | string | `null` | Server host (null for embedded) |
| `port` | integer | 8000 | Server port |
| `distance_function` | string | `cosine` | `cosine`, `l2`, `ip` |
| `distance_metric` | string | `cosine` | Alternative name for distance_function |
| `embedding_dimension` | integer | 768 | Vector dimension (1-4096) |
| `enable_deduplication` | boolean | `true` | Enable document deduplication |
| `embedding_function` | string | `null` | Built-in embedding function |

### Distance Functions

| Function | Description | Use Case |
|----------|-------------|----------|
| `cosine` | Cosine similarity | General purpose, normalized vectors |
| `l2` | Euclidean distance | When magnitude matters |
| `ip` | Inner product | Already normalized embeddings |

### Client vs Server Mode

**Embedded (default):**
```yaml
config:
  collection_name: documents
  persist_directory: ./data/chroma
```

**Server mode:**
```yaml
config:
  collection_name: documents
  host: localhost
  port: 8000
```

---

## QdrantStore (Coming Soon)

**NOTE: Coming Soon**
QdrantStore is planned but not yet implemented. Use **ChromaStore** for now.

Qdrant is a high-performance vector database with rich filtering and clustering.

**Best for:** Production deployments, large datasets, advanced filtering

```yaml
- name: production_db
  type: QdrantStore
  config:
    collection_name: documents
    host: localhost
    port: 6333
    vector_size: 768
    distance: Cosine
```

### Planned Options

| Option | Type | Default | Required | Description |
|--------|------|---------|----------|-------------|
| `collection_name` | string | `documents` | No | Collection name |
| `host` | string | `localhost` | No | Server host |
| `port` | integer | 6333 | No | REST API port |
| `grpc_port` | integer | 6334 | No | gRPC port |
| `api_key` | string | `null` | No | API key for auth |
| `vector_size` | integer | - | Yes | Vector dimension (1-65536) |
| `distance` | string | `Cosine` | No | `Cosine`, `Euclid`, `Dot` |
| `on_disk` | boolean | `false` | No | Store vectors on disk |

### Distance Metrics

| Metric | Description | Use Case |
|--------|-------------|----------|
| `Cosine` | Cosine similarity | General semantic search |
| `Euclid` | Euclidean distance | Geometric comparisons |
| `Dot` | Dot product | Pre-normalized vectors |

### Docker Setup (for when available)

```bash
docker run -p 6333:6333 -p 6334:6334 \
  -v $(pwd)/qdrant_data:/qdrant/storage \
  qdrant/qdrant
```

---

## FAISSStore (Coming Soon)

**NOTE: Coming Soon**
FAISSStore is planned but not yet implemented. Use **ChromaStore** for now.

FAISS (Facebook AI Similarity Search) for high-performance similarity search.

**Best for:** Large-scale search, research, GPU acceleration

```yaml
- name: faiss_db
  type: FAISSStore
  config:
    dimension: 768
    index_type: HNSW
    metric: Cosine
```

### Planned Options

| Option | Type | Default | Required | Description |
|--------|------|---------|----------|-------------|
| `dimension` | integer | - | Yes | Vector dimension (1-4096) |
| `index_type` | string | `Flat` | No | `Flat`, `IVF`, `HNSW`, `LSH` |
| `metric` | string | `L2` | No | `L2`, `IP`, `Cosine` |
| `nlist` | integer | 100 | No | Number of clusters (IVF) |
| `nprobe` | integer | 10 | No | Clusters to search (IVF) |
| `use_gpu` | boolean | `false` | No | Enable GPU acceleration |

### Index Types

| Type | Speed | Accuracy | Memory | Best For |
|------|-------|----------|--------|----------|
| `Flat` | Slow | Exact | High | Small datasets, exact search |
| `IVF` | Fast | Good | Medium | Large datasets |
| `HNSW` | Very Fast | Very Good | High | Production, low latency |
| `LSH` | Fast | Moderate | Low | Very large datasets |

---

## PineconeStore (Coming Soon)

**NOTE: Coming Soon**
PineconeStore is planned but not yet implemented. Use **ChromaStore** for now.

Pinecone is a fully managed cloud vector database.

**Best for:** Managed service, enterprise, global scale

```yaml
- name: cloud_db
  type: PineconeStore
  config:
    api_key: ${PINECONE_API_KEY}
    index_name: my-index
    dimension: 768
    environment: us-east-1-aws
```

### Planned Options

| Option | Type | Default | Required | Description |
|--------|------|---------|----------|-------------|
| `api_key` | string | - | Yes | Pinecone API key |
| `index_name` | string | - | Yes | Index name (lowercase, hyphens) |
| `dimension` | integer | - | Yes | Vector dimension (1-20000) |
| `environment` | string | `us-east-1-aws` | No | Pinecone environment |
| `metric` | string | `cosine` | No | `euclidean`, `cosine`, `dotproduct` |
| `namespace` | string | `""` | No | Namespace for isolation |
| `replicas` | integer | 1 | No | Number of replicas (1-20) |

---

## Complete Database Configuration

Full example with embedding and retrieval strategies:

```yaml
rag:
  default_database: main_db

  databases:
    - name: main_db
      type: ChromaStore
      config:
        collection_name: documents
        distance_function: cosine
        persist_directory: ./data/main_db

      # Embedding strategies
      embedding_strategies:
        - name: default
          type: OllamaEmbedder
          config:
            model: nomic-embed-text
            dimension: 768
          priority: 0

        - name: openai
          type: OpenAIEmbedder
          config:
            model: text-embedding-3-small
            api_key: ${OPENAI_API_KEY}
          priority: 1

      # Retrieval strategies
      retrieval_strategies:
        - name: semantic
          type: BasicSimilarityStrategy
          config:
            distance_metric: cosine
            top_k: 10
          default: true

        - name: filtered
          type: MetadataFilteredStrategy
          config:
            top_k: 10
            filter_mode: pre

      default_embedding_strategy: default
      default_retrieval_strategy: semantic
```

## Multiple Databases

Configure separate databases for different use cases:

```yaml
databases:
  # Primary document store
  - name: documents_db
    type: ChromaStore
    config:
      collection_name: documents

  # Archive for older content (embedded mode)
  - name: archive_db
    type: ChromaStore
    config:
      collection_name: archive
      persist_directory: ./data/archive

  # Server-mode for production
  - name: production_db
    type: ChromaStore
    config:
      collection_name: production
      host: localhost
      port: 8000
```

**TIP: Future Options**
When QdrantStore and FAISSStore are available, you'll be able to use them for larger scale deployments and more advanced features.

Query specific databases:

```bash
lf rag query --database documents_db "recent reports"
lf rag query --database archive_db "historical data"
```

## Database Selection Guide

| Database | Deployment | Scale | Features | Status |
|----------|------------|-------|----------|--------|
| ChromaStore | Embedded/Server | Small-Medium | Simple, portable | **Available** |
| QdrantStore | Self-hosted | Medium-Large | Rich filtering, clustering | Coming Soon |
| FAISSStore | Self-hosted | Very Large | GPU support, research | Coming Soon |
| PineconeStore | Cloud | Any | Managed, global | Coming Soon |

## Next Steps

- [Retrieval Strategies](./retrieval-strategies.md) - Configure retrieval methods
- [Embedders Reference](./embedders.md) - Configure embeddings
- [RAG Guide](./index.md) - Full RAG overview

---

# Embedders Reference

Embedders convert text chunks into vector representations for semantic search. LlamaFarm supports multiple embedding providers to match your infrastructure and requirements.

## Quick Start

Embedders are configured within `embedding_strategies` in your database definition:

```yaml
rag:
  databases:
    - name: my_database
      embedding_strategies:
        - name: default_embeddings
          type: OllamaEmbedder
          config:
            model: nomic-embed-text
```

---

## UniversalEmbedder

Flexible embedder that works with Ollama and falls back to HuggingFace models.

**Best for:** Most use cases, automatic model management

```yaml
- name: default_embeddings
  type: UniversalEmbedder
  config:
    model: nomic-ai/nomic-embed-text-v2-moe
    base_url: http://localhost:11434
    dimension: 768
```

### Options

| Option | Type | Default | Required | Description |
|--------|------|---------|----------|-------------|
| `model` | string | `nomic-ai/nomic-embed-text-v2-moe` | No | HuggingFace model ID |
| `base_url` | string | `http://localhost:11434` | No | Ollama API endpoint |
| `dimension` | integer | 768 | No | Embedding dimension (128-4096) |
| `batch_size` | integer | 16 | No | Batch processing size (1-128) |
| `timeout` | integer | 60 | No | Request timeout in seconds |

---

## OllamaEmbedder

Native Ollama embedder with auto-pull support.

**Best for:** Ollama-based deployments, local inference

```yaml
- name: ollama_embeddings
  type: OllamaEmbedder
  config:
    model: nomic-embed-text:latest
    base_url: http://localhost:11434
    auto_pull: true
```

### Options

| Option | Type | Default | Required | Description |
|--------|------|---------|----------|-------------|
| `model` | string | `nomic-embed-text` | No | Ollama model name |
| `base_url` | string | `http://localhost:11434` | No | Ollama API endpoint |
| `dimension` | integer | 768 | No | Embedding dimension (128-4096) |
| `batch_size` | integer | 16 | No | Batch processing size (1-128) |
| `timeout` | integer | 60 | No | Request timeout in seconds |
| `auto_pull` | boolean | `true` | No | Auto-pull missing models |

### Recommended Models

```bash
# Pull embedding models
ollama pull nomic-embed-text          # 768 dim, general purpose
ollama pull mxbai-embed-large         # 1024 dim, higher quality
ollama pull all-minilm                # 384 dim, fast
```

---

## HuggingFaceEmbedder (Coming Soon)

**NOTE: Coming Soon**
HuggingFaceEmbedder is planned but not yet implemented. Use **UniversalEmbedder** or **OllamaEmbedder** instead.

Direct HuggingFace model loading with GPU/MPS support.

**Best for:** Custom models, GPU acceleration, offline use

```yaml
- name: hf_embeddings
  type: HuggingFaceEmbedder
  config:
    model_name: sentence-transformers/all-MiniLM-L6-v2
    device: auto
    normalize_embeddings: true
```

### Planned Options

| Option | Type | Default | Required | Description |
|--------|------|---------|----------|-------------|
| `model_name` | string | `sentence-transformers/all-MiniLM-L6-v2` | No | HuggingFace model ID |
| `device` | string | `auto` | No | `cpu`, `cuda`, `mps`, or `auto` |
| `batch_size` | integer | 32 | No | Batch size (1-256) |
| `normalize_embeddings` | boolean | `true` | No | L2 normalize embeddings |
| `show_progress_bar` | boolean | `false` | No | Show progress bar |
| `cache_folder` | string | `null` | No | Model cache directory |

### Recommended Models (for when available)

| Model | Dimensions | Speed | Quality |
|-------|------------|-------|---------|
| `all-MiniLM-L6-v2` | 384 | Fast | Good |
| `all-mpnet-base-v2` | 768 | Medium | High |
| `BAAI/bge-base-en-v1.5` | 768 | Medium | High |
| `BAAI/bge-large-en-v1.5` | 1024 | Slow | Highest |

---

## SentenceTransformerEmbedder (Coming Soon)

**NOTE: Coming Soon**
SentenceTransformerEmbedder is planned but not yet implemented. Use **UniversalEmbedder** or **OllamaEmbedder** instead.

Sentence Transformers library integration.

**Best for:** Sentence-level embeddings, specialized models

```yaml
- name: st_embeddings
  type: SentenceTransformerEmbedder
  config:
    model_name: sentence-transformers/all-MiniLM-L6-v2
    device: cpu
```

### Planned Options

| Option | Type | Default | Required | Description |
|--------|------|---------|----------|-------------|
| `model_name` | string | `sentence-transformers/all-MiniLM-L6-v2` | No | Model name |
| `device` | string | `cpu` | No | `cpu`, `cuda`, or `mps` |

---

## OpenAIEmbedder (Coming Soon)

**NOTE: Coming Soon**
OpenAIEmbedder is planned but not yet implemented. Use **UniversalEmbedder** or **OllamaEmbedder** instead.

OpenAI embedding API integration.

**Best for:** OpenAI API users, high-quality embeddings

```yaml
- name: openai_embeddings
  type: OpenAIEmbedder
  config:
    model: text-embedding-3-small
    api_key: ${OPENAI_API_KEY}
```

### Options

| Option | Type | Default | Required | Description |
|--------|------|---------|----------|-------------|
| `model` | string | `text-embedding-3-small` | No | OpenAI embedding model |
| `api_key` | string | - | Yes | OpenAI API key |
| `base_url` | string | `null` | No | Custom API base URL |
| `organization` | string | `null` | No | OpenAI organization ID |
| `batch_size` | integer | 100 | No | Batch size (1-2048) |
| `max_retries` | integer | 3 | No | Maximum retry attempts |
| `timeout` | integer | 60 | No | Request timeout in seconds |

### Available Models

| Model | Dimensions | Max Tokens | Use Case |
|-------|------------|------------|----------|
| `text-embedding-3-small` | 1536 | 8191 | Cost-effective |
| `text-embedding-3-large` | 3072 | 8191 | Highest quality |
| `text-embedding-ada-002` | 1536 | 8191 | Legacy |

---

## Multiple Embedding Strategies

Configure multiple embedders for different content types:

```yaml
embedding_strategies:
  - name: general
    type: OllamaEmbedder
    config:
      model: nomic-embed-text
    priority: 0

  - name: code
    type: HuggingFaceEmbedder
    config:
      model_name: microsoft/codebert-base
    condition: "doc.type == 'code'"
    priority: 1

default_embedding_strategy: general
```

## Dimension Matching

Ensure embedding dimensions match your vector store configuration:

```yaml
databases:
  - name: my_db
    type: ChromaStore
    config:
      embedding_dimension: 768  # Must match embedder output

    embedding_strategies:
      - name: default
        type: OllamaEmbedder
        config:
          model: nomic-embed-text
          dimension: 768  # Match store dimension
```

## Next Steps

- [Extractors Reference](./extractors.md) - Add metadata extraction
- [Databases Reference](./databases.md) - Configure vector stores
- [RAG Guide](./index.md) - Full RAG overview

---

# Parsers Reference

Parsers transform raw documents into text chunks suitable for embedding and retrieval. LlamaFarm provides multiple parsers for each file format, allowing you to choose based on your performance and capability needs.

## Parser Configuration

Parsers are defined within `data_processing_strategies` in your `llamafarm.yaml`:

```yaml
rag:
  data_processing_strategies:
    - name: my_processor
      description: "Process various document types"
      parsers:
        - type: PDFParser_LlamaIndex
          file_include_patterns:
            - "*.pdf"
          priority: 0  # Lower = try first
          config:
            chunk_size: 1000
            chunk_overlap: 100
```

### Common Parser Properties

| Property | Required | Description |
|----------|----------|-------------|
| `type` | Yes | Parser type identifier (e.g., `PDFParser_PyPDF2`) |
| `file_include_patterns` | No | Glob patterns for files to process (e.g., `["*.pdf"]`) |
| `file_extensions` | No | File extensions this parser handles (e.g., `[".pdf"]`) |
| `priority` | No | Parser priority (lower = try first, default: 50) |
| `mime_types` | No | MIME types this parser handles |
| `fallback_parser` | No | Parser to use if this one fails |
| `config` | Yes | Parser-specific configuration |

---

## Auto Parser

The `auto` parser automatically detects file types and applies the appropriate parser.

### Configuration

```yaml
parsers:
  - type: auto
    config:
      chunk_size: 1000      # Chunk size for text splitting (100-10000, default: 1000)
      chunk_overlap: 200    # Overlap between chunks (0-500, default: 200)
```

### Options

| Option | Type | Default | Range | Description |
|--------|------|---------|-------|-------------|
| `chunk_size` | integer | 1000 | 100-10000 | Chunk size for text splitting |
| `chunk_overlap` | integer | 200 | 0-500 | Overlap between chunks |

---

## PDF Parsers

### PDFParser_PyPDF2

Enhanced PDF parser using PyPDF2 with comprehensive text and metadata extraction.

**Best for:** Simple PDFs, form extraction, annotation extraction

```yaml
parsers:
  - type: PDFParser_PyPDF2
    file_include_patterns:
      - "*.pdf"
      - "*.PDF"
    config:
      chunk_size: 1000
      chunk_overlap: 100
      chunk_strategy: paragraphs
      extract_metadata: true
      preserve_layout: true
```

#### Options

| Option | Type | Default | Range | Description |
|--------|------|---------|-------|-------------|
| `chunk_size` | integer | 1000 | 100-50000 | Chunk size in characters |
| `chunk_overlap` | integer | 100 | 0-5000 | Overlap between chunks |
| `chunk_strategy` | string | `paragraphs` | `paragraphs`, `sentences`, `characters` | Chunking strategy |
| `extract_metadata` | boolean | `true` | - | Extract PDF metadata |
| `preserve_layout` | boolean | `true` | - | Use layout-preserving extraction |
| `extract_page_info` | boolean | `true` | - | Extract page numbers and rotation |
| `extract_annotations` | boolean | `false` | - | Extract PDF annotations |
| `extract_links` | boolean | `false` | - | Extract hyperlinks |
| `extract_form_fields` | boolean | `false` | - | Extract form fields |
| `extract_outlines` | boolean | `false` | - | Extract document outlines/bookmarks |
| `extract_images` | boolean | `false` | - | Extract embedded images |
| `extract_xmp_metadata` | boolean | `false` | - | Extract XMP metadata |
| `clean_text` | boolean | `true` | - | Clean extracted text |
| `combine_pages` | boolean | `false` | - | Combine all pages (must be `false` for chunking) |

### PDFParser_LlamaIndex

Advanced PDF parser using LlamaIndex with multiple fallback strategies and semantic chunking.

**Best for:** Complex PDFs, scanned documents, semantic chunking

```yaml
parsers:
  - type: PDFParser_LlamaIndex
    file_include_patterns:
      - "*.pdf"
    priority: 0  # Lower = try first (preferred parser)
    config:
      chunk_size: 1200
      chunk_overlap: 150
      chunk_strategy: semantic
      extract_metadata: true
      extract_tables: true
      fallback_strategies:
        - llama_pdf_reader
        - llama_pymupdf_reader
        - direct_pymupdf
        - pypdf2_fallback
```

#### Options

| Option | Type | Default | Range | Description |
|--------|------|---------|-------|-------------|
| `chunk_size` | integer | 1000 | 100-50000 | Chunk size in characters |
| `chunk_overlap` | integer | 100 | 0-5000 | Overlap between chunks |
| `chunk_strategy` | string | `sentences` | `sentences`, `paragraphs`, `pages`, `semantic` | Chunking strategy |
| `extract_metadata` | boolean | `true` | - | Extract PDF metadata |
| `extract_images` | boolean | `false` | - | Extract images from PDF |
| `extract_tables` | boolean | `true` | - | Extract tables from PDF |
| `fallback_strategies` | array | All strategies | See below | Fallback strategies in order |

**Fallback Strategies:**
- `llama_pdf_reader` - LlamaIndex PDFReader
- `llama_pymupdf_reader` - LlamaIndex PyMuPDFReader
- `direct_pymupdf` - Direct PyMuPDF
- `pypdf2_fallback` - PyPDF2 fallback

---

## CSV Parsers

### CSVParser_Pandas

Advanced CSV parser using Pandas with data analysis capabilities.

**Best for:** Data analysis, large CSV files, complex data handling

```yaml
parsers:
  - type: CSVParser_Pandas
    file_include_patterns:
      - "*.csv"
    config:
      chunk_size: 1000
      chunk_strategy: rows
      extract_metadata: true
      encoding: utf-8
      delimiter: ","
      na_values:
        - ""
        - "NA"
        - "N/A"
        - "null"
```

#### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `chunk_size` | integer | 1000 | Number of rows per chunk |
| `chunk_strategy` | string | `rows` | `rows`, `columns`, `full` |
| `extract_metadata` | boolean | `true` | Extract data statistics |
| `encoding` | string | `utf-8` | File encoding |
| `delimiter` | string | `,` | CSV delimiter |
| `na_values` | array | `["", "NA", "N/A", "null", "None"]` | Values to treat as NaN |

### CSVParser_Python

Simple CSV parser using native Python csv module.

**Best for:** Simple CSV files, minimal dependencies

```yaml
parsers:
  - type: CSVParser_Python
    file_include_patterns:
      - "*.csv"
    config:
      chunk_size: 1000
      encoding: utf-8
      delimiter: ","
      quotechar: '"'
```

#### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `chunk_size` | integer | 1000 | Number of rows per chunk |
| `encoding` | string | `utf-8` | File encoding |
| `delimiter` | string | `,` | CSV delimiter |
| `quotechar` | string | `"` | Quote character |

### CSVParser_LlamaIndex

CSV parser using LlamaIndex with Pandas backend for advanced processing.

**Best for:** Semantic chunking, field mapping, integration with LlamaIndex

```yaml
parsers:
  - type: CSVParser_LlamaIndex
    file_include_patterns:
      - "*.csv"
      - "*.tsv"
    config:
      chunk_size: 1000
      chunk_strategy: rows
      field_mapping:
        title: name
        content: description
      combine_fields: true
      skiprows: 0
```

#### Options

| Option | Type | Default | Range | Description |
|--------|------|---------|-------|-------------|
| `chunk_size` | integer | 1000 | 100-50000 | Number of rows per chunk |
| `chunk_strategy` | string | `rows` | `rows`, `semantic`, `full` | Chunking strategy |
| `field_mapping` | object | - | - | Map CSV columns to standard fields |
| `extract_metadata` | boolean | `true` | - | Extract metadata from CSV |
| `combine_fields` | boolean | `true` | - | Combine fields into text content |
| `skiprows` | integer | 0 | 0+ | Number of rows to skip |
| `na_values` | array | `["", "NA", "N/A", "null", "None"]` | - | Values to treat as missing |

---

## Excel Parsers

### ExcelParser_OpenPyXL

Excel parser using OpenPyXL for XLSX files with formula support.

**Best for:** XLSX files, formula extraction, workbook metadata

```yaml
parsers:
  - type: ExcelParser_OpenPyXL
    file_include_patterns:
      - "*.xlsx"
    config:
      chunk_size: 1000
      extract_formulas: false
      extract_metadata: true
      data_only: true
      sheets: null  # Process all sheets
```

#### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `chunk_size` | integer | 1000 | Number of rows per chunk |
| `extract_formulas` | boolean | `false` | Extract cell formulas |
| `extract_metadata` | boolean | `true` | Extract workbook metadata |
| `sheets` | array/null | `null` | Specific sheets to process (null = all) |
| `data_only` | boolean | `true` | Extract values instead of formulas |

### ExcelParser_Pandas

Excel parser using Pandas with data analysis capabilities.

**Best for:** Data analysis, statistical processing

```yaml
parsers:
  - type: ExcelParser_Pandas
    file_include_patterns:
      - "*.xlsx"
      - "*.xls"
    config:
      chunk_size: 1000
      sheets: null
      extract_metadata: true
      skiprows: null
      na_values:
        - ""
        - "NA"
```

#### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `chunk_size` | integer | 1000 | Number of rows per chunk |
| `sheets` | array/null | `null` | Specific sheets to process |
| `extract_metadata` | boolean | `true` | Extract data statistics |
| `skiprows` | integer/null | `null` | Rows to skip at beginning |
| `na_values` | array | `["", "NA", "N/A", "null", "None"]` | Values to treat as NaN |

### ExcelParser_LlamaIndex

Excel parser using LlamaIndex with Pandas backend for advanced processing.

**Best for:** Semantic chunking, combining sheets, advanced data handling

```yaml
parsers:
  - type: ExcelParser_LlamaIndex
    file_include_patterns:
      - "*.xlsx"
      - "*.xls"
    config:
      chunk_size: 1000
      chunk_strategy: rows
      sheets: null
      combine_sheets: false
      extract_metadata: true
      extract_formulas: false
      header_row: 0
```

#### Options

| Option | Type | Default | Range | Description |
|--------|------|---------|-------|-------------|
| `chunk_size` | integer | 1000 | 100-50000 | Number of rows per chunk |
| `chunk_strategy` | string | `rows` | `rows`, `semantic`, `full` | Chunking strategy |
| `sheets` | array/null | `null` | - | Specific sheets to parse |
| `combine_sheets` | boolean | `false` | - | Combine all sheets into one document |
| `extract_metadata` | boolean | `true` | - | Extract metadata |
| `extract_formulas` | boolean | `false` | - | Extract formulas instead of values |
| `header_row` | integer | 0 | 0+ | Row index for headers |
| `skiprows` | integer | 0 | 0+ | Number of rows to skip |
| `na_values` | array | `["", "NA", "N/A", "null", "None"]` | - | Values to treat as missing |

---

## Word Document Parsers

### DocxParser_PythonDocx

Word document parser using python-docx library.

**Best for:** Simple DOCX files, table extraction

```yaml
parsers:
  - type: DocxParser_PythonDocx
    file_include_patterns:
      - "*.docx"
    config:
      chunk_size: 1000
      chunk_strategy: paragraphs
      extract_metadata: true
      extract_tables: true
      extract_headers: true
      extract_footers: false
      extract_comments: false
```

#### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `chunk_size` | integer | 1000 | Chunk size in characters |
| `chunk_strategy` | string | `paragraphs` | `paragraphs`, `sentences`, `characters` |
| `extract_metadata` | boolean | `true` | Extract document metadata |
| `extract_tables` | boolean | `true` | Extract tables |
| `extract_headers` | boolean | `true` | Extract headers |
| `extract_footers` | boolean | `false` | Extract footers |
| `extract_comments` | boolean | `false` | Extract comments |

### DocxParser_LlamaIndex

Advanced DOCX parser using LlamaIndex with enhanced chunking.

**Best for:** Semantic chunking, image extraction, complex documents

```yaml
parsers:
  - type: DocxParser_LlamaIndex
    file_include_patterns:
      - "*.docx"
    config:
      chunk_size: 1000
      chunk_overlap: 100
      chunk_strategy: paragraphs
      extract_metadata: true
      extract_tables: true
      extract_images: false
      preserve_formatting: true
      include_header_footer: false
```

#### Options

| Option | Type | Default | Range | Description |
|--------|------|---------|-------|-------------|
| `chunk_size` | integer | 1000 | 100-50000 | Chunk size in characters |
| `chunk_overlap` | integer | 100 | 0-5000 | Overlap between chunks |
| `chunk_strategy` | string | `paragraphs` | `paragraphs`, `sentences`, `semantic` | Chunking strategy |
| `extract_metadata` | boolean | `true` | - | Extract document metadata |
| `extract_tables` | boolean | `true` | - | Extract tables |
| `extract_images` | boolean | `false` | - | Extract images |
| `preserve_formatting` | boolean | `true` | - | Preserve text formatting |
| `include_header_footer` | boolean | `false` | - | Include header and footer |

---

## Markdown Parsers

### MarkdownParser_Python

Markdown parser using native Python with regex parsing.

**Best for:** Simple Markdown files, minimal dependencies

```yaml
parsers:
  - type: MarkdownParser_Python
    file_include_patterns:
      - "*.md"
      - "*.markdown"
    config:
      chunk_size: 1000
      chunk_strategy: sections
      extract_metadata: true
      extract_code_blocks: true
      extract_links: true
```

#### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `chunk_size` | integer | 1000 | Chunk size in characters |
| `chunk_strategy` | string | `sections` | `sections`, `paragraphs`, `characters` |
| `extract_metadata` | boolean | `true` | Extract YAML frontmatter |
| `extract_code_blocks` | boolean | `true` | Extract code blocks |
| `extract_links` | boolean | `true` | Extract markdown links |

### MarkdownParser_LlamaIndex

Advanced markdown parser using LlamaIndex with semantic chunking.

**Best for:** Complex Markdown, semantic chunking, table extraction

```yaml
parsers:
  - type: MarkdownParser_LlamaIndex
    file_include_patterns:
      - "*.md"
      - "*.markdown"
    config:
      chunk_size: 800
      chunk_overlap: 80
      chunk_strategy: headings
      extract_metadata: true
      extract_code_blocks: true
      extract_tables: true
      extract_links: true
      preserve_structure: true
```

#### Options

| Option | Type | Default | Range | Description |
|--------|------|---------|-------|-------------|
| `chunk_size` | integer | 1000 | 100-50000 | Chunk size in characters |
| `chunk_overlap` | integer | 100 | 0-5000 | Overlap between chunks |
| `chunk_strategy` | string | `headings` | `headings`, `paragraphs`, `sentences`, `semantic` | Chunking strategy |
| `extract_metadata` | boolean | `true` | - | Extract frontmatter metadata |
| `extract_code_blocks` | boolean | `true` | - | Extract code blocks separately |
| `extract_tables` | boolean | `true` | - | Extract markdown tables |
| `extract_links` | boolean | `true` | - | Extract links and references |
| `preserve_structure` | boolean | `true` | - | Preserve heading hierarchy |

---

## Text Parsers

### TextParser_Python

Text parser using native Python with encoding detection.

**Best for:** Plain text files, minimal dependencies

```yaml
parsers:
  - type: TextParser_Python
    file_include_patterns:
      - "*.txt"
      - "*.log"
    config:
      chunk_size: 1000
      chunk_overlap: 100
      chunk_strategy: sentences
      encoding: utf-8
      clean_text: true
      extract_metadata: true
```

#### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `chunk_size` | integer | 1000 | Chunk size in characters |
| `chunk_overlap` | integer | 100 | Overlap between chunks |
| `chunk_strategy` | string | `sentences` | `sentences`, `paragraphs`, `characters` |
| `encoding` | string | `utf-8` | Text encoding (or auto-detect) |
| `clean_text` | boolean | `true` | Remove excessive whitespace |
| `extract_metadata` | boolean | `true` | Extract file statistics |

### TextParser_LlamaIndex

Advanced text parser using LlamaIndex with semantic splitting and code parsing.

**Best for:** Semantic chunking, code files, multi-format support

```yaml
parsers:
  - type: TextParser_LlamaIndex
    file_include_patterns:
      - "*.txt"
      - "*.py"
      - "*.js"
    config:
      chunk_size: 800
      chunk_overlap: 80
      chunk_strategy: semantic
      encoding: utf-8
      clean_text: true
      extract_metadata: true
      preserve_code_structure: true
      detect_language: true
      include_prev_next_rel: true
```

#### Options

| Option | Type | Default | Range | Description |
|--------|------|---------|-------|-------------|
| `chunk_size` | integer | 1000 | 100-50000 | Chunk size in characters |
| `chunk_overlap` | integer | 100 | 0-5000 | Overlap between chunks |
| `chunk_strategy` | string | `semantic` | `characters`, `sentences`, `paragraphs`, `tokens`, `semantic`, `code` | Chunking strategy |
| `encoding` | string | `utf-8` | - | Text encoding |
| `clean_text` | boolean | `true` | - | Clean extracted text |
| `extract_metadata` | boolean | `true` | - | Extract comprehensive metadata |
| `semantic_buffer_size` | integer | 1 | 1-10 | Buffer size for semantic chunking |
| `semantic_breakpoint_percentile_threshold` | integer | 95 | 50-99 | Percentile threshold for breakpoints |
| `token_model` | string | `gpt-3.5-turbo` | - | Tokenizer model for token chunking |
| `preserve_code_structure` | boolean | `true` | - | Preserve code syntax and structure |
| `detect_language` | boolean | `true` | - | Auto-detect programming language |
| `include_prev_next_rel` | boolean | `true` | - | Include relationships between chunks |

---

## Email Parser

### MsgParser_ExtractMsg

Outlook MSG file parser using extract-msg library.

**Best for:** Outlook emails, attachment extraction

```yaml
parsers:
  - type: MsgParser_ExtractMsg
    file_include_patterns:
      - "*.msg"
    config:
      chunk_size: 1000
      chunk_overlap: 100
      chunk_strategy: email_sections
      extract_metadata: true
      extract_attachments: true
      extract_headers: true
      include_attachment_content: true
      clean_text: true
      preserve_formatting: false
      encoding: utf-8
```

#### Options

| Option | Type | Default | Range | Description |
|--------|------|---------|-------|-------------|
| `chunk_size` | integer | 1000 | 100-50000 | Chunk size in characters |
| `chunk_overlap` | integer | 100 | 0-5000 | Overlap between chunks |
| `chunk_strategy` | string | `email_sections` | `sentences`, `paragraphs`, `characters`, `email_sections` | Chunking strategy |
| `extract_metadata` | boolean | `true` | - | Extract email metadata |
| `extract_attachments` | boolean | `true` | - | Extract attachments |
| `extract_headers` | boolean | `true` | - | Extract email headers |
| `include_attachment_content` | boolean | `true` | - | Include attachment content |
| `clean_text` | boolean | `true` | - | Clean text |
| `preserve_formatting` | boolean | `false` | - | Preserve HTML formatting |
| `encoding` | string | `utf-8` | - | Text encoding |

---

## Directory Parser

### DirectoryParser

Recursively parses all files in a directory, automatically detecting file types.

**Best for:** Batch processing, directory ingestion, multi-format processing

```yaml
parsers:
  - type: DirectoryParser
    file_include_patterns:
      - "data/**/*"
    config:
      recursive: true
      exclude_patterns:
        - "*.tmp"
        - "*.bak"
      max_depth: 5
```

#### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `recursive` | boolean | `true` | Recursively process subdirectories |
| `exclude_patterns` | array | `[]` | Glob patterns to exclude |
| `max_depth` | integer | 10 | Maximum directory depth |
| `follow_symlinks` | boolean | `false` | Follow symbolic links |

---

## Complete Example

Here's a comprehensive example processing multiple file types:

```yaml
rag:
  data_processing_strategies:
    - name: universal_processor
      description: "Handles multiple document formats"
      parsers:
        # PDF files with LlamaIndex (try first - lower priority value)
        - type: PDFParser_LlamaIndex
          file_include_patterns:
            - "*.pdf"
            - "*.PDF"
          priority: 0
          config:
            chunk_size: 1200
            chunk_overlap: 150
            chunk_strategy: semantic
            extract_metadata: true
            extract_tables: true

        # PDF fallback with PyPDF2 (try second - higher priority value)
        - type: PDFParser_PyPDF2
          file_include_patterns:
            - "*.pdf"
          priority: 50
          fallback_parser: null
          config:
            chunk_size: 1000
            chunk_overlap: 100
            chunk_strategy: paragraphs

        # Markdown files
        - type: MarkdownParser_LlamaIndex
          file_include_patterns:
            - "*.md"
            - "*.markdown"
          priority: 0
          config:
            chunk_size: 800
            chunk_overlap: 80
            chunk_strategy: headings
            extract_code_blocks: true

        # CSV files
        - type: CSVParser_Pandas
          file_include_patterns:
            - "*.csv"
          priority: 0
          config:
            chunk_size: 500
            chunk_strategy: rows
            extract_metadata: true

        # Excel files
        - type: ExcelParser_LlamaIndex
          file_include_patterns:
            - "*.xlsx"
            - "*.xls"
          priority: 0
          config:
            chunk_size: 500
            chunk_strategy: rows
            combine_sheets: false

        # Word documents
        - type: DocxParser_LlamaIndex
          file_include_patterns:
            - "*.docx"
          priority: 0
          config:
            chunk_size: 1000
            chunk_overlap: 100
            chunk_strategy: paragraphs
            extract_tables: true

        # Plain text and code
        - type: TextParser_LlamaIndex
          file_include_patterns:
            - "*.txt"
            - "*.py"
            - "*.js"
            - "*.html"
          priority: 0
          config:
            chunk_size: 800
            chunk_overlap: 80
            chunk_strategy: semantic
            preserve_code_structure: true

        # Outlook emails
        - type: MsgParser_ExtractMsg
          file_include_patterns:
            - "*.msg"
          priority: 0
          config:
            chunk_strategy: email_sections
            extract_attachments: true
```

## Chunking Strategy Guidelines

| Strategy | Best For | Considerations |
|----------|----------|----------------|
| `sentences` | General text, documentation | Good balance of granularity |
| `paragraphs` | Articles, reports | Preserves natural breaks |
| `characters` | Fixed-size needs | Predictable chunk sizes |
| `semantic` | Technical docs, varied content | Content-aware splitting |
| `sections` / `headings` | Markdown, structured docs | Respects document structure |
| `pages` | PDFs, page-based docs | One chunk per page |
| `rows` | CSV, Excel | Data-oriented chunking |
| `code` | Source code files | Preserves syntax |
| `email_sections` | Emails | Headers/body/signature |

## Next Steps

- [Embedders Reference](./embedders.md) - Configure embedding strategies
- [Extractors Reference](./extractors.md) - Add metadata extraction
- [RAG Guide](./index.md) - Full RAG configuration overview

---

# Extractors Reference

Extractors enrich document chunks with metadata, enabling filtered searches and better retrieval. They run after parsing and add structured data to each chunk.

## Quick Start

Extractors are defined in `data_processing_strategies`:

```yaml
rag:
  data_processing_strategies:
    - name: my_processor
      parsers:
        - type: PDFParser_LlamaIndex
          config:
            chunk_size: 1000
      extractors:
        - type: EntityExtractor
          config:
            entity_types: [PERSON, ORG, DATE]
        - type: KeywordExtractor
          config:
            max_keywords: 10
```

### Common Extractor Properties

| Property | Required | Description |
|----------|----------|-------------|
| `type` | Yes | Extractor type (e.g., `EntityExtractor`) |
| `config` | No | Extractor-specific configuration |
| `priority` | No | Execution order (lower numbers run first) |
| `file_include_patterns` | No | Glob patterns for files to apply to |
| `condition` | No | Condition expression for when to run |

---

## EntityExtractor

Extracts named entities using NER models with regex fallback.

**Extracts:** People, organizations, dates, emails, phone numbers, URLs, products

```yaml
- type: EntityExtractor
  config:
    entity_types: [PERSON, ORG, DATE, EMAIL, PHONE]
    use_fallback: true
    confidence_threshold: 0.7
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `model` | string | `en_core_web_sm` | spaCy NER model |
| `entity_types` | array | See below | Entity types to extract |
| `use_fallback` | boolean | `true` | Use regex fallback |
| `min_entity_length` | integer | 2 | Minimum entity length |
| `merge_entities` | boolean | `true` | Merge adjacent entities |
| `confidence_threshold` | number | 0.7 | Minimum confidence (0-1) |

**Supported Entity Types:**
`PERSON`, `ORG`, `GPE`, `DATE`, `TIME`, `MONEY`, `EMAIL`, `PHONE`, `URL`, `LAW`, `PERCENT`, `PRODUCT`, `EVENT`, `VERSION`, `FAC`, `LOC`

---

## KeywordExtractor

Extracts important keywords using various algorithms.

**Extracts:** Key terms, phrases, n-grams

```yaml
- type: KeywordExtractor
  config:
    algorithm: rake
    max_keywords: 10
    language: en
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `algorithm` | string | `rake` | `rake`, `yake`, `tfidf`, `textrank` |
| `max_keywords` | integer | 10 | Maximum keywords (1-100) |
| `min_length` | integer | 1 | Minimum word length |
| `max_length` | integer | 4 | Maximum word length |
| `min_frequency` | integer | 1 | Minimum frequency |
| `stop_words` | array | - | Custom stop words |
| `language` | string | `en` | Language for YAKE |
| `max_ngram_size` | integer | 3 | Max n-gram size for YAKE |
| `deduplication_threshold` | number | 0.9 | Dedup threshold for YAKE |

---

## DateTimeExtractor

Extracts dates, times, and durations with fuzzy parsing.

**Extracts:** Dates, times, relative expressions, durations

```yaml
- type: DateTimeExtractor
  config:
    fuzzy_parsing: true
    extract_relative: true
    extract_times: true
    default_timezone: UTC
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `fuzzy_parsing` | boolean | `true` | Enable fuzzy date parsing |
| `extract_relative` | boolean | `true` | Extract relative dates |
| `extract_times` | boolean | `true` | Extract time expressions |
| `extract_durations` | boolean | `true` | Extract durations |
| `default_timezone` | string | `UTC` | Default timezone |
| `date_format` | string | `ISO` | Output date format |
| `prefer_dates_from` | string | `current` | `past`, `future`, `current` |

---

## HeadingExtractor

Extracts document headings and builds outline structure.

**Extracts:** Headings, hierarchy, document outline

```yaml
- type: HeadingExtractor
  config:
    max_level: 6
    include_hierarchy: true
    extract_outline: true
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `max_level` | integer | 6 | Maximum heading level (1-6) |
| `include_hierarchy` | boolean | `true` | Include hierarchy structure |
| `extract_outline` | boolean | `true` | Generate document outline |
| `min_heading_length` | integer | 3 | Minimum heading length |
| `enabled` | boolean | `true` | Enable extractor |

---

## LinkExtractor

Extracts URLs, emails, and domain information.

**Extracts:** URLs, email addresses, domains

```yaml
- type: LinkExtractor
  config:
    extract_urls: true
    extract_emails: true
    extract_domains: true
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `extract_urls` | boolean | `true` | Extract URLs |
| `extract_emails` | boolean | `true` | Extract email addresses |
| `extract_domains` | boolean | `true` | Extract unique domains |
| `validate_urls` | boolean | `false` | Validate URL format |
| `resolve_redirects` | boolean | `false` | Resolve URL redirects |

---

## PathExtractor

Extracts file paths, URLs, and S3 paths.

**Extracts:** File paths, URL paths, cloud storage paths

```yaml
- type: PathExtractor
  config:
    extract_file_paths: true
    extract_s3_paths: true
    normalize_paths: true
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `extract_file_paths` | boolean | `true` | Extract file paths |
| `extract_urls` | boolean | `true` | Extract URL paths |
| `extract_s3_paths` | boolean | `true` | Extract S3 paths |
| `validate_paths` | boolean | `false` | Validate path existence |
| `normalize_paths` | boolean | `true` | Normalize path formats |

---

## PatternExtractor

Extracts data using predefined or custom regex patterns.

**Extracts:** Emails, phones, IPs, SSNs, credit cards, versions, custom patterns

```yaml
- type: PatternExtractor
  config:
    predefined_patterns:
      - email
      - phone
      - ip_address
      - version
    custom_patterns:
      - name: order_id
        pattern: "ORD-[0-9]{6}"
        description: "Order identifier"
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `predefined_patterns` | array | `[]` | Built-in patterns to use |
| `custom_patterns` | array | `[]` | Custom regex patterns |
| `case_sensitive` | boolean | `false` | Case-sensitive matching |
| `return_positions` | boolean | `false` | Return match positions |
| `include_context` | boolean | `false` | Include surrounding context |
| `max_matches_per_pattern` | integer | 100 | Max matches per pattern |
| `deduplicate_matches` | boolean | `true` | Remove duplicates |

**Predefined Patterns:**
`email`, `phone`, `url`, `ip`, `ip_address`, `ssn`, `credit_card`, `zip_code`, `file_path`, `version`, `date`

---

## ContentStatisticsExtractor

Calculates readability scores, vocabulary stats, and text structure.

**Extracts:** Word count, readability scores, vocabulary metrics

```yaml
- type: ContentStatisticsExtractor
  config:
    include_readability: true
    include_vocabulary: true
    include_structure: true
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `include_readability` | boolean | `true` | Calculate readability scores |
| `include_vocabulary` | boolean | `true` | Analyze vocabulary |
| `include_structure` | boolean | `true` | Analyze text structure |
| `include_sentiment_indicators` | boolean | `false` | Include sentiment indicators |

---

## SummaryExtractor

Generates extractive summaries using text ranking algorithms.

**Extracts:** Summary sentences, key phrases, text statistics

```yaml
- type: SummaryExtractor
  config:
    summary_sentences: 3
    algorithm: textrank
    include_key_phrases: true
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `summary_sentences` | integer | 3 | Number of sentences (1-10) |
| `algorithm` | string | `textrank` | `textrank`, `lsa`, `luhn`, `lexrank` |
| `include_key_phrases` | boolean | `true` | Extract key phrases |
| `include_statistics` | boolean | `true` | Include text statistics |
| `min_sentence_length` | integer | 10 | Minimum sentence length |
| `max_sentence_length` | integer | 500 | Maximum sentence length |

---

## TableExtractor

Extracts tabular data from documents.

**Extracts:** Tables, headers, cell data

```yaml
- type: TableExtractor
  config:
    output_format: dict
    extract_headers: true
    merge_cells: true
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `output_format` | string | `dict` | `dict`, `list`, `csv`, `markdown` |
| `extract_headers` | boolean | `true` | Extract table headers |
| `merge_cells` | boolean | `true` | Handle merged cells |
| `min_rows` | integer | 2 | Minimum rows for table |

---

## YAKEExtractor

YAKE (Yet Another Keyword Extractor) - unsupervised keyword extraction.

**Extracts:** Keywords using statistical features

```yaml
- type: YAKEExtractor
  config:
    max_keywords: 10
    language: en
    max_ngram_size: 3
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `max_keywords` | integer | 10 | Maximum keywords (1-100) |
| `language` | string | `en` | Language code |
| `max_ngram_size` | integer | 3 | Max n-gram size (1-5) |
| `deduplication_threshold` | number | 0.9 | Dedup threshold (0-1) |

---

## RAKEExtractor

RAKE (Rapid Automatic Keyword Extraction) algorithm.

**Extracts:** Keywords based on word co-occurrence

```yaml
- type: RAKEExtractor
  config:
    max_keywords: 10
    min_length: 1
    max_length: 4
```

Use via `KeywordExtractor` with `algorithm: rake`.

---

## TFIDFExtractor

TF-IDF based keyword extraction.

**Extracts:** Keywords based on term frequency-inverse document frequency

```yaml
- type: TFIDFExtractor
  config:
    max_keywords: 10
    min_frequency: 1
```

Use via `KeywordExtractor` with `algorithm: tfidf`.

---

## Complete Example

Combine multiple extractors for rich metadata:

```yaml
extractors:
  # High priority - entities first
  - type: EntityExtractor
    priority: 100
    config:
      entity_types: [PERSON, ORG, DATE, PRODUCT]
      use_fallback: true

  # Keywords for searchability
  - type: KeywordExtractor
    priority: 90
    config:
      algorithm: yake
      max_keywords: 15

  # Statistics for filtering
  - type: ContentStatisticsExtractor
    priority: 80
    config:
      include_readability: true

  # Patterns for specific data
  - type: PatternExtractor
    priority: 70
    file_include_patterns: ["*.pdf"]
    config:
      predefined_patterns: [email, phone, date]
      custom_patterns:
        - name: case_number
          pattern: "CASE-[A-Z]{2}-[0-9]{6}"
```

## Using Extracted Metadata

Query with metadata filters:

```bash
# Filter by entity
lf rag query --database main_db --filter "entities.ORG:Acme Corp" "contracts"

# Filter by keyword
lf rag query --database main_db --filter "keywords:merger" "recent news"

# Filter by date
lf rag query --database main_db --filter "dates:2024" "quarterly reports"
```

## Next Steps

- [Databases Reference](./databases.md) - Configure vector stores
- [Retrieval Strategies](./retrieval-strategies.md) - Configure retrieval
- [RAG Guide](./index.md) - Full RAG overview

---

# Retrieval Strategies Reference

Retrieval strategies control how documents are searched and ranked. LlamaFarm provides strategies from basic similarity search to advanced multi-stage retrieval.

## Quick Start

Retrieval strategies are configured in `retrieval_strategies` within a database:

```yaml
rag:
  databases:
    - name: main_db
      retrieval_strategies:
        - name: semantic_search
          type: BasicSimilarityStrategy
          config:
            top_k: 10
          default: true
```

### Common Properties

| Property | Required | Description |
|----------|----------|-------------|
| `name` | Yes | Strategy identifier |
| `type` | Yes | Strategy type |
| `config` | No | Strategy-specific options |
| `default` | No | Whether this is the default strategy |

---

## BasicSimilarityStrategy

Simple vector similarity search. Fast and effective for most use cases.

**Best for:** General semantic search, simple queries

```yaml
- name: semantic_search
  type: BasicSimilarityStrategy
  config:
    top_k: 10
    distance_metric: cosine
    score_threshold: 0.5
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `top_k` | integer | 10 | Number of results (1-1000) |
| `distance_metric` | string | `cosine` | `cosine`, `euclidean`, `manhattan`, `dot` |
| `score_threshold` | number | `null` | Minimum similarity score (0-1) |

---

## MetadataFilteredStrategy

Filter results using document metadata before or after retrieval.

**Best for:** Faceted search, filtering by date/type/category

```yaml
- name: filtered_search
  type: MetadataFilteredStrategy
  config:
    top_k: 10
    filter_mode: pre
    filters:
      doc_type: report
      year: 2024
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `top_k` | integer | 10 | Number of results (1-1000) |
| `filters` | object | `{}` | Metadata key-value filters |
| `filter_mode` | string | `pre` | `pre` (before retrieval) or `post` (after) |
| `fallback_multiplier` | integer | 3 | Multiplier for post-filtering |

### Filter Examples

```yaml
filters:
  # Exact match
  doc_type: report

  # Multiple values (OR)
  category: [finance, legal]

  # Numeric
  year: 2024
```

---

## MultiQueryStrategy

Generate multiple query variations for better recall.

**Best for:** Complex queries, improving recall

```yaml
- name: multi_query
  type: MultiQueryStrategy
  config:
    num_queries: 3
    top_k: 10
    aggregation_method: weighted
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `num_queries` | integer | 3 | Query variations (1-10) |
| `top_k` | integer | 10 | Results per query (1-1000) |
| `aggregation_method` | string | `weighted` | `max`, `mean`, `weighted`, `reciprocal_rank` |
| `query_weights` | array | `null` | Weights for each query |

### Aggregation Methods

| Method | Description |
|--------|-------------|
| `max` | Take highest score per document |
| `mean` | Average scores across queries |
| `weighted` | Weight by query importance |
| `reciprocal_rank` | RRF fusion of rankings |

---

## RerankedStrategy

Two-stage retrieval with score-based reranking.

**Best for:** Balancing recall and precision

```yaml
- name: reranked
  type: RerankedStrategy
  config:
    initial_k: 30
    final_k: 10
    normalize_scores: true
    rerank_factors:
      similarity_weight: 0.7
      recency_weight: 0.2
      length_weight: 0.1
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `initial_k` | integer | 30 | Initial candidates (10-1000) |
| `final_k` | integer | 10 | Final results (1-100) |
| `normalize_scores` | boolean | `true` | Normalize before combining |
| `rerank_factors` | object | See below | Weighting factors |

### Rerank Factors

```yaml
rerank_factors:
  similarity_weight: 0.7   # Vector similarity
  recency_weight: 0.1      # Document freshness
  length_weight: 0.1       # Content length
  metadata_weight: 0.1     # Metadata relevance
```

---

## HybridUniversalStrategy

Combine multiple retrieval strategies with score fusion.

**Best for:** Complex retrieval needs, combining sparse and dense

```yaml
- name: hybrid
  type: HybridUniversalStrategy
  config:
    final_k: 10
    combination_method: weighted_average
    strategies:
      - type: BasicSimilarityStrategy
        weight: 0.6
        config:
          top_k: 20
      - type: MetadataFilteredStrategy
        weight: 0.4
        config:
          top_k: 20
          filters:
            doc_type: report
```

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `strategies` | array | - | Sub-strategies to combine (2-5) |
| `combination_method` | string | `weighted_average` | Fusion method |
| `final_k` | integer | 10 | Final results (1-1000) |

### Combination Methods

| Method | Description |
|--------|-------------|
| `weighted_average` | Weighted average of scores |
| `rank_fusion` | Reciprocal rank fusion |
| `score_fusion` | Direct score fusion |

### Sub-Strategy Format

```yaml
strategies:
  - type: BasicSimilarityStrategy  # Strategy type
    weight: 0.6                     # Relative weight (0-1)
    config:                         # Strategy config
      top_k: 20
```

**Available sub-strategy types:**
- `BasicSimilarityStrategy`
- `MetadataFilteredStrategy`
- `MultiQueryStrategy`
- `RerankedStrategy`

---

## CrossEncoderRerankedStrategy

Neural reranking using cross-encoder models for highest accuracy.

**Best for:** High-precision search, quality-critical applications

```yaml
- name: reranked_search
  type: CrossEncoderRerankedStrategy
  config:
    model_name: reranker
    initial_k: 30
    final_k: 5
    relevance_threshold: 0.3
```

**Requires a reranker model in runtime:**

```yaml
runtime:
  models:
    - name: reranker
      provider: universal
      model: cross-encoder/ms-marco-MiniLM-L-6-v2
      base_url: http://127.0.0.1:11540
```

### Options

| Option | Type | Default | Required | Description |
|--------|------|---------|----------|-------------|
| `model_name` | string | - | Yes | Name of model from `runtime.models` |
| `initial_k` | integer | 30 | No | Initial candidates (10-100) |
| `final_k` | integer | 10 | No | Final results (1-50) |
| `base_strategy` | string | `BasicSimilarityStrategy` | No | Initial retrieval strategy |
| `base_strategy_config` | object | `{}` | No | Config for base strategy |
| `relevance_threshold` | number | 0.0 | No | Minimum relevance (0-1) |
| `timeout` | integer | 60 | No | Request timeout in seconds |

### Recommended Models

| Model | Size | Speed | Multilingual |
|-------|------|-------|--------------|
| `cross-encoder/ms-marco-MiniLM-L-6-v2` | 90MB | Very Fast | No |
| `BAAI/bge-reranker-base` | 280MB | Fast | Yes |
| `BAAI/bge-reranker-v2-m3` | 560MB | Medium | Yes |

See [Advanced Retrieval Strategies](./advanced-retrieval.md) for detailed model recommendations.

---

## MultiTurnRAGStrategy

Query decomposition for complex, multi-part questions.

**Best for:** Complex queries, multi-aspect questions

```yaml
- name: multi_turn
  type: MultiTurnRAGStrategy
  config:
    model_name: query_decomposer
    max_sub_queries: 3
    complexity_threshold: 50
    final_top_k: 10
    enable_reranking: true
```

**Requires a decomposition model in runtime:**

```yaml
runtime:
  models:
    - name: query_decomposer
      provider: openai
      model: gemma3:1b
      base_url: http://localhost:11434/v1
```

### Options

| Option | Type | Default | Required | Description |
|--------|------|---------|----------|-------------|
| `model_name` | string | - | Yes | Decomposition model name |
| `max_sub_queries` | integer | 3 | No | Max sub-queries (1-5) |
| `complexity_threshold` | integer | 50 | No | Min chars to trigger decomposition |
| `min_query_length` | integer | 20 | No | Min length per sub-query |
| `base_strategy` | string | `BasicSimilarityStrategy` | No | Strategy for sub-queries |
| `base_strategy_config` | object | `{}` | No | Base strategy config |
| `sub_query_top_k` | integer | 10 | No | Results per sub-query |
| `final_top_k` | integer | 10 | No | Final merged results |
| `enable_reranking` | boolean | `false` | No | Rerank each sub-query |
| `reranker_strategy` | string | `CrossEncoderRerankedStrategy` | No | Reranking strategy |
| `reranker_config` | object | `{}` | No | Reranker configuration |
| `dedup_similarity_threshold` | number | 0.95 | No | Dedup threshold (0.5-1) |
| `max_workers` | integer | 3 | No | Parallel workers (1-10) |

See [Advanced Retrieval Strategies](./advanced-retrieval.md) for detailed configuration.

---

## Additional Retriever Types

These types are also available for use in database configurations:

| Type | Description |
|------|-------------|
| `VectorRetriever` | Basic vector search |
| `HybridRetriever` | Hybrid sparse/dense |
| `BM25Retriever` | Keyword-based BM25 |
| `RerankedRetriever` | Generic reranker |
| `GraphRetriever` | Graph-based retrieval |
| `ElasticRetriever` | Elasticsearch backend |

---

## Strategy Selection Guide

| Query Type | Recommended Strategy | Why |
|------------|---------------------|-----|
| Simple, specific | `BasicSimilarityStrategy` | Fast, effective |
| Filtered search | `MetadataFilteredStrategy` | Precise filtering |
| Broad topics | `MultiQueryStrategy` | Better recall |
| Quality-critical | `CrossEncoderRerankedStrategy` | Highest accuracy |
| Complex questions | `MultiTurnRAGStrategy` | Query decomposition |
| Mixed requirements | `HybridUniversalStrategy` | Combine approaches |

---

## Multiple Strategies Example

Configure multiple strategies and switch at query time:

```yaml
retrieval_strategies:
  # Fast default
  - name: fast
    type: BasicSimilarityStrategy
    config:
      top_k: 10
    default: true

  # Filtered by type
  - name: by_type
    type: MetadataFilteredStrategy
    config:
      top_k: 10
      filter_mode: pre

  # High accuracy
  - name: accurate
    type: CrossEncoderRerankedStrategy
    config:
      model_name: reranker
      initial_k: 30
      final_k: 5

  # Complex queries
  - name: complex
    type: MultiTurnRAGStrategy
    config:
      model_name: query_decomposer
      enable_reranking: true

default_retrieval_strategy: fast
```

Query with specific strategies:

```bash
# Use default (fast)
lf rag query --database main_db "simple question"

# Use accurate strategy
lf rag query --database main_db --retrieval-strategy accurate "important question"

# Use complex for multi-part questions
lf rag query --database main_db --retrieval-strategy complex \
  "What are the benefits of X and how does it compare to Y?"
```

## Next Steps

- [Advanced Retrieval Strategies](./advanced-retrieval.md) - Detailed cross-encoder and multi-turn setup
- [Databases Reference](./databases.md) - Configure vector stores
- [RAG Guide](./index.md) - Full RAG overview

---

# Advanced Retrieval Strategies

LlamaFarm supports advanced retrieval strategies that improve result quality through reranking and query decomposition.

## Cross-Encoder Reranking

### Overview

Cross-encoder reranking improves retrieval accuracy by reranking initial candidates using a specialized model. This is **10-100x faster** than LLM-based reranking while often providing better relevance scores.

**How it works:**
1. Initial retrieval gets 30+ candidates using fast vector search
2. Cross-encoder model jointly encodes query+document pairs
3. Results are reranked by relevance scores
4. Top N results returned

**Performance:**
- Speed: Fast (50-400 docs/sec)
- Accuracy: Very High
- Best for: Simple, focused questions requiring accurate ranking

### Configuration

Add a reranker model to your runtime configuration using Universal Runtime:

```yaml
runtime:
  models:
    - name: reranker
      description: Fast cross-encoder for document reranking (HuggingFace model via Universal Runtime)
      provider: universal
      model: cross-encoder/ms-marco-MiniLM-L-6-v2
      base_url: http://127.0.0.1:11540
```

Then configure the retrieval strategy in your database:

```yaml
rag:
  databases:
    - name: main_database
      type: ChromaStore
      retrieval_strategies:
        - name: reranked_search
          type: CrossEncoderRerankedStrategy
          config:
            model_name: reranker  # References runtime.models
            initial_k: 30         # Initial candidates
            final_k: 5            # Final results after reranking
            base_strategy: BasicSimilarityStrategy
            base_strategy_config:
              distance_metric: cosine
            relevance_threshold: 0.0
            timeout: 60
          default: true
```

### Recommended Reranking Models

These HuggingFace cross-encoder models are automatically downloaded when first used via Universal Runtime:

#### Model Comparison

| Model | Size | Speed | Accuracy | Languages | Best For |
|-------|------|-------|----------|-----------|----------|
| **ms-marco-MiniLM-L-6-v2** | ~90MB | Very Fast | High | English | Default, best balance |
| **bge-reranker-v2-m3** | ~560MB | Medium | Highest | 100+ | Production, multilingual |
| **bge-reranker-base** | ~280MB | Fast | High | 100+ | Good balance |

#### Detailed Model Info

**cross-encoder/ms-marco-MiniLM-L-6-v2 (Recommended Default)**
```yaml
model: cross-encoder/ms-marco-MiniLM-L-6-v2
```
- **Strengths:** Very small (~90MB), fast, excellent accuracy, widely used
- **Weaknesses:** English only
- **Use when:** Need fast, accurate reranking with minimal overhead (default choice)
- **Performance:** ~300-500 docs/sec

**BAAI/bge-reranker-v2-m3 (Recommended for Multilingual)**
```yaml
model: BAAI/bge-reranker-v2-m3
```
- **Strengths:** Best accuracy, multilingual (100+ languages), state-of-the-art
- **Weaknesses:** Larger size, slightly slower
- **Use when:** Quality is critical, multilingual support needed
- **Performance:** ~100-200 docs/sec

**BAAI/bge-reranker-base (Good Balance)**
```yaml
model: BAAI/bge-reranker-base
```
- **Strengths:** Good accuracy, multilingual, medium size
- **Weaknesses:** Slower than MiniLM
- **Use when:** Need multilingual support with reasonable speed
- **Performance:** ~150-300 docs/sec

### Configuration Options

| Parameter | Default | Description |
|-----------|---------|-------------|
| `model_name` | `reranker` | Name of model from `runtime.models` |
| `initial_k` | 30 | Number of candidates before reranking |
| `final_k` | 10 | Number of results after reranking |
| `base_strategy` | `BasicSimilarityStrategy` | Initial retrieval strategy |
| `relevance_threshold` | 0.0 | Minimum score to include (0-1 range, auto-normalized) |
| `timeout` | 60 | Request timeout in seconds |

### Complete Example Configuration

Here's a full `llamafarm.yaml` example with cross-encoder reranking:

```yaml
version: v1
name: my-project
namespace: default

runtime:
  default_model: default
  models:
    - name: default
      provider: ollama
      model: gemma3:1b
      base_url: http://localhost:11434/v1

    # Reranker model for CrossEncoderRerankedStrategy
    - name: reranker
      description: Fast cross-encoder for document reranking (HuggingFace model via Universal Runtime)
      provider: universal
      model: cross-encoder/ms-marco-MiniLM-L-6-v2
      base_url: http://127.0.0.1:11540

rag:
  databases:
    - name: main_database
      type: ChromaStore
      config:
        collection_name: documents
        distance_function: cosine
        port: 8000

      # Embedding strategy
      embedding_strategies:
        - name: default_embeddings
          type: OllamaEmbedder
          config:
            model: nomic-embed-text
            dimension: 768
            batch_size: 16
          priority: 0

      # Retrieval strategies
      retrieval_strategies:
        # Basic search (fast)
        - name: basic_search
          type: BasicSimilarityStrategy
          config:
            distance_metric: cosine
            top_k: 10
          default: false

        # Reranked search (accurate)
        - name: reranked_search
          type: CrossEncoderRerankedStrategy
          config:
            model_name: reranker
            initial_k: 30
            final_k: 5
            base_strategy: BasicSimilarityStrategy
            base_strategy_config:
              distance_metric: cosine
            relevance_threshold: 0.0
            timeout: 60
          default: true

      default_embedding_strategy: default_embeddings
      default_retrieval_strategy: reranked_search
```

### Usage

The reranker model is automatically downloaded from HuggingFace when first used:

```bash
# Query with reranking (uses default strategy)
lf rag query --database main_database "What are the differences between llama and alpaca fibers?"

# Explicitly specify strategy
lf rag query --database main_database --retrieval-strategy reranked_search "your question"

# Use basic search for speed
lf rag query --database main_database --retrieval-strategy basic_search "simple question"
```

## Multi-Turn RAG

### Overview

Multi-turn RAG handles complex, multi-part queries by:
1. Detecting query complexity
2. Decomposing complex queries into focused sub-queries
3. Retrieving documents for each sub-query in parallel
4. Optionally reranking each sub-query's results
5. Merging and deduplicating final results

**Performance:**
- Speed: Medium-slow (1-3 seconds)
- Accuracy: Very High for complex queries
- Best for: Context-heavy questions with multiple aspects

### When to Use

**Good candidates for multi-turn RAG:**
- "What are the differences between X and Y, and how does this affect Z?"
- "Compare A and B, and also explain their applications"
- "What is X? How does it work? What are the benefits?"

**Not ideal for:**
- Short, simple questions ("What is AI?")
- Single-aspect queries ("Describe neural networks")

### Configuration

Add a query decomposition model:

```yaml
runtime:
  models:
    - name: query_decomposer
      description: Model for query decomposition
      provider: openai
      model: gemma3:1b  # Small, fast model works well
      base_url: http://localhost:11434/v1
```

Configure the multi-turn strategy:

```yaml
rag:
  databases:
    - name: main_database
      retrieval_strategies:
        - name: multi_turn_search
          type: MultiTurnRAGStrategy
          config:
            model_name: query_decomposer
            max_sub_queries: 3
            complexity_threshold: 50  # Min chars to trigger decomposition
            min_query_length: 20
            base_strategy: BasicSimilarityStrategy
            base_strategy_config:
              distance_metric: cosine
            sub_query_top_k: 10       # Results per sub-query
            final_top_k: 5            # Final merged results
            # Enable reranking for each sub-query
            enable_reranking: true
            reranker_strategy: CrossEncoderRerankedStrategy
            reranker_config:
              model_name: reranker
              initial_k: 15
              final_k: 10
              timeout: 60
            dedup_similarity_threshold: 0.95
            max_workers: 3            # Parallel sub-query retrieval
          default: true
```

### Complexity Detection

Queries are considered complex if they:
- Are longer than `complexity_threshold` characters (default: 50)
- Contain patterns like: `and`, `also`, `additionally`, `furthermore`, `moreover`
- Have multiple question marks

**Examples:**

âœ… Complex (will decompose):
- "What are the current paradigm limitations in AI development, and how do centralized cloud-based models compare to local AI deployment?" (178 chars + "and")
- "Explain attention mechanisms and also describe transformers" (62 chars + "also")

âŒ Simple (won't decompose):
- "What is AI?" (12 chars)
- "Explain machine learning" (25 chars, no patterns)

### Recommended Models

#### Query Decomposition Models

| Model | Size | Speed | Format Adherence | Best For |
|-------|------|-------|------------------|----------|
| **gemma3:1b** | ~1.3GB | Fast | Excellent | Recommended - best balance |
| **qwen3:1.7B** | ~1.7GB | Medium | Good | Alternative option |
| **gemma3:3b** | ~3GB | Slower | Excellent | High accuracy needs |

**gemma3:1b (Recommended)**
```bash
ollama pull gemma3:1b
```
- **Strengths:** Excellent at following XML format, good decomposition quality
- **Weaknesses:** Slightly larger than alternatives
- **Use when:** Need reliable query decomposition (default choice)
- **Tested:** Works well with temperature 0.3

**qwen3:1.7B (Alternative)**
```bash
ollama pull qwen3:1.7B
```
- **Strengths:** Fast, good balance
- **Weaknesses:** Can struggle with format adherence at very small sizes
- **Use when:** Need maximum speed
- **Note:** Use qwen3:1.7B or larger (not 0.6B)

#### Reranking Models (Optional but Recommended)

For best results, enable reranking using Universal Runtime with HuggingFace models (automatically downloaded):

- **Recommended:** `cross-encoder/ms-marco-MiniLM-L-6-v2` (~90MB, very fast)
- **Best accuracy:** `BAAI/bge-reranker-v2-m3` (~560MB, multilingual)

See [Cross-Encoder Reranking Models](#recommended-reranking-models) above for detailed comparison.

### Configuration Options

| Parameter | Default | Description |
|-----------|---------|-------------|
| `model_name` | - | Name of decomposition model from `runtime.models` |
| `max_sub_queries` | 3 | Maximum sub-queries to generate |
| `complexity_threshold` | 50 | Min chars to trigger decomposition |
| `min_query_length` | 20 | Min length for each sub-query |
| `sub_query_top_k` | 10 | Results per sub-query |
| `final_top_k` | 10 | Final results after merging |
| `enable_reranking` | `false` | Enable reranking per sub-query |
| `max_workers` | 3 | Parallel workers for sub-queries |
| `dedup_similarity_threshold` | 0.95 | Deduplication threshold |

### Complete Example Configuration

Here's a full `llamafarm.yaml` example with multi-turn RAG and reranking:

```yaml
version: v1
name: my-project
namespace: default

runtime:
  default_model: default
  models:
    - name: default
      provider: ollama
      model: gemma3:1b
      base_url: http://localhost:11434/v1

    # Query decomposition model for MultiTurnRAGStrategy
    - name: query_decomposer
      description: Small fast model for query decomposition
      provider: openai
      model: gemma3:1b
      base_url: http://localhost:11434/v1

    # Reranker model for CrossEncoderRerankedStrategy
    - name: reranker
      description: Fast cross-encoder for document reranking (HuggingFace model via Universal Runtime)
      provider: universal
      model: cross-encoder/ms-marco-MiniLM-L-6-v2
      base_url: http://127.0.0.1:11540

rag:
  databases:
    - name: main_database
      type: ChromaStore
      config:
        collection_name: documents
        distance_function: cosine
        port: 8000

      # Embedding strategy
      embedding_strategies:
        - name: default_embeddings
          type: OllamaEmbedder
          config:
            model: nomic-embed-text
            dimension: 768
            batch_size: 16
          priority: 0

      # Retrieval strategies
      retrieval_strategies:
        # Basic search (fastest)
        - name: basic_search
          type: BasicSimilarityStrategy
          config:
            distance_metric: cosine
            top_k: 10
          default: false

        # Multi-turn RAG with reranking (best for complex queries)
        - name: multi_turn_search
          type: MultiTurnRAGStrategy
          config:
            model_name: query_decomposer
            max_sub_queries: 3
            complexity_threshold: 50
            min_query_length: 20
            base_strategy: BasicSimilarityStrategy
            base_strategy_config:
              distance_metric: cosine
            sub_query_top_k: 10
            final_top_k: 5
            # Enable reranking for each sub-query
            enable_reranking: true
            reranker_strategy: CrossEncoderRerankedStrategy
            reranker_config:
              model_name: reranker
              initial_k: 15
              final_k: 10
              base_strategy: BasicSimilarityStrategy
              base_strategy_config:
                distance_metric: cosine
              batch_size: 16
              normalize_scores: true
              relevance_threshold: 0.0
              max_chars_per_doc: 1000
            dedup_similarity_threshold: 0.95
            max_workers: 3
          default: true

      default_embedding_strategy: default_embeddings
      default_retrieval_strategy: multi_turn_search
```

**Key Configuration Points:**
- `query_decomposer` model: Uses gemma3:1b for reliable XML format adherence
- `reranker` model: Uses bce-reranker for good balance of speed/accuracy
- `enable_reranking: true`: Each sub-query gets reranked for best results
- `complexity_threshold: 50`: Queries shorter than 50 chars use simple retrieval
- `max_workers: 3`: Process up to 3 sub-queries in parallel

### Usage

```bash
# Complex query (will decompose)
lf rag query --database main_database \
  "What are the current paradigm limitations in AI development, and how do centralized cloud-based models compare to local AI deployment in terms of cost and performance?"

# The system will:
# 1. Detect complexity (length + "and" pattern)
# 2. Decompose into sub-queries like:
#    - "What are current paradigm limitations in AI development?"
#    - "How do centralized cloud-based AI models work?"
#    - "What are the costs and performance of local AI deployment?"
# 3. Retrieve for each sub-query in parallel
# 4. Rerank each sub-query's results (if enabled)
# 5. Merge and deduplicate to top 5 results
```

### Monitoring

Check RAG server logs to see decomposition in action:

```
[info] Query complexity detected (reason='pattern: \band\b')
[info] Query is complex, decomposing into sub-queries
[info] Decomposed query into 3 sub-queries
[info] Retrieving for 3 sub-queries in parallel
[info] Merging and deduplicating results
```

## Combining Strategies

You can configure multiple strategies in a single database:

```yaml
rag:
  databases:
    - name: main_database
      retrieval_strategies:
        # Fast, basic search
        - name: basic_search
          type: BasicSimilarityStrategy
          config:
            top_k: 10
          default: false

        # Accurate reranked search for simple queries
        - name: reranked_search
          type: CrossEncoderRerankedStrategy
          config:
            model_name: reranker
            initial_k: 30
            final_k: 5
          default: false

        # Comprehensive search for complex queries
        - name: multi_turn_search
          type: MultiTurnRAGStrategy
          config:
            model_name: query_decomposer
            enable_reranking: true
            reranker_config:
              model_name: reranker
          default: true  # Use this by default
```

Then specify the strategy at query time:

```bash
# Use default (multi-turn)
lf rag query --database main_database "complex multi-part question"

# Use basic search for speed
lf rag query --database main_database --retrieval-strategy basic_search "simple question"

# Use reranking for accuracy
lf rag query --database main_database --retrieval-strategy reranked_search "focused question"
```

## Performance Tips

### Cross-Encoder Reranking

1. **Adjust `initial_k` and `final_k`:**
   - Higher `initial_k` = better recall, slower
   - Lower `final_k` = faster LLM processing
   - Good defaults: `initial_k: 30`, `final_k: 5`

2. **Model selection:**
   - Use `cross-encoder/ms-marco-MiniLM-L-6-v2` for best speed/size tradeoff
   - Use `BAAI/bge-reranker-v2-m3` when accuracy is critical or multilingual support needed

3. **Timeout adjustment:**
   - Default `60` seconds works for most cases
   - Increase if processing very large document sets

### Multi-Turn RAG

1. **Complexity threshold:**
   - Lower (e.g., 30) = more queries decomposed, slower but thorough
   - Higher (e.g., 100) = only very complex queries decomposed, faster
   - Default `50` is a good balance

2. **Parallel workers:**
   - More workers = faster parallel retrieval
   - Limited by CPU cores and model concurrency
   - Default `3` works well

3. **Reranking toggle:**
   - `enable_reranking: true` = highest accuracy, slower
   - `enable_reranking: false` = faster, still good quality
   - Use reranking for critical applications

## Troubleshooting

### Cross-Encoder Issues

**Problem:** Error loading reranker model
```
ValueError: Model 'reranker' not found in runtime.models
```

**Solution:** Ensure the model exists in `runtime.models` with `provider: universal` and the name matches:
```yaml
runtime:
  models:
    - name: reranker  # Must match model_name in strategy config
      provider: universal
      model: cross-encoder/ms-marco-MiniLM-L-6-v2
      base_url: http://127.0.0.1:11540
```

**Problem:** Reranking is slow

**Solution:**
- Reduce `initial_k` (fewer candidates to rerank)
- Use a smaller, faster model (e.g., `cross-encoder/ms-marco-MiniLM-L-6-v2` instead of `BAAI/bge-reranker-v2-m3`)
- Increase `timeout` if getting timeout errors with large document sets

### Multi-Turn Issues

**Problem:** Query decomposition returns original query (no decomposition)
```
[warning] No <question> tags found in LLM response
```

**Solution:**
- Use a more capable model (gemma3:1b recommended over qwen3-0.6B)
- Check model is running: `ollama list`
- Lower temperature if model is too creative

**Problem:** Sub-queries are nonsensical

**Solution:**
- Increase temperature slightly (0.3 recommended)
- Try a different model
- Check your complexity threshold isn't too low

**Problem:** Too slow for production

**Solution:**
- Increase `complexity_threshold` to decompose fewer queries
- Disable `enable_reranking` or use it selectively
- Reduce `max_sub_queries` to 2
- Reduce `sub_query_top_k` to 5

## Next Steps

- [RAG Guide](./index.md) - Core RAG concepts
- [CLI Reference](../cli/index.md) - Command usage
- [Configuration Guide](../configuration/index.md) - Full YAML reference
- [Examples](../examples/index.md) - See real-world configurations

---

# Models & Runtime

LlamaFarm focuses on inference rather than fine-tuning. The runtime section of `llamafarm.yaml` describes how chat completions are executedâ€”whether against local Ollama, Lemonade, a vLLM gateway, or a remote hosted provider.

## Multi-Model Support

LlamaFarm supports configuring multiple models in a single project. You can switch between models via CLI or API:

```yaml
runtime:
  default_model: fast  # Which model to use by default

  models:
    fast:
      description: "Fast Ollama model for quick responses"
      provider: ollama
      model: gemma3:1b
      prompt_format: unstructured

    powerful:
      description: "More capable model"
      provider: ollama
      model: qwen3:8b
```

**Using multi-model:**
- CLI: `lf chat --model powerful "your question"`
- CLI: `lf models list` (shows all available models)
- API: `POST /v1/projects/{ns}/{id}/chat/completions` with `{"model": "powerful", ...}`

**Legacy single-model configs are still supported** and automatically converted internally.

## Runtime Responsibilities

- Route chat requests to the configured provider.
- Respect instructor modes (`tools`, `json`, `md_json`, etc.) when available.
- Surface provider errors directly (incorrect model name, missing API key).
- Cooperate with agent handlers (simple chat, structured output, RAG-aware prompts).

## Choosing a Provider

| Use Case | Configuration |
| -------- | ------------- |
| **Local models (Ollama)** | `provider: ollama` (omit API key). Supports models pulled via `ollama pull`. |
| **Local models (Lemonade)** | `provider: lemonade` with GGUF models. Hardware-accelerated (NPU/GPU). See [Lemonade Setup](#lemonade-runtime) below. |
| **Self-hosted vLLM / OpenAI-compatible** | `provider: openai`, set `base_url` to your gateway, `api_key` as required. |
| **Hosted APIs (OpenAI, Anthropic via proxy, Together, LM Studio)** | `provider: openai`, set `base_url` if not using api.openai.com, provide API key. |

Example using vLLM locally:

```yaml
runtime:
  models:
    - name: vllm-model
      provider: openai
      model: mistral-small
      base_url: http://localhost:8000/v1
      api_key: sk-local-placeholder
      instructor_mode: json
      default: true
```

## Agent Handlers

LlamaFarm selects an agent handler based on configuration:

- **Simple chat** â€“ direct user/system prompts, suitable for models without tool support.
- **Structured chat** â€“ uses instructor modes (`tools`, `json`) for models that support function/tool calls.
- **RAG chat** â€“ augments prompts with retrieved context, citations, and guardrails.
- **Classifier / Custom** â€“ future handlers for specialized workflows.

Choose handler behaviour in your project configuration (e.g., advanced agents defined by the server). Ensure the model supports the required featuresâ€”some small models (TinyLlama) donâ€™t handle tools, so stick with simple chat.

## Lemonade Runtime

Lemonade is a high-performance local runtime that runs GGUF models with NPU/GPU acceleration. It's an alternative to Ollama with excellent performance on Apple Silicon and other hardware.

### Quick Setup

**1. Install Lemonade SDK:**
```bash
uv pip install lemonade-sdk
```

**2. Download a model:**
```bash
# Balanced 4B model (recommended)
uv run lemonade-server-dev pull user.Qwen3-4B \
  --checkpoint unsloth/Qwen3-4B-GGUF:Q4_K_M \
  --recipe llamacpp
```

**3. Start Lemonade server:**
```bash
# From the llamafarm project root
LEMONADE_MODEL=user.Qwen3-4B nx start lemonade
```

> **Note:** The `nx start lemonade` command automatically picks up configuration from your `llamafarm.yaml`. Currently, Lemonade must be manually started. In the future, Lemonade will run as a container and be auto-started by the LlamaFarm server.

**4. Configure your project:**
```yaml
runtime:
  models:
    - name: lemon
      description: "Lemonade local model"
      provider: lemonade
      model: user.Qwen3-4B
      base_url: "http://127.0.0.1:11534/v1"
      default: true
      lemonade:
        backend: llamacpp  # llamacpp (GGUF), onnx, or transformers
        port: 11534
        context_size: 32768
```

### Key Features

- **Hardware acceleration**: Automatically detects and uses Metal (macOS), CUDA (NVIDIA), Vulkan (AMD/Intel), or CPU
- **Multiple backends**: llamacpp (GGUF models), ONNX, or Transformers (PyTorch)
- **OpenAI-compatible API**: Drop-in replacement for OpenAI-compatible endpoints
- **Port 11534**: Default port (different from LlamaFarm's 8000 and Ollama's 11434)

### Multi-Model with Lemonade

Run multiple Lemonade instances on different ports:

```yaml
runtime:
  models:
    - name: lemon-fast
      provider: lemonade
      model: user.Qwen3-0.6B
      base_url: "http://127.0.0.1:11534/v1"
      lemonade:
        port: 11534

    - name: lemon-powerful
      provider: lemonade
      model: user.Qwen3-8B
      base_url: "http://127.0.0.1:11535/v1"
      lemonade:
        port: 11535
```

Start each instance in a separate terminal (from the llamafarm project root):
```bash
# Terminal 1
LEMONADE_MODEL=user.Qwen3-0.6B LEMONADE_PORT=11534 nx start lemonade

# Terminal 2
LEMONADE_MODEL=user.Qwen3-8B LEMONADE_PORT=11535 nx start lemonade
```

> **Note:** In the future, Lemonade instances will run as containers and be auto-started by the LlamaFarm server.

### More Information

For detailed setup instructions, model recommendations, and troubleshooting, see:
- [Lemonade Quickstart](#quick-setup)
- [Lemonade Runtime overview](#lemonade-runtime)

## Universal Runtime

The Universal Runtime is LlamaFarm's most versatile runtime provider, supporting **any HuggingFace model** through PyTorch Transformers and Diffusers. Unlike Ollama (GGUF-only) or Lemonade (optimized quantized models), Universal Runtime provides access to the entire HuggingFace Hub ecosystem.

### Supported Model Formats

**Current Support (Production):**
- **HuggingFace Transformers** â€“ All PyTorch text models (GPT-2, Llama, Mistral, Qwen, Phi, BERT, etc.)
- **HuggingFace Diffusers** â€“ All PyTorch diffusion models (Stable Diffusion, SDXL, FLUX)
- **Model Types**: Text Generation, Embeddings, Image Generation, Vision Classification, Audio Processing, Multimodal

**Coming Soon:**
- **ONNX Runtime** â€“ 2-5x faster inference with automatic model conversion
- **TensorRT** â€“ GPU-optimized inference for NVIDIA hardware

### Quick Setup

**1. Start Universal Runtime server:**
```bash
# From project root (recommended)
nx start universal-runtime

# Or with custom port
LF_RUNTIME_PORT=8080 nx start universal-runtime
```

**2. Configure your project:**
```yaml
runtime:
  models:
    - name: phi-2
      description: "Fast small language model"
      provider: universal
      model: microsoft/phi-2
      base_url: http://127.0.0.1:11540
      transformers:
        device: auto              # auto, cuda, mps, cpu
        dtype: auto               # auto, fp16, fp32, bf16
        trust_remote_code: true
        model_type: text          # text, embedding, image
```

**3. Start chatting:**
```bash
lf chat --model phi-2 "Explain quantum computing"
```

### Configuration Examples

**Example 1: Multi-Model Setup (Chat + Embeddings + Images)**

```yaml
runtime:
  default_model: balanced

  models:
    # Fast chat for quick responses
    - name: fast
      provider: universal
      model: Qwen/Qwen2.5-0.5B-Instruct
      base_url: http://127.0.0.1:11540
      transformers:
        device: auto
        dtype: auto

    # Balanced chat for quality
    - name: balanced
      provider: universal
      model: microsoft/phi-2
      base_url: http://127.0.0.1:11540
      transformers:
        device: auto
        dtype: auto

    # Embeddings for RAG
    - name: embedder
      provider: universal
      model: sentence-transformers/all-MiniLM-L6-v2
      base_url: http://127.0.0.1:11540
      transformers:
        device: auto
        dtype: auto
        model_type: embedding

    # Image generation
    - name: image-gen
      provider: universal
      model: stabilityai/stable-diffusion-2-1
      base_url: http://127.0.0.1:11540
      transformers:
        device: auto
        dtype: auto
        model_type: image
      diffusion:
        default_steps: 30
        default_guidance: 7.5
        default_size: "512x512"
        scheduler: euler
        enable_optimizations: true
```

**Example 2: RAG with Universal Embeddings**

```yaml
runtime:
  models:
    - name: chat
      provider: universal
      model: microsoft/phi-2
      base_url: http://127.0.0.1:11540

    - name: embedder
      provider: universal
      model: nomic-ai/nomic-embed-text-v1.5
      base_url: http://127.0.0.1:11540
      transformers:
        model_type: embedding

rag:
  databases:
    - name: main_database
      type: ChromaStore
      embedding_strategies:
        - name: default_embeddings
          type: UniversalEmbedder
          config:
            model: nomic-ai/nomic-embed-text-v1.5
            dimension: 768
            batch_size: 16
```

### Hardware Acceleration

Universal Runtime automatically detects and optimizes for your hardware:

**Detection Priority:**
1. **NVIDIA CUDA** â€“ Best performance on NVIDIA GPUs
2. **Apple Metal (MPS)** â€“ Optimized for Apple Silicon (M1/M2/M3)
3. **CPU** â€“ Fallback for all platforms

**Device Configuration:**
```yaml
transformers:
  device: auto    # Recommended: auto-detect best device
  device: cuda    # Force CUDA (NVIDIA)
  device: mps     # Force Metal (Apple Silicon)
  device: cpu     # Force CPU (compatible everywhere)
```

**Data Type Configuration:**
```yaml
transformers:
  dtype: auto     # auto (fp16 on GPU, fp32 on CPU)
  dtype: fp16     # Half precision (faster, less memory)
  dtype: fp32     # Full precision (highest quality)
  dtype: bf16     # BFloat16 (NVIDIA Ampere+)
```

### Environment Variables

```bash
# Server configuration
UNIVERSAL_RUNTIME_HOST=127.0.0.1
UNIVERSAL_RUNTIME_PORT=11540

# Model caching
TRANSFORMERS_CACHE=~/.cache/huggingface
HF_TOKEN=hf_xxxxx  # For gated models (Llama, etc.)

# Device control
TRANSFORMERS_FORCE_CPU=1     # Force CPU mode
TRANSFORMERS_SKIP_MPS=1      # Skip MPS, use CPU instead
```

### API Usage

Universal Runtime provides an OpenAI-compatible API:

**Chat Completions:**
```bash
curl -X POST http://localhost:11540/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/phi-2",
    "messages": [{"role": "user", "content": "Hello!"}],
    "max_tokens": 50
  }'
```

**Embeddings:**
```bash
curl -X POST http://localhost:11540/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "sentence-transformers/all-MiniLM-L6-v2",
    "input": "Hello world"
  }'
```

**Image Generation:**
```bash
curl -X POST http://localhost:11540/v1/images/generations \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "a serene mountain lake at sunset",
    "model": "stabilityai/stable-diffusion-2-1",
    "size": "512x512"
  }'
```

### Supported Model Categories

1. **Text Generation (CausalLM)** â€“ GPT-2, Llama, Mistral, Qwen, Phi
2. **Embeddings (Encoder)** â€“ BERT, sentence-transformers, BGE, nomic-embed
3. **Image Generation (Diffusion)** â€“ Stable Diffusion, SDXL, FLUX
4. **Vision Classification** â€“ ViT, CLIP, DINOv2
5. **Audio Processing** â€“ Whisper, Wav2Vec2
6. **Multimodal (Vision-Language)** â€“ BLIP, LLaVA, Florence

### Key Differences: Universal vs Other Runtimes

| Feature | Universal Runtime | Ollama | Lemonade |
|---------|------------------|--------|----------|
| **Model Format** | PyTorch (Transformers) | GGUF (llama.cpp) | GGUF (llama.cpp) |
| **Model Source** | HuggingFace Hub | Ollama library | HuggingFace GGUF |
| **Model Types** | 6 types (text, image, audio, etc.) | Text only | Text only |
| **Optimization** | PyTorch native | Quantized CPU/GPU | NPU/GPU accelerated |
| **Memory Usage** | Higher (full precision) | Lower (quantized) | Lower (quantized) |
| **Setup** | Auto-download from HF | `ollama pull` | `lemonade-server-dev pull` |
| **Default Port** | 11540 | 11434 | 11534 |
| **Best For** | Flexibility, multimodal | CPU speed, ease of use | NPU/GPU acceleration |

### When to Use Universal Runtime

**Choose Universal Runtime when you need:**
- Access to any HuggingFace model (latest research models, domain-specific fine-tunes)
- Multimodal capabilities (images, audio, vision)
- Embedding generation for RAG
- Custom model configurations (LoRA, adapters)

**Choose Ollama when you need:**
- Fast CPU inference with quantized models
- Simplest setup experience
- Established model library

**Choose Lemonade when you need:**
- Maximum performance on Apple Silicon
- NPU/GPU acceleration
- GGUF model optimization

### Troubleshooting

**Model not found:**
```bash
# Models auto-download from HuggingFace on first use
# Check internet connection and HF_TOKEN for gated models
export HF_TOKEN=hf_your_token_here
```

**Out of memory:**
```bash
# Use smaller models or force CPU mode
TRANSFORMERS_FORCE_CPU=1 nx start universal-runtime

# Or reduce batch size in config
transformers:
  batch_size: 1
```

**Slow inference:**
```bash
# Ensure GPU is detected
# Check device selection in logs
# Consider using GGUF models with Ollama/Lemonade instead
```

## Extending Provider Support

To add a new provider enum:

1. Update `config/schema.yaml` (`runtime.provider` enum).
2. Regenerate datamodels via `config/generate_types.py`.
3. Map the provider to an execution path in the server runtime service.
4. Update CLI defaults or additional flags if needed.
5. Document usage in this guide.

## Upcoming Roadmap

- **Advanced agent handler configuration** â€“ choose handlers per command and dataset.
- **Fine-tuning pipeline integration** â€“ track status in the roadmap.

## Specialized ML Models

Beyond text generation, the Universal Runtime provides specialized ML capabilities:

| Capability | Endpoint | Use Case |
|-----------|----------|----------|
| **OCR** | `POST /v1/ocr` | Extract text from images/PDFs |
| **Document Extraction** | `POST /v1/documents/extract` | Extract structured data from forms |
| **Text Classification** | `POST /v1/classify` | Sentiment analysis, routing |
| **Named Entity Recognition** | `POST /v1/ner` | Extract people, places, organizations |
| **Reranking** | `POST /v1/rerank` | Improve RAG retrieval accuracy |
| **Anomaly Detection** | `POST /v1/anomaly/*` | Detect outliers in data |

See the detailed guides:
- [Specialized ML Models](./specialized-ml.md) - OCR, document extraction, classification, NER, reranking
- [Anomaly Detection Guide](./anomaly-detection.md) - Complete anomaly detection documentation

## Next Steps

- [Specialized ML Models](./specialized-ml.md) â€“ OCR, document extraction, and more.
- [Anomaly Detection](./anomaly-detection.md) â€“ detect outliers in your data.
- [Configuration Guide](../configuration/index.md) â€“ runtime schema details.
- [Extending runtimes](../extending/index.md#extend-runtimes) â€“ step-by-step provider integration.
- [Prompts](../prompts/index.md) â€“ control how system prompts interact with runtime capabilities.

---

# Specialized ML Models

Beyond text generation, the Universal Runtime provides a comprehensive suite of specialized ML endpoints for document processing, text analysis, and anomaly detection. These endpoints run on the Universal Runtime server (port 11540).

## Quick Reference

| Capability | Endpoint | Use Case |
|-----------|----------|----------|
| [OCR](#ocr-text-extraction) | `POST /v1/ocr` | Extract text from images/PDFs |
| [Document Extraction](#document-extraction) | `POST /v1/documents/extract` | Extract structured data from forms |
| [Text Classification](#text-classification-pre-trained) | `POST /v1/classify` | Sentiment, spam detection (pre-trained models) |
| [Custom Classification](#custom-text-classification-setfit) | `POST /v1/classifier/*` | Train your own classifier with few examples |
| [Named Entity Recognition](#named-entity-recognition-ner) | `POST /v1/ner` | Extract people, places, organizations |
| [Reranking](#reranking-cross-encoder) | `POST /v1/rerank` | Improve RAG retrieval accuracy |
| [Anomaly Detection](#anomaly-detection) | `POST /v1/anomaly/*` | Detect outliers in numeric/mixed data |

## Starting the Universal Runtime

```bash
# Start the runtime server
nx start universal-runtime

# Or with custom port
LF_RUNTIME_PORT=8080 nx start universal-runtime
```

The server runs on `http://localhost:11540` by default.

---

## OCR (Text Extraction)

Extract text from images and PDF documents using multiple OCR backends.

### Supported Backends

| Backend | Description | Best For |
|---------|-------------|----------|
| `surya` | Transformer-based, layout-aware (recommended) | Best accuracy, complex documents |
| `easyocr` | 80+ languages, widely used | Multilingual documents |
| `paddleocr` | Fast, production-optimized | Asian languages, speed |
| `tesseract` | Classic OCR, CPU-only | Simple documents, CPU-only environments |

### Using the LlamaFarm API (Recommended)

The easiest way to use OCR is through the LlamaFarm API, which handles file uploads and PDF-to-image conversion automatically:

```bash
# Upload a PDF or image directly
curl -X POST http://localhost:8000/v1/vision/ocr \
  -F "file=@document.pdf" \
  -F "model=easyocr" \
  -F "languages=en"
```

Or with base64-encoded images:

```bash
curl -X POST http://localhost:8000/v1/vision/ocr \
  -F 'images=["data:image/png;base64,iVBORw0KGgo..."]' \
  -F "model=surya" \
  -F "languages=en"
```

**Supported file types:** PDF, PNG, JPG, JPEG, GIF, WebP, BMP, TIFF

### Using the Universal Runtime Directly

For more control, you can use the Universal Runtime directly with base64 images:

```bash
# OCR with base64 image
curl -X POST http://localhost:11540/v1/ocr \
  -H "Content-Type: application/json" \
  -d '{
    "model": "surya",
    "images": ["'$(base64 -w0 document.png)'"],
    "languages": ["en"]
  }'
```

### PDF Processing Workflow (Universal Runtime)

For multi-page documents using the Universal Runtime directly:

```bash
# 1. Upload PDF (auto-converts to images)
curl -X POST http://localhost:11540/v1/files \
  -F "file=@document.pdf" \
  -F "convert_pdf=true" \
  -F "pdf_dpi=150"

# Response: {"id": "file_abc123", "page_count": 5, ...}

# 2. Run OCR on all pages
curl -X POST http://localhost:11540/v1/ocr \
  -H "Content-Type: application/json" \
  -d '{
    "model": "surya",
    "file_id": "file_abc123",
    "languages": ["en"],
    "return_boxes": true
  }'
```

### Response Format

```json
{
  "object": "list",
  "data": [
    {
      "index": 0,
      "text": "Invoice #12345\nDate: 2024-01-15\nTotal: $1,234.56",
      "confidence": 0.95,
      "boxes": [
        {"x1": 10, "y1": 20, "x2": 150, "y2": 40, "text": "Invoice #12345", "confidence": 0.98}
      ]
    }
  ],
  "model": "surya",
  "usage": {"images_processed": 1}
}
```

---

## Document Extraction

Extract structured key-value pairs from forms, invoices, and receipts using vision-language models.

### Supported Models

| Model | Description |
|-------|-------------|
| `naver-clova-ix/donut-base-finetuned-cord-v2` | Receipt/invoice extraction (no OCR needed) |
| `naver-clova-ix/donut-base-finetuned-docvqa` | Document Q&A |
| `microsoft/layoutlmv3-base-finetuned-docvqa` | Document Q&A with layout understanding |

### Using the LlamaFarm API (Recommended)

The easiest way to extract data from documents is through the LlamaFarm API:

```bash
# Extract from a receipt (file upload)
curl -X POST http://localhost:8000/v1/vision/documents/extract \
  -F "file=@receipt.pdf" \
  -F "model=naver-clova-ix/donut-base-finetuned-cord-v2" \
  -F "task=extraction"
```

**Supported file types:** PDF, PNG, JPG, JPEG, GIF, WebP, BMP, TIFF

### Extract from Receipt (Universal Runtime)

Using the Universal Runtime directly with a file ID:

```bash
curl -X POST http://localhost:11540/v1/documents/extract \
  -H "Content-Type: application/json" \
  -d '{
    "model": "naver-clova-ix/donut-base-finetuned-cord-v2",
    "file_id": "file_abc123",
    "task": "extraction"
  }'
```

### Response Format

```json
{
  "object": "list",
  "data": [
    {
      "index": 0,
      "confidence": 0.92,
      "fields": [
        {"key": "store_name", "value": "Coffee Shop", "confidence": 0.95, "bbox": [10, 20, 100, 40]},
        {"key": "total", "value": "$15.99", "confidence": 0.98, "bbox": [10, 60, 80, 80]},
        {"key": "date", "value": "2024-01-15", "confidence": 0.94, "bbox": [10, 100, 100, 120]}
      ]
    }
  ]
}
```

### Document Q&A

Ask questions about document content using the LlamaFarm API:

```bash
# Document VQA with file upload (LlamaFarm API)
curl -X POST http://localhost:8000/v1/vision/documents/extract \
  -F "file=@invoice.pdf" \
  -F "model=naver-clova-ix/donut-base-finetuned-docvqa" \
  -F "prompts=What is the total amount?,What is the invoice date?" \
  -F "task=vqa"
```

Or using the Universal Runtime directly:

```bash
curl -X POST http://localhost:11540/v1/documents/extract \
  -H "Content-Type: application/json" \
  -d '{
    "model": "naver-clova-ix/donut-base-finetuned-docvqa",
    "file_id": "file_abc123",
    "prompts": ["What is the total amount?", "What is the invoice date?"],
    "task": "vqa"
  }'
```

---

## Text Classification (Pre-trained)

Use **pre-trained HuggingFace models** for common classification tasks like sentiment analysis. No training required - just pick a model and classify.

**TIP: When to Use This vs Custom Classification**
- **Use `/v1/classify`** when a pre-trained model exists for your task (sentiment, spam, toxicity)
- **Use `/v1/classifier/*`** when you need custom categories specific to your domain (intent routing, ticket categorization)

### Popular Models

| Model | Use Case |
|-------|----------|
| `distilbert-base-uncased-finetuned-sst-2-english` | Sentiment analysis |
| `facebook/bart-large-mnli` | Zero-shot classification |
| `cardiffnlp/twitter-roberta-base-sentiment-latest` | Social media sentiment |

### Basic Classification

```bash
curl -X POST http://localhost:11540/v1/classify \
  -H "Content-Type: application/json" \
  -d '{
    "model": "distilbert-base-uncased-finetuned-sst-2-english",
    "texts": [
      "I love this product!",
      "This is terrible and broken.",
      "It works okay I guess."
    ]
  }'
```

### Response Format

```json
{
  "object": "list",
  "data": [
    {"index": 0, "label": "POSITIVE", "score": 0.9998, "all_scores": {"POSITIVE": 0.9998, "NEGATIVE": 0.0002}},
    {"index": 1, "label": "NEGATIVE", "score": 0.9995, "all_scores": {"POSITIVE": 0.0005, "NEGATIVE": 0.9995}},
    {"index": 2, "label": "POSITIVE", "score": 0.6234, "all_scores": {"POSITIVE": 0.6234, "NEGATIVE": 0.3766}}
  ],
  "model": "distilbert-base-uncased-finetuned-sst-2-english"
}
```

---

## Custom Text Classification (SetFit)

Train **your own text classifier** with as few as 8-16 examples per class using [SetFit](https://huggingface.co/docs/setfit) (Sentence Transformer Fine-tuning). Perfect for domain-specific classification tasks.

**INFO: How SetFit Works**
SetFit uses contrastive learning to fine-tune a sentence-transformer model on your examples, then trains a small classification head. This approach achieves strong performance with minimal labeled data and no GPU required.

### When to Use Custom Classification

| Scenario | Use `/v1/classify` | Use `/v1/classifier/*` |
|----------|-------------------|----------------------|
| Sentiment analysis | âœ… Pre-trained models available | âŒ Overkill |
| Intent routing (booking, support, billing) | âŒ No pre-trained model | âœ… Train on your intents |
| Ticket categorization | âŒ Domain-specific | âœ… Train on your categories |
| Content moderation | âœ… Toxicity models exist | âœ… If you need custom rules |
| Document classification | âŒ Domain-specific | âœ… Train on your doc types |

### Workflow Overview

```
1. Fit model     â†’  2. Predict  â†’  3. Save (optional)
   /classifier/fit    /classifier/predict    /classifier/save
```

**TIP: Using the LlamaFarm API (Recommended)**
The LlamaFarm API (`/v1/ml/classifier/*`) provides the same functionality as the Universal Runtime with added features:
- **Model Versioning**: Automatic timestamped versions when `overwrite: false`
- **Latest Resolution**: Use `model-name-latest` to auto-resolve to the newest version
- **File Upload Support**: Direct file handling without base64 encoding

```bash
# Via LlamaFarm API (port 8000)
curl -X POST http://localhost:8000/v1/ml/classifier/fit ...

# Via Universal Runtime (port 11540)
curl -X POST http://localhost:11540/v1/classifier/fit ...
```

### Step 1: Train Your Classifier

Provide labeled examples (minimum 2, recommended 8-16 per class):

```bash
curl -X POST http://localhost:11540/v1/classifier/fit \
  -H "Content-Type: application/json" \
  -d '{
    "model": "intent-classifier",
    "base_model": "sentence-transformers/all-MiniLM-L6-v2",
    "training_data": [
      {"text": "I need to book a flight to NYC", "label": "booking"},
      {"text": "Reserve a hotel room for next week", "label": "booking"},
      {"text": "Can I get a table for two tonight?", "label": "booking"},
      {"text": "Cancel my reservation please", "label": "cancellation"},
      {"text": "I want to cancel my booking", "label": "cancellation"},
      {"text": "Please remove my appointment", "label": "cancellation"},
      {"text": "What is the weather like?", "label": "other"},
      {"text": "Tell me a joke", "label": "other"}
    ],
    "num_iterations": 20
  }'
```

**Response:**
```json
{
  "object": "fit_result",
  "model": "intent-classifier",
  "base_model": "sentence-transformers/all-MiniLM-L6-v2",
  "samples_fitted": 8,
  "num_classes": 3,
  "labels": ["booking", "cancellation", "other"],
  "training_time_ms": 1234.56,
  "status": "fitted"
}
```

### Step 2: Classify New Texts

```bash
curl -X POST http://localhost:11540/v1/classifier/predict \
  -H "Content-Type: application/json" \
  -d '{
    "model": "intent-classifier",
    "texts": [
      "I want to book a car for tomorrow",
      "Please cancel everything",
      "How are you doing?"
    ]
  }'
```

**Response:**
```json
{
  "object": "list",
  "data": [
    {"text": "I want to book a car for tomorrow", "label": "booking", "score": 0.94, "all_scores": {"booking": 0.94, "cancellation": 0.03, "other": 0.03}},
    {"text": "Please cancel everything", "label": "cancellation", "score": 0.91, "all_scores": {"booking": 0.04, "cancellation": 0.91, "other": 0.05}},
    {"text": "How are you doing?", "label": "other", "score": 0.87, "all_scores": {"booking": 0.06, "cancellation": 0.07, "other": 0.87}}
  ],
  "model": "intent-classifier"
}
```

### Step 3: Save for Production

Save your trained model to persist across server restarts:

```bash
curl -X POST http://localhost:11540/v1/classifier/save \
  -H "Content-Type: application/json" \
  -d '{"model": "intent-classifier"}'
```

**Response:**
```json
{
  "object": "save_result",
  "model": "intent-classifier",
  "path": "~/.llamafarm/models/classifier/intent-classifier",
  "status": "saved"
}
```

### Loading Saved Models

After a server restart, load your saved model:

```bash
curl -X POST http://localhost:11540/v1/classifier/load \
  -H "Content-Type: application/json" \
  -d '{"model": "intent-classifier"}'
```

### List & Delete Models

```bash
# List all saved classifiers
curl http://localhost:11540/v1/classifier/models

# Delete a model
curl -X DELETE http://localhost:11540/v1/classifier/models/intent-classifier
```

### API Reference

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/classifier/fit` | POST | Train a classifier on labeled examples |
| `/v1/classifier/predict` | POST | Classify texts using a trained model |
| `/v1/classifier/save` | POST | Save model to disk |
| `/v1/classifier/load` | POST | Load model from disk |
| `/v1/classifier/models` | GET | List saved models |
| `/v1/classifier/models/{name}` | DELETE | Delete a saved model |

### Training Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `model` | string | required | Unique name for your classifier |
| `base_model` | string | `all-MiniLM-L6-v2` | Sentence transformer to fine-tune |
| `training_data` | array | required | List of `{text, label}` objects |
| `num_iterations` | int | 20 | Contrastive learning iterations |
| `batch_size` | int | 16 | Training batch size |

### Recommended Base Models

| Model | Size | Speed | Quality |
|-------|------|-------|---------|
| `sentence-transformers/all-MiniLM-L6-v2` | 80MB | Fast | Good |
| `sentence-transformers/all-mpnet-base-v2` | 420MB | Medium | Better |
| `BAAI/bge-small-en-v1.5` | 130MB | Fast | Good |
| `BAAI/bge-base-en-v1.5` | 440MB | Medium | Better |

### Best Practices

1. **Provide diverse examples**: Include variations in phrasing, not just similar sentences
2. **Balance classes**: Aim for similar numbers of examples per class
3. **Start small**: 8-16 examples per class is often sufficient
4. **Test before saving**: Verify accuracy on held-out examples before saving
5. **Iterate**: Add more examples for classes with lower accuracy

---

## Named Entity Recognition (NER)

Extract named entities (people, organizations, locations) from text.

### Popular Models

| Model | Description |
|-------|-------------|
| `dslim/bert-base-NER` | English NER (PERSON/ORG/LOC/MISC) |
| `Jean-Baptiste/roberta-large-ner-english` | High-accuracy English NER |
| `xlm-roberta-large-finetuned-conll03-english` | Multilingual NER |

### Basic NER

```bash
curl -X POST http://localhost:11540/v1/ner \
  -H "Content-Type: application/json" \
  -d '{
    "model": "dslim/bert-base-NER",
    "texts": [
      "John Smith works at Google in San Francisco.",
      "Apple CEO Tim Cook announced new products."
    ]
  }'
```

### Response Format

```json
{
  "object": "list",
  "data": [
    {
      "index": 0,
      "entities": [
        {"text": "John Smith", "label": "PER", "start": 0, "end": 10, "score": 0.99},
        {"text": "Google", "label": "ORG", "start": 20, "end": 26, "score": 0.98},
        {"text": "San Francisco", "label": "LOC", "start": 30, "end": 43, "score": 0.97}
      ]
    },
    {
      "index": 1,
      "entities": [
        {"text": "Apple", "label": "ORG", "start": 0, "end": 5, "score": 0.99},
        {"text": "Tim Cook", "label": "PER", "start": 10, "end": 18, "score": 0.98}
      ]
    }
  ]
}
```

---

## Reranking (Cross-Encoder)

Improve RAG retrieval accuracy by reranking candidate documents with a cross-encoder model.

### Why Rerank?

Cross-encoders are **significantly more accurate** than bi-encoder similarity (10-20% improvement) and **10-100x faster** than LLM-based reranking.

### Popular Models

| Model | Description |
|-------|-------------|
| `cross-encoder/ms-marco-MiniLM-L-6-v2` | Fast, general purpose |
| `BAAI/bge-reranker-v2-m3` | Multilingual, high accuracy |
| `cross-encoder/ms-marco-MiniLM-L-12-v2` | Higher accuracy, slower |

### Basic Reranking

```bash
curl -X POST http://localhost:11540/v1/rerank \
  -H "Content-Type: application/json" \
  -d '{
    "model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
    "query": "What are the clinical trial requirements?",
    "documents": [
      "Clinical trials must follow FDA regulations for safety.",
      "The weather in California is sunny.",
      "Phase 3 trials require at least 300 participants.",
      "Our company was founded in 2010."
    ],
    "top_k": 2,
    "return_documents": true
  }'
```

### Response Format

```json
{
  "object": "list",
  "data": [
    {"index": 0, "relevance_score": 0.92, "document": "Clinical trials must follow FDA regulations..."},
    {"index": 2, "relevance_score": 0.87, "document": "Phase 3 trials require at least 300 participants."}
  ],
  "model": "cross-encoder/ms-marco-MiniLM-L-6-v2"
}
```

### Integration with RAG

Use reranking to improve your RAG pipeline:

```python
# 1. Get initial candidates from vector search (fast, approximate)
candidates = rag_query(query, top_k=20)

# 2. Rerank with cross-encoder (accurate, slower)
reranked = rerank(query, candidates[:20], top_k=5)

# 3. Use top results for LLM context
context = "\n".join([doc["document"] for doc in reranked])
```

---

## Anomaly Detection

Detect outliers and anomalies in numeric and mixed data using multiple algorithms.

See the dedicated [Anomaly Detection Guide](./anomaly-detection.md) for complete documentation.

### Quick Example

```bash
# 1. Train on normal data
curl -X POST http://localhost:11540/v1/anomaly/fit \
  -H "Content-Type: application/json" \
  -d '{
    "model": "api-monitor",
    "backend": "isolation_forest",
    "data": [[100, 1024], [105, 1100], [98, 980], [102, 1050]],
    "contamination": 0.1
  }'

# 2. Detect anomalies in new data
curl -X POST http://localhost:11540/v1/anomaly/detect \
  -H "Content-Type: application/json" \
  -d '{
    "model": "api-monitor",
    "data": [[100, 1024], [9999, 50000], [103, 1080]]
  }'
```

---

## File Management Endpoints

The Universal Runtime provides file storage for processing documents across multiple requests.

### Upload File

```bash
curl -X POST http://localhost:11540/v1/files \
  -F "file=@document.pdf" \
  -F "convert_pdf=true" \
  -F "pdf_dpi=150"
```

### List Files

```bash
curl http://localhost:11540/v1/files
```

### Get File Info

```bash
curl http://localhost:11540/v1/files/{file_id}
```

### Get File as Images

```bash
curl http://localhost:11540/v1/files/{file_id}/images
```

### Delete File

```bash
curl -X DELETE http://localhost:11540/v1/files/{file_id}
```

Files are stored temporarily (5-minute TTL by default).

---

## Next Steps

- [Anomaly Detection Guide](./anomaly-detection.md) - Complete anomaly detection documentation
- [Universal Runtime Overview](./index.md#universal-runtime) - General runtime configuration
- [API Reference](../api/index.md) - Full API documentation

---

# Anomaly Detection Guide

The Universal Runtime provides production-ready anomaly detection for monitoring APIs, sensors, financial transactions, and any time-series or tabular data.

## Overview

Anomaly detection learns what "normal" looks like from training data and then identifies deviations in new data. LlamaFarm supports:

- **Multiple algorithms**: Isolation Forest, One-Class SVM, Local Outlier Factor, Autoencoder
- **Three normalization methods**: Standardization (0-1), Z-Score (std devs), Raw scores
- **Mixed data types**: Numeric values, categorical features, and combinations
- **Production workflow**: Train, save, load, and score with persistence
- **Feature encoding**: Automatic encoding of categorical data (hash, label, one-hot, etc.)
- **Model versioning**: Automatic timestamped versions with `-latest` resolution

**TIP: Using the LlamaFarm API (Recommended)**
The LlamaFarm API (`/v1/ml/anomaly/*`) provides the same functionality as the Universal Runtime with added features:
- **Model Versioning**: When `overwrite: false` (default), models are saved with timestamps like `my-model_20251215_160000`
- **Latest Resolution**: Use `model-name-latest` to auto-resolve to the newest version

```bash
# Via LlamaFarm API (port 8000) - with versioning
curl -X POST http://localhost:8000/v1/ml/anomaly/fit \
  -H "Content-Type: application/json" \
  -d '{"model": "sensor-monitor", "backend": "isolation_forest", "data": [...], "overwrite": false}'

# Use -latest suffix to auto-resolve
curl -X POST http://localhost:8000/v1/ml/anomaly/detect \
  -H "Content-Type: application/json" \
  -d '{"model": "sensor-monitor-latest", "backend": "isolation_forest", "data": [...]}'
```

## Quick Start

### 1. Train on Normal Data

```bash
curl -X POST http://localhost:11540/v1/anomaly/fit \
  -H "Content-Type: application/json" \
  -d '{
    "model": "sensor-monitor",
    "backend": "isolation_forest",
    "normalization": "zscore",
    "data": [
      [22.1], [23.5], [21.8], [24.2], [22.7],
      [23.1], [21.5], [24.8], [22.3], [23.9]
    ],
    "contamination": 0.1
  }'
```

### 2. Detect Anomalies

```bash
curl -X POST http://localhost:11540/v1/anomaly/detect \
  -H "Content-Type: application/json" \
  -d '{
    "model": "sensor-monitor",
    "normalization": "zscore",
    "data": [[22.0], [5.0], [50.0], [-10.0]],
    "threshold": 2.0
  }'
```

Response:
```json
{
  "object": "list",
  "data": [
    {"index": 1, "score": 3.25, "raw_score": 0.65},
    {"index": 2, "score": 2.91, "raw_score": 0.64},
    {"index": 3, "score": 3.25, "raw_score": 0.65}
  ],
  "summary": {
    "anomalies_detected": 3,
    "threshold": 2.0
  }
}
```

With z-score normalization, scores represent standard deviations from normal. The readings at 5Â°C, 50Â°C, and -10Â°C are all flagged as anomalies (>2 std devs from the training mean of ~22Â°C).

---

## Score Normalization Methods

The `normalization` parameter controls how raw anomaly scores are transformed. This is crucial for interpreting results and setting thresholds.

### Comparison Table

| Method | Score Range | Default Threshold | Best For |
|--------|-------------|-------------------|----------|
| `standardization` | 0-1 | 0.5 | General use, bounded scores |
| `zscore` | Unbounded (std devs) | 2.0 | Statistical interpretation |
| `raw` | Backend-specific | 0.0 (set your own) | Debugging, advanced users |

### 1. Standardization (Default)

Sigmoid transformation to a 0-1 range using median and IQR from training data.

```json
{
  "normalization": "standardization",
  "threshold": 0.5
}
```

**How it works:**
- Scores near 0.5 are "normal"
- Scores approaching 1.0 are increasingly anomalous
- Uses median and interquartile range (IQR) for robustness to outliers

**When to use:**
- General-purpose anomaly detection
- When you want bounded, easy-to-interpret scores
- When comparing across different datasets

**Threshold guidance:**
- 0.5: Default, catches moderate anomalies
- 0.6-0.7: More conservative, fewer false positives
- 0.8-0.9: Very conservative, only extreme anomalies

**Example output:**
```
Normal reading (22Â°C):  score = 0.51
Cold anomaly (5Â°C):     score = 0.96
Hot anomaly (50Â°C):     score = 0.95
```

### 2. Z-Score (Standard Deviations)

Scores represent how many standard deviations a point is from the training mean.

```json
{
  "normalization": "zscore",
  "threshold": 2.0
}
```

**How it works:**
- Score of 0 = exactly at the training mean
- Score of 2.0 = 2 standard deviations above normal
- Score of 3.0 = 3 standard deviations (statistically rare)

**When to use:**
- When you want statistically meaningful scores
- When domain experts understand standard deviations
- For scientific/engineering applications
- When you need to explain "how anomalous" something is

**Threshold guidance:**
- 2.0: Unusual (~5% of normal distribution)
- 3.0: Rare (~0.3% of normal distribution)
- 4.0+: Extreme (very rare)

**Example output:**
```
Normal reading (22Â°C):    score = -0.22 std devs (normal)
Cold anomaly (5Â°C):       score = 3.25 std devs (rare)
Hot anomaly (50Â°C):       score = 2.91 std devs (unusual)
Freezing anomaly (-10Â°C): score = 3.25 std devs (rare)
```

**Key insight:** With z-score, you can immediately see that -10Â°C and 5Â°C are equally anomalous (both 3.25 std devs), while 50Â°C is slightly less extreme (2.91 std devs).

### 3. Raw (Backend Native)

No normalization - returns the backend's native anomaly scores.

```json
{
  "normalization": "raw",
  "threshold": 0.1
}
```

**How it works:**
- Scores are passed through unchanged from the underlying algorithm
- Range and meaning vary by backend (see table below)

**Raw score ranges by backend:**

| Backend | Typical Range | Normal Value | Anomaly Indicator |
|---------|--------------|--------------|-------------------|
| `isolation_forest` | -0.5 to 0.5 | ~0 or negative | Higher = more anomalous |
| `one_class_svm` | Unbounded | ~0 or negative | Higher = more anomalous |
| `local_outlier_factor` | 1 to 10+ | ~1 | Higher = more anomalous |

**When to use:**
- Debugging anomaly detection behavior
- When you understand the specific backend's scoring
- When you need maximum control over thresholding
- For research or algorithm comparison

**Example output (Isolation Forest):**
```
Normal reading (22Â°C):  raw_score = 0.49
Cold anomaly (5Â°C):     raw_score = 0.65
Hot anomaly (50Â°C):     raw_score = 0.64
```

### Choosing a Normalization Method

| Scenario | Recommended Method | Why |
|----------|-------------------|-----|
| First time using anomaly detection | `standardization` | Easy to understand 0-1 scores |
| Production monitoring dashboards | `standardization` | Consistent scale across models |
| Scientific/engineering analysis | `zscore` | Statistically meaningful |
| Explaining to domain experts | `zscore` | "3 std devs from normal" is clear |
| Comparing different backends | `zscore` | Normalizes different score scales |
| Debugging detection issues | `raw` | See actual algorithm output |
| Replicating research papers | `raw` | Match paper's methodology |

---

## Algorithms (Backends)

### Isolation Forest (Recommended)

Best general-purpose algorithm. Fast, works well out of the box.

```json
{
  "backend": "isolation_forest",
  "contamination": 0.1
}
```

**How it works:**
Isolates anomalies by randomly partitioning data using decision trees. Anomalies require fewer splits to isolate because they are "few and different."

**Strengths:**
- Fast training and inference
- Handles high-dimensional data well
- No assumptions about data distribution
- Works with small to large datasets

**Limitations:**
- May struggle with local anomalies in dense regions
- Less effective when anomalies are clustered together

**Best for:**
- General-purpose anomaly detection
- Large datasets (scales linearly)
- Unknown anomaly patterns
- First choice when unsure which algorithm to use

**Training data requirements:**
- Minimum: 50-100 samples
- Recommended: 500+ samples for robust models
- Should be mostly normal with some contamination

**Raw score range:** ~-0.5 to 0.5 (higher = more anomalous after negation)

### One-Class SVM

Support vector machine for outlier detection.

```json
{
  "backend": "one_class_svm",
  "contamination": 0.1
}
```

**How it works:**
Learns a boundary (hyperplane) around normal data in a high-dimensional feature space. Points outside the boundary are anomalies.

**Strengths:**
- Effective when normal data is well-clustered
- Works well with clear separation between normal and anomalous
- Good for small to medium datasets

**Limitations:**
- Slower training on large datasets (O(nÂ²) to O(nÂ³))
- Sensitive to kernel and hyperparameter choices
- May not handle multiple clusters of normal data well

**Best for:**
- Small to medium datasets (&lt;10,000 samples)
- When normal data forms tight clusters
- High-precision requirements (few false positives)

**Training data requirements:**
- Minimum: 50-100 samples
- Recommended: 200-1000 samples
- Works best with clean, well-defined normal data

**Raw score range:** Unbounded real numbers (higher = more anomalous after negation)

### Local Outlier Factor (LOF)

Density-based anomaly detection.

```json
{
  "backend": "local_outlier_factor",
  "contamination": 0.1
}
```

**How it works:**
Compares the local density of each point to the density of its neighbors. Points with substantially lower density than their neighbors are anomalies.

**Strengths:**
- Detects local anomalies (outliers relative to their neighborhood)
- Handles data with varying densities
- Good for clustered data with different cluster sizes

**Limitations:**
- Requires setting number of neighbors (k)
- Computationally expensive for large datasets
- May produce extreme scores for very isolated points

**Best for:**
- Data with multiple clusters of different densities
- When anomalies are "locally unusual" but not globally extreme
- Spatial data, network data, clustered distributions

**Training data requirements:**
- Minimum: 100+ samples (needs enough neighbors)
- Recommended: 500+ samples
- Benefits from representative sampling of all normal regions

**Raw score range:** ~1 to 10+ (higher = more anomalous; can be very large for extreme outliers)

### Autoencoder (Neural Network)

Deep learning approach for complex patterns.

```json
{
  "backend": "autoencoder",
  "epochs": 100,
  "batch_size": 32
}
```

**How it works:**
Neural network learns to compress (encode) and reconstruct (decode) normal data. Anomalies have high reconstruction error because the network hasn't learned to represent them.

**Strengths:**
- Captures complex, non-linear patterns
- Excellent for high-dimensional data
- Can learn hierarchical features
- Scales well with GPU acceleration

**Limitations:**
- Requires more training data
- Needs hyperparameter tuning (architecture, epochs)
- Training is slower than sklearn methods
- May overfit on small datasets

**Best for:**
- Complex patterns (images, time series, multi-modal data)
- Large datasets (10,000+ samples)
- When simpler methods underperform
- GPU-accelerated environments

**Training data requirements:**
- Minimum: 1000+ samples
- Recommended: 10,000+ samples
- More data generally improves performance

**Raw score range:** 0 to unbounded (reconstruction error; higher = more anomalous)

### Algorithm Comparison

| Feature | Isolation Forest | One-Class SVM | LOF | Autoencoder |
|---------|-----------------|---------------|-----|-------------|
| Speed (training) | Fast | Slow | Medium | Slow |
| Speed (inference) | Fast | Fast | Medium | Fast |
| Min samples | 50 | 50 | 100 | 1000 |
| High dimensions | Good | Good | Poor | Excellent |
| Local anomalies | Poor | Poor | Excellent | Good |
| Complex patterns | Medium | Medium | Medium | Excellent |
| GPU support | No | No | No | Yes |

---

## Understanding Contamination

The `contamination` parameter is one of the most important settings for anomaly detection. It tells the algorithm what percentage of your **training data** might already contain anomalies.

### What Contamination Does

- **Sets the decision boundary**: The algorithm uses contamination to determine where to draw the line between normal and anomalous
- **Affects threshold calculation**: During training, the model computes an anomaly threshold such that approximately `contamination Ã— 100%` of training samples would be flagged
- **Impacts sensitivity**: Lower contamination = stricter definition of "normal" = more sensitive to deviations

### How to Choose Contamination

| Scenario | Contamination | When to Use |
|----------|---------------|-------------|
| **Very clean data** | 0.01 - 0.05 | Curated datasets, lab conditions, known-good samples |
| **Typical production** | 0.05 - 0.15 | API logs, sensor readings, user activity |
| **Noisy data** | 0.15 - 0.30 | Raw logs with errors, unfiltered data streams |
| **Unknown** | 0.10 (default) | Start here and tune based on results |

### Impact on Detection

```
Training data: [normal, normal, normal, anomaly, normal, ...]
                                         â†‘
                           If contamination=0.1, model expects
                           ~10% of training data to be anomalies
```

**Contamination too low** (e.g., 0.01 when true rate is 0.10):
- Model assumes almost all training data is normal
- Decision boundary is too tight around training distribution
- Result: **High false negatives** (misses real anomalies that look like the "contaminated" training samples)

**Contamination too high** (e.g., 0.30 when true rate is 0.05):
- Model assumes many normal samples are actually anomalies
- Decision boundary is too loose
- Result: **High false positives** (flags normal variations as anomalies)

### Per-Algorithm Behavior

| Algorithm | How Contamination is Used |
|-----------|-----------------------------|
| **Isolation Forest** | Sets the `contamination` parameter directly, which determines the threshold on the anomaly score distribution |
| **One-Class SVM** | Maps to the `nu` parameter (upper bound on training error fraction) |
| **Local Outlier Factor** | Sets the contamination parameter for decision threshold |
| **Autoencoder** | Sets the reconstruction error threshold at the contamination percentile |

### Best Practices

1. **Start with 0.1** (10%) if you don't know the true anomaly rate
2. **Use domain knowledge**: If you know ~5% of API requests are errors, set `contamination: 0.05`
3. **Prefer clean training data**: If possible, curate a dataset of known-normal samples and use `contamination: 0.01-0.05`
4. **Tune empirically**: Run detection on labeled test data and adjust based on precision/recall
5. **Consider the cost of errors**: High-stakes (security) â†’ lower contamination; low-stakes (monitoring) â†’ higher contamination

### Example: Tuning Contamination

```bash
# Start conservative (assume clean training data)
curl -X POST http://localhost:11540/v1/anomaly/fit \
  -d '{"model": "test", "data": [...], "contamination": 0.05}'

# Test on data with known anomalies
curl -X POST http://localhost:11540/v1/anomaly/score \
  -d '{"model": "test", "data": [known_normal, known_anomaly, ...]}'

# If too many false positives â†’ increase contamination
# If missing anomalies â†’ decrease contamination (or clean training data)
```

---

## Mixed Data Types

Real-world data often includes both numeric and categorical features. Use the `schema` parameter to automatically encode mixed data.

### Schema Encoding Types

| Type | Description | Example |
|------|-------------|---------|
| `numeric` | Pass through as-is | Response time, bytes |
| `hash` | MD5 hash to integer | User agents, IPs (high cardinality) |
| `label` | Category to integer | HTTP methods, status codes |
| `onehot` | One-hot encoding | Low cardinality categoricals |
| `binary` | Boolean to 0/1 | yes/no, true/false |
| `frequency` | Encode as occurrence frequency | Rare vs common values |

### Example: API Log Monitoring

```bash
# Train with mixed data
curl -X POST http://localhost:11540/v1/anomaly/fit \
  -H "Content-Type: application/json" \
  -d '{
    "model": "api-log-detector",
    "backend": "isolation_forest",
    "normalization": "zscore",
    "data": [
      {"response_time_ms": 100, "bytes": 1024, "method": "GET", "user_agent": "Mozilla/5.0"},
      {"response_time_ms": 105, "bytes": 1100, "method": "POST", "user_agent": "Chrome/90.0"},
      {"response_time_ms": 98, "bytes": 980, "method": "GET", "user_agent": "Safari/14.0"},
      {"response_time_ms": 102, "bytes": 1050, "method": "GET", "user_agent": "Mozilla/5.0"}
    ],
    "schema": {
      "response_time_ms": "numeric",
      "bytes": "numeric",
      "method": "label",
      "user_agent": "hash"
    },
    "contamination": 0.1
  }'
```

```bash
# Detect anomalies (schema already learned)
curl -X POST http://localhost:11540/v1/anomaly/detect \
  -H "Content-Type: application/json" \
  -d '{
    "model": "api-log-detector",
    "normalization": "zscore",
    "threshold": 2.0,
    "data": [
      {"response_time_ms": 100, "bytes": 1024, "method": "GET", "user_agent": "Mozilla/5.0"},
      {"response_time_ms": 9000, "bytes": 500000, "method": "DELETE", "user_agent": "sqlmap/1.0"}
    ]
  }'
```

The encoder is automatically cached with the model - no need to pass the schema again for detection.

---

## Production Workflow

### Save Trained Model

After training, save the model for production use:

```bash
curl -X POST http://localhost:11540/v1/anomaly/save \
  -H "Content-Type: application/json" \
  -d '{
    "model": "api-log-detector",
    "backend": "isolation_forest"
  }'
```

Response:
```json
{
  "object": "save_result",
  "model": "api-log-detector",
  "backend": "isolation_forest",
  "filename": "api-log-detector_isolation_forest.joblib",
  "path": "~/.llamafarm/models/anomaly/api-log-detector_isolation_forest.joblib",
  "encoder_path": "~/.llamafarm/models/anomaly/api-log-detector_isolation_forest_encoder.json",
  "status": "saved"
}
```

Models are saved to `~/.llamafarm/models/anomaly/` with auto-generated filenames based on the model name and backend. The normalization method and statistics are persisted with the model.

### Load Saved Model

Load a pre-trained model (e.g., after server restart):

```bash
curl -X POST http://localhost:11540/v1/anomaly/load \
  -H "Content-Type: application/json" \
  -d '{
    "model": "api-log-detector",
    "backend": "isolation_forest"
  }'
```

The model is loaded from the standard location based on its name. The encoder and normalization settings are automatically restored.

### List Saved Models

```bash
curl http://localhost:11540/v1/anomaly/models
```

Response:
```json
{
  "object": "list",
  "data": [
    {"filename": "api-log-detector_isolation_forest.joblib", "size_bytes": 45678, "modified": 1705312345.0, "backend": "sklearn"},
    {"filename": "sensor-model_autoencoder.pt", "size_bytes": 123456, "modified": 1705312000.0, "backend": "autoencoder"}
  ],
  "models_dir": "~/.llamafarm/models/anomaly",
  "total": 2
}
```

### Delete Model

```bash
curl -X DELETE http://localhost:11540/v1/anomaly/models/api_detector_v1.joblib
```

---

## API Reference

### POST /v1/anomaly/fit

Train an anomaly detector on data assumed to be mostly normal.

**Request Body:**
```json
{
  "model": "string",           // Model identifier (for caching)
  "backend": "string",         // isolation_forest | one_class_svm | local_outlier_factor | autoencoder
  "data": [[...]] | [{...}],   // Training data (numeric arrays or dicts)
  "schema": {...},             // Feature encoding schema (required for dict data)
  "contamination": 0.1,        // Expected proportion of anomalies
  "normalization": "zscore",   // standardization | zscore | raw
  "epochs": 100,               // Training epochs (autoencoder only)
  "batch_size": 32             // Batch size (autoencoder only)
}
```

**Response:**
```json
{
  "object": "fit_result",
  "model": "api-detector",
  "backend": "isolation_forest",
  "samples_fitted": 1000,
  "training_time_ms": 123.45,
  "model_params": {
    "backend": "isolation_forest",
    "contamination": 0.1,
    "threshold": 2.23,
    "input_dim": 4
  },
  "encoder": {"schema": {...}, "features": [...]},
  "status": "fitted"
}
```

### POST /v1/anomaly/score

Score data points for anomalies. Returns all points with scores.

**Request Body:**
```json
{
  "model": "string",
  "backend": "string",
  "data": [[...]] | [{...}],
  "schema": {...},             // Optional (uses cached encoder if available)
  "normalization": "zscore",   // Must match training normalization
  "threshold": 2.0             // Override default threshold
}
```

**Response:**
```json
{
  "object": "list",
  "data": [
    {"index": 0, "score": -0.22, "is_anomaly": false, "raw_score": 0.49},
    {"index": 1, "score": 3.25, "is_anomaly": true, "raw_score": 0.65}
  ],
  "summary": {
    "total_points": 2,
    "anomaly_count": 1,
    "anomaly_rate": 0.5,
    "threshold": 2.0
  }
}
```

### POST /v1/anomaly/detect

Detect anomalies (returns only anomalous points).

Same request format as `/score`, but response only includes points classified as anomalies.
The response does not include an `is_anomaly` field since all returned points are anomalies.

**Response:**
```json
{
  "object": "list",
  "data": [
    {"index": 1, "score": 3.25, "raw_score": 0.65}
  ],
  "total_count": 1,
  "summary": {
    "anomalies_detected": 1,
    "threshold": 2.0
  }
}
```

### POST /v1/anomaly/save

Save a fitted model to disk. Models are saved to `~/.llamafarm/models/anomaly/` with auto-generated filenames.

**Request Body:**
```json
{
  "model": "string",           // Model identifier (must be fitted)
  "backend": "string"          // Backend type
}
```

### POST /v1/anomaly/load

Load a pre-trained model from disk. The file is automatically located based on model name and backend.

**Request Body:**
```json
{
  "model": "string",           // Model identifier to load/cache as
  "backend": "string"          // Backend type
}
```

### GET /v1/anomaly/models

List all saved models.

### DELETE /v1/anomaly/models/\{filename\}

Delete a saved model.

---

## Use Cases

### API Log Monitoring

Detect suspicious API requests (attacks, abuse, anomalies):

```json
{
  "backend": "isolation_forest",
  "normalization": "zscore",
  "data": [
    {"response_time_ms": 100, "bytes": 1024, "status": 200, "method": "GET", "endpoint": "/api/users", "user_agent": "Mozilla/5.0"},
    ...
  ],
  "schema": {
    "response_time_ms": "numeric",
    "bytes": "numeric",
    "status": "label",
    "method": "label",
    "endpoint": "label",
    "user_agent": "hash"
  }
}
```

**Detects:**
- SQL injection attempts (unusual user agents like `sqlmap`)
- Data exfiltration (high bytes transferred)
- DoS attempts (many requests, unusual patterns)
- Scanning (unusual endpoints, methods)

**Recommended settings:**
- Backend: `isolation_forest` (fast, handles mixed data well)
- Normalization: `zscore` (easy to explain: "this request is 5 std devs from normal")
- Threshold: 2.0-3.0 depending on false positive tolerance

### Sensor Monitoring (IoT)

Detect faulty sensors or unusual conditions:

```json
{
  "backend": "isolation_forest",
  "normalization": "zscore",
  "data": [[temperature, pressure, humidity, vibration], ...],
  "contamination": 0.05
}
```

**Detects:**
- Sensor failures (stuck values, spikes)
- Equipment issues (correlated anomalies)
- Environmental anomalies

**Recommended settings:**
- Backend: `isolation_forest` or `local_outlier_factor` (for local anomalies)
- Normalization: `zscore` (engineers understand std deviations)
- Threshold: 3.0 (industrial settings often use 3-sigma)

### Financial Transactions

Detect fraudulent transactions:

```json
{
  "backend": "one_class_svm",
  "normalization": "standardization",
  "data": [
    {"amount": 50.00, "merchant_category": "grocery", "hour": 14, "country": "US"},
    ...
  ],
  "schema": {
    "amount": "numeric",
    "merchant_category": "label",
    "hour": "numeric",
    "country": "label"
  }
}
```

**Detects:**
- Unusual amounts for category
- Unusual times
- Geographic anomalies

**Recommended settings:**
- Backend: `one_class_svm` (tight boundaries around normal behavior)
- Normalization: `standardization` (0-1 scores for risk scoring)
- Contamination: 0.01-0.05 (fraud is rare)

### Network Intrusion Detection

Detect malicious network activity:

```json
{
  "backend": "local_outlier_factor",
  "normalization": "zscore",
  "data": [
    {"bytes_in": 1024, "bytes_out": 512, "packets": 10, "duration_ms": 100, "protocol": "TCP", "port": 443},
    ...
  ],
  "schema": {
    "bytes_in": "numeric",
    "bytes_out": "numeric",
    "packets": "numeric",
    "duration_ms": "numeric",
    "protocol": "label",
    "port": "label"
  }
}
```

**Detects:**
- Port scanning (unusual port patterns)
- Data exfiltration (high outbound bytes)
- C2 communication (unusual timing patterns)

**Recommended settings:**
- Backend: `local_outlier_factor` (detects local anomalies in network clusters)
- Normalization: `zscore` (clear severity indication)
- Threshold: 2.0-3.0

---

## Best Practices

### Training Data

1. **Use mostly normal data**: The `contamination` parameter tells the algorithm what proportion of anomalies to expect
2. **Include variety**: Cover different normal patterns (weekday/weekend, seasonal, etc.)
3. **Sufficient samples**: At least 100-1000 samples for good results
4. **Clean data**: Remove known bad data if possible before training

### Feature Selection

1. **Include relevant features**: All features that might indicate anomalies
2. **Normalize scales**: Features are automatically scaled, but extreme ranges can affect sensitivity
3. **Choose appropriate encodings**: Use `hash` for high-cardinality, `label` for ordered categories

### Threshold Tuning

1. **Use the learned threshold**: The runtime automatically computes a threshold during training based on the `contamination` parameter (percentile of normalized scores). This learned threshold is returned in the fit response and used by default.
2. **Override when needed**: You can pass a custom `threshold` parameter to `/v1/anomaly/score` or `/v1/anomaly/detect` endpoints.
3. **Match normalization to threshold**:
   - `standardization`: threshold 0.5-0.9
   - `zscore`: threshold 2.0-4.0
   - `raw`: threshold depends on backend
4. **Test with known anomalies**: Tune based on your false positive tolerance

### Production Deployment

1. **Save models**: Don't retrain on every restart
2. **Version models**: Use descriptive filenames like `api_detector_v2_2024_01`
3. **Monitor performance**: Track false positive/negative rates
4. **Retrain periodically**: Normal patterns may drift over time
5. **Log raw scores**: Even if using normalized scores, log `raw_score` for debugging

---

## Troubleshooting

### Scores are all around 0.5 (standardization)

**Cause:** Training data has low variance, or test data is very similar to training.

**Solutions:**
- Switch to `zscore` normalization for more spread
- Check that training data covers the full range of normal behavior
- Verify test data actually contains anomalies

### Z-scores are extremely large (100+)

**Cause:** Test point is far outside the training distribution.

**Solutions:**
- This is actually correct behavior - the point is genuinely extreme
- Consider capping scores for display purposes
- Use `standardization` if you need bounded scores

### Different backends give different scores

**Cause:** Each backend has different native score ranges.

**Solutions:**
- Use `zscore` normalization to make scores comparable
- Don't compare raw scores across backends
- Choose one backend and stick with it for consistency

### Model not detecting obvious anomalies

**Causes:**
1. Contamination too high (model thinks anomalies are normal)
2. Training data already contains similar anomalies
3. Threshold too high

**Solutions:**
- Lower contamination (e.g., 0.01-0.05)
- Curate cleaner training data
- Lower threshold
- Try `local_outlier_factor` for local anomalies

---

## Environment Variables

```bash
# Base data directory (default: ~/.llamafarm)
# Anomaly models are saved to $LF_DATA_DIR/models/anomaly/
LF_DATA_DIR=/path/to/llamafarm/data
```

---

## Next Steps

- [Specialized ML Models](./specialized-ml.md) - Overview of all ML endpoints
- [Universal Runtime](./index.md#universal-runtime) - General runtime configuration
- [API Reference](../api/index.md) - Full API documentation

---

# Prompts

Prompts in LlamaFarm are simple but powerful: you define static instructions in `llamafarm.yaml`, and the runtime merges them with chat history and (optionally) RAG context. This section outlines current capabilities and roadmap plans.

## Prompt Configuration

```yaml
prompts:
  - name: default
    messages:
      - role: system
        content: >-
          You are a regulatory assistant. Provide concise answers and cite sources by title.
      - role: user
        content: "Use bullet points by default."
```

- Prompts are named sets that can be selectively applied to models.
- Messages within each prompt set are preserved in order and prepended to conversations.
- Roles should match what your provider understands (`system`, `user`, `assistant`).
- Models can specify which prompt sets to use via `prompts: [list of names]`; if omitted, all prompts stack in definition order.
- Combine with RAG by including instructions explaining how to use context snippets (the server injects them automatically).

## Best Practices

- **Explain context usage**: remind the model that context chunks contain citations or metadata.
- **Handle non-RAG scenarios**: mention what to do when no documents are retrieved (â€œanswer from general knowledgeâ€ or â€œstate that no information was foundâ€).
- **Keep prompts concise**: long system instructions can reduce available tokens on smaller models.
- **Avoid conflicting instructions**: align prompts with agent handler expectations (structured vs. simple chat).

## Roadmap & Limitations

- Prompt templates, versioning, and evaluation tooling are in development. Track progress in the roadmap.
- For now, dynamic templating (Jinja, variables) is not built-inâ€”generate prompts upstream if needed.

## Related Guides

- [Configuration Guide](../configuration/index.md)
- [RAG Guide](../rag/index.md) (for context injection tips)
- [Extending agent handlers](../extending/index.md#extend-runtimes)

---

# Tool Calling

LlamaFarm supports two methods for giving AI models access to tools:

1. **MCP (Model Context Protocol)** - Connect to external tool servers using the standardized MCP protocol
2. **Inline Tools** - Define tools directly in your `llamafarm.yaml` configuration

Both methods allow models to execute functions, query databases, access file systems, and interact with external services.

## Two Approaches

| Approach | Best For | Configuration |
|----------|----------|---------------|
| **MCP Servers** | External tools, shared services, complex integrations | `mcp.servers[]` + `mcp_servers` on model |
| **Inline Tools** | Simple functions, CLI commands, project-specific tools | `tools[]` on model |

You can use both approaches togetherâ€”they are merged at runtime.

---

## MCP (Model Context Protocol)

MCP is a standardized protocol that gives AI models access to external tools, APIs, and data sources. LlamaFarm supports MCP both as a **client** (connecting to external MCP servers) and as a **server** (exposing its own API as MCP tools).

### Why MCP?

Instead of limiting your AI to text generation, MCP lets you connect models to:

- **File systems** - Read and analyze local files
- **Databases** - Query and modify data
- **APIs** - Interact with external services (weather, CRM, calendars)
- **Custom tools** - Expose your own business logic

### Quick Start

#### 1. Add an MCP Server to Your Project

In your `llamafarm.yaml`:

```yaml
mcp:
  servers:
    - name: filesystem
      transport: stdio
      command: npx
      args:
        - '-y'
        - '@modelcontextprotocol/server-filesystem'
        - '/path/to/allowed/directory'
```

#### 2. Assign It to a Model

```yaml
runtime:
  models:
    - name: assistant
      provider: ollama
      model: llama3.1:8b
      mcp_servers:
        - filesystem
```

#### 3. Chat with Tool Access

```bash
lf chat "What files are in my documents folder?"
```

The model can now use the filesystem tools to answer your question.

---

## Transport Types

LlamaFarm supports three transport mechanisms for connecting to MCP servers:

### STDIO (Local Process)

Spawns the MCP server as a local subprocess. Best for local tools.

```yaml
mcp:
  servers:
    - name: filesystem
      transport: stdio
      command: npx
      args:
        - '-y'
        - '@modelcontextprotocol/server-filesystem'
        - '/Users/myuser/documents'
      env:
        NODE_ENV: production
```

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `command` | string | Yes | Command to launch the server |
| `args` | array | No | Arguments to pass to the command |
| `env` | object | No | Environment variables for the process |

**Common STDIO Servers:**

```yaml
# Official Anthropic filesystem server
- name: filesystem
  transport: stdio
  command: npx
  args: ['-y', '@modelcontextprotocol/server-filesystem', '/path/to/dir']

# SQLite database server
- name: sqlite
  transport: stdio
  command: npx
  args: ['-y', '@modelcontextprotocol/server-sqlite', 'path/to/database.db']

# Custom Python MCP server
- name: my-python-tool
  transport: stdio
  command: python
  args: ['-m', 'my_mcp_server']
```

### HTTP (Remote Server)

Connects to a remote MCP server via HTTP streaming. Best for cloud services and shared tools.

```yaml
mcp:
  servers:
    - name: remote-api
      transport: http
      base_url: https://api.example.com/mcp
      headers:
        Authorization: Bearer ${env:API_TOKEN}
        X-Custom-Header: my-value
```

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `base_url` | string | Yes | Base URL of the MCP server |
| `headers` | object | No | HTTP headers (supports `${env:VAR}` substitution) |

**Use Cases:**
- FastAPI-MCP servers
- Cloud-hosted tool services
- Shared team tools

### SSE (Server-Sent Events)

Connects via Server-Sent Events for real-time streaming.

```yaml
mcp:
  servers:
    - name: streaming-server
      transport: sse
      base_url: https://api.example.com/mcp/sse
```

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `base_url` | string | Yes | Base URL of the SSE endpoint |

---

## Per-Model Tool Access

Control which models can access which MCP servers. This is essential for security and capability management.

```yaml
mcp:
  servers:
    - name: filesystem
      transport: stdio
      command: npx
      args: ['-y', '@modelcontextprotocol/server-filesystem', '/data']

    - name: database
      transport: http
      base_url: http://localhost:8080/mcp

runtime:
  models:
    # Research assistant - read-only file access
    - name: research-assistant
      provider: openai
      model: gpt-4
      mcp_servers:
        - filesystem

    # Data analyst - database access only
    - name: data-analyst
      provider: openai
      model: gpt-4
      mcp_servers:
        - database

    # General chat - no tool access (safer)
    - name: general-chat
      provider: ollama
      model: llama3.1:8b
      mcp_servers: []  # Empty list = no MCP
```

### Access Control Rules

| Configuration | Behavior |
|---------------|----------|
| `mcp_servers: [server1, server2]` | Model can only use listed servers |
| `mcp_servers: []` | Model has no MCP access |
| `mcp_servers` omitted | Model can use all configured servers |

---

## LlamaFarm as an MCP Server

LlamaFarm itself exposes its API as MCP tools, allowing external clients (like Claude Desktop or Cursor) to control it.

### Connecting from Claude Desktop

Add to your Claude Desktop config (`~/Library/Application Support/Claude/claude_desktop_config.json` on macOS):

```json
{
  "mcpServers": {
    "llamafarm": {
      "transport": "http",
      "url": "http://localhost:8000/mcp"
    }
  }
}
```

Or use the HTTP transport if your MCP client supports it:

```json
{
  "mcpServers": {
    "llamafarm": {
      "transport": "http",
      "url": "http://localhost:8000/mcp"
    }
  }
}
```

### Available Tools (Exposed by LlamaFarm)

When you connect to LlamaFarm as an MCP server, you get access to:

| Tool | Description |
|------|-------------|
| List projects | Get all projects in a namespace |
| Create project | Create a new LlamaFarm project |
| Get project | Get project details and configuration |
| Update project | Modify project settings |
| Delete project | Remove a project |
| Chat completions | Send messages to AI models |
| RAG query | Query documents in vector databases |
| List models | Get available AI models |

### Self-Reference Configuration

You can configure your project to use its own LlamaFarm server as an MCP source:

```yaml
mcp:
  servers:
    - name: llamafarm-server
      transport: http
      base_url: http://localhost:8000/mcp

runtime:
  models:
    - name: default
      provider: ollama
      model: llama3.1:8b
      mcp_servers: [llamafarm-server]
```

This allows models to interact with other LlamaFarm features programmatically.

---

## Tool Call Strategies

LlamaFarm supports two strategies for tool calling:

### Native API (Recommended)

Uses the model provider's native tool calling capabilities (e.g., OpenAI's `tools` parameter).

```yaml
runtime:
  models:
    - name: assistant
      provider: openai
      model: gpt-4
      tool_call_strategy: native_api
```

**Supported providers:** OpenAI, Anthropic, Ollama (with compatible models), Universal Runtime

### Prompt-Based

Injects tool definitions into the system prompt for models that don't support native tool calling. The tools are formatted as XML and the model is instructed to output `<tool_call>` tags when it wants to invoke a tool.

```yaml
runtime:
  models:
    - name: assistant
      provider: ollama
      model: llama2
      tool_call_strategy: prompt_based
```

**Use when:** Using older models or providers without native tool support

**How it works:** When `prompt_based` is set, LlamaFarm appends the following to your system prompt:

```
You may call one or more tools to assist with the user query.
You are provided with function signatures within <tools></tools> XML tags:
<tools>
<tool>{"type": "function", "function": {...}}</tool>
</tools>
For each tool call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
{"name": <function-name>, "arguments": <args-json-object>}
</tool_call>
```

The orchestrator detects these XML tags in the model's response and executes the corresponding tools.

---

## Inline Tool Definitions

In addition to MCP servers, you can define tools directly in your model configuration using the `tools` array. This is useful for:

- Exposing CLI commands as tools
- Creating custom function interfaces
- Defining tools without running an MCP server

### Configuration

```yaml
runtime:
  models:
    - name: assistant
      provider: universal
      model: Qwen/Qwen2.5-7B-Instruct
      tool_call_strategy: native_api
      tools:
        - type: function
          name: cli.dataset_upload
          description: Upload a file to a dataset
          parameters:
            type: object
            required:
              - filepath
              - namespace
              - project
              - dataset
            properties:
              filepath:
                type: string
                description: The path to the file to upload
              namespace:
                type: string
                description: The namespace of the project
              project:
                type: string
                description: The project name
              dataset:
                type: string
                description: The dataset name
```

### Tool Schema

Each tool in the `tools` array follows this schema:

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `type` | string | Yes | Must be `"function"` |
| `name` | string | Yes | Unique identifier for the tool (e.g., `cli.dataset_upload`) |
| `description` | string | Yes | Human-readable description shown to the model |
| `parameters` | object | Yes | JSON Schema defining the tool's input parameters |

### Parameters Schema

The `parameters` field follows [JSON Schema](https://json-schema.org/) format:

```yaml
parameters:
  type: object
  required:
    - param1
    - param2
  properties:
    param1:
      type: string
      description: Description of parameter 1
    param2:
      type: integer
      description: Description of parameter 2
    param3:
      type: boolean
      description: Optional boolean parameter
      default: false
```

**Supported parameter types:**
- `string` - Text values
- `integer` - Whole numbers
- `number` - Decimal numbers
- `boolean` - True/false values
- `array` - Lists of items
- `object` - Nested objects

### How Tool Execution Works

1. **Model Response:** When the model decides to call a tool, it returns a response with `tool_calls` containing the tool name and arguments.

2. **Orchestrator Loop:** The `ChatOrchestratorAgent` receives the response and checks if the tool can be executed:
   - For MCP tools: Executes via the MCP tool factory
   - For inline tools: Currently passed back to the client for execution (the `_can_execute_tool_call` method checks if a tool is registered)

3. **Result Injection:** The tool result is added to the conversation history as a `tool` message, and the model continues generating a response.

4. **Max Iterations:** The orchestrator limits tool call loops to 10 iterations to prevent infinite loops.

### Example: Tool Call Flow

```
User: "Upload the file report.pdf to my research dataset"

Model Response:
{
  "role": "assistant",
  "tool_calls": [{
    "id": "call_abc123",
    "type": "function",
    "function": {
      "name": "cli.dataset_upload",
      "arguments": "{\"filepath\": \"report.pdf\", \"namespace\": \"default\", \"project\": \"my-project\", \"dataset\": \"research\"}"
    }
  }]
}

Tool Execution â†’ Result: "File uploaded successfully"

Model Response (after tool result):
"I've uploaded report.pdf to your research dataset. The file is now available for processing."
```

### Combining MCP and Inline Tools

You can use both MCP servers and inline tools together. They are merged at runtime:

```yaml
mcp:
  servers:
    - name: filesystem
      transport: stdio
      command: npx
      args: ['-y', '@modelcontextprotocol/server-filesystem', '/data']

runtime:
  models:
    - name: assistant
      provider: openai
      model: gpt-4
      tool_call_strategy: native_api
      mcp_servers:
        - filesystem
      tools:
        - type: function
          name: custom.send_notification
          description: Send a notification to the user
          parameters:
            type: object
            required: [message]
            properties:
              message:
                type: string
                description: The notification message
```

The model will have access to:
- All tools from the `filesystem` MCP server
- The `custom.send_notification` inline tool

---

## Complete Configuration Example

Here's a full `llamafarm.yaml` with MCP configured:

```yaml
version: v1
name: my-project
namespace: default

mcp:
  servers:
    # Local filesystem access
    - name: filesystem
      transport: stdio
      command: npx
      args:
        - '-y'
        - '@modelcontextprotocol/server-filesystem'
        - '/Users/myuser/projects'

    # SQLite database access
    - name: database
      transport: stdio
      command: npx
      args:
        - '-y'
        - '@modelcontextprotocol/server-sqlite'
        - './data/app.db'

    # Remote API with authentication
    - name: company-api
      transport: http
      base_url: https://api.mycompany.com/mcp
      headers:
        Authorization: Bearer ${env:COMPANY_API_KEY}

    # LlamaFarm's own API
    - name: llamafarm
      transport: http
      base_url: http://localhost:8000/mcp

runtime:
  default_model: assistant
  models:
    # Full-featured assistant with all tools
    - name: assistant
      provider: openai
      model: gpt-4
      tool_call_strategy: native_api
      mcp_servers:
        - filesystem
        - database
        - company-api

    # Restricted model for public use
    - name: public-chat
      provider: ollama
      model: llama3.1:8b
      mcp_servers: []  # No tool access

    # Internal tool for automation
    - name: automation
      provider: openai
      model: gpt-4
      mcp_servers:
        - llamafarm
        - database

prompts:
  - name: default
    messages:
      - role: system
        content: |
          You are a helpful assistant with access to various tools.
          Use the available tools to help answer questions and complete tasks.
```

---

## Environment Variable Substitution

Use `${env:VARIABLE_NAME}` to inject environment variables into your configuration:

```yaml
mcp:
  servers:
    - name: secure-api
      transport: http
      base_url: ${env:API_BASE_URL}
      headers:
        Authorization: Bearer ${env:API_TOKEN}
        X-API-Key: ${env:API_KEY}
```

This keeps secrets out of your configuration files.

---

## Session Management

LlamaFarm maintains persistent MCP sessions for better performance:

- **Connection pooling** - Sessions are reused across requests
- **5-minute cache** - Tool lists are cached to reduce overhead
- **Graceful shutdown** - Sessions are properly closed when the server stops
- **1-hour timeout** - Long-running sessions for persistent connections

---

## Debugging MCP

### Check Server Configuration

```bash
lf config show | grep -A 20 "mcp:"
```

### Test MCP Connection

The server logs will show MCP initialization:

```
MCPService initialized [services.mcp_service] server_count=2
Created all MCP tools [tools.mcp_tool] servers=['filesystem', 'database'] total_tools=5
MCP tools loaded [agents.chat_orchestrator] tool_count=5 tool_names=['read_file', 'write_file', ...]
```

### Common Issues

| Issue | Solution |
|-------|----------|
| "Server not found" | Check server name matches in `mcp_servers` list |
| "Connection refused" | Ensure HTTP server is running at `base_url` |
| "Command not found" | Install required packages (e.g., `npm install -g @modelcontextprotocol/server-filesystem`) |
| "Permission denied" | Check file/directory permissions for STDIO servers |

---

## Real-World Use Cases

### Code Analysis Assistant

```yaml
mcp:
  servers:
    - name: codebase
      transport: stdio
      command: npx
      args: ['-y', '@modelcontextprotocol/server-filesystem', './src']

runtime:
  models:
    - name: code-reviewer
      provider: openai
      model: gpt-4
      mcp_servers: [codebase]
      prompts: [code-review]

prompts:
  - name: code-review
    messages:
      - role: system
        content: |
          You are a senior software engineer reviewing code.
          Use the filesystem tools to read and analyze source files.
          Provide constructive feedback on code quality, patterns, and potential issues.
```

### Database Query Assistant

```yaml
mcp:
  servers:
    - name: analytics-db
      transport: stdio
      command: npx
      args: ['-y', '@modelcontextprotocol/server-sqlite', './analytics.db']

runtime:
  models:
    - name: data-analyst
      provider: openai
      model: gpt-4
      mcp_servers: [analytics-db]
```

### Multi-Tool Automation

```yaml
mcp:
  servers:
    - name: filesystem
      transport: stdio
      command: npx
      args: ['-y', '@modelcontextprotocol/server-filesystem', '/data']

    - name: github
      transport: http
      base_url: https://github-mcp.example.com/mcp
      headers:
        Authorization: Bearer ${env:GITHUB_TOKEN}

    - name: slack
      transport: http
      base_url: https://slack-mcp.example.com/mcp
      headers:
        Authorization: Bearer ${env:SLACK_TOKEN}

runtime:
  models:
    - name: devops-bot
      provider: openai
      model: gpt-4
      mcp_servers: [filesystem, github, slack]
```

---

## Building Your Own MCP Server

You can create custom MCP servers to expose your own tools. See the [MCP documentation](https://modelcontextprotocol.io/) for the full specification.

### Example: Python MCP Server

```python
from mcp.server import Server
from mcp.server.stdio import stdio_server

app = Server("my-tools")

@app.tool()
async def my_custom_tool(param1: str, param2: int) -> str:
    """Description of what this tool does."""
    # Your implementation
    return f"Result: {param1}, {param2}"

async def main():
    async with stdio_server() as (read_stream, write_stream):
        await app.run(read_stream, write_stream)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

Then configure it in LlamaFarm:

```yaml
mcp:
  servers:
    - name: my-tools
      transport: stdio
      command: python
      args: ['-m', 'my_mcp_server']
```

---

## Next Steps

- [Configuration Guide](../configuration/index.md) - Full configuration reference
- [Models & Runtime](../models/index.md) - Configure AI models
- [RAG Guide](../rag/index.md) - Set up document retrieval
- [MCP Specification](https://modelcontextprotocol.io/) - Official MCP documentation

---

# Deployment

LlamaFarm is designed for native deployment without containers. The application runs directly on your system using native binaries and the UV package manager for Python dependencies.

## Local Development

### Option A: `lf start` (Recommended)

The easiest path is the integrated dev stack:

```bash
lf start
```

This command:
- Starts the server and Universal Runtime natively
- Watches `llamafarm.yaml` for changes
- Opens the Designer web UI at `http://localhost:8000`

### Option B: Manual control

Open separate terminals for each service:

```bash
# Terminal 1 â€“ API Server
nx start server

# Terminal 2 â€“ Universal Runtime (for ML, OCR, embeddings)
nx start universal-runtime
```

Then run CLI commands against the default server at `http://localhost:8000`.

## Desktop Application

The easiest way to run LlamaFarm is with the **Desktop App**, which bundles everything you need:

- **Mac (Universal)**: [Download](https://github.com/llama-farm/llamafarm/releases/latest/download/LlamaFarm-desktop-app-mac-universal.dmg)
- **Windows**: [Download](https://github.com/llama-farm/llamafarm/releases/latest/download/LlamaFarm-desktop-app-windows.exe)
- **Linux (x86_64)**: [Download](https://github.com/llama-farm/llamafarm/releases/latest/download/LlamaFarm-desktop-app-linux-x86_64.AppImage)
- **Linux (arm64)**: [Download](https://github.com/llama-farm/llamafarm/releases/latest/download/LlamaFarm-desktop-app-linux-arm64.AppImage)

The desktop app includes the server, Universal Runtime, and Designer UI in a single package.

## Production Deployment

### Native Process Management

For production, use process supervisors to keep services running:

**systemd (Linux)**:
```ini
[Unit]
Description=LlamaFarm Server
After=network.target

[Service]
Type=simple
User=llamafarm
WorkingDirectory=/opt/llamafarm
ExecStart=/opt/llamafarm/lf start
Restart=always
Environment=LF_DATA_DIR=/var/lib/llamafarm

[Install]
WantedBy=multi-user.target
```

**PM2 (Node.js)**:
```bash
pm2 start "lf start" --name llamafarm
pm2 save
```

**launchd (macOS)**:
```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.llamafarm.server</string>
    <key>ProgramArguments</key>
    <array>
        <string>/usr/local/bin/lf</string>
        <string>start</string>
    </array>
    <key>RunAtLoad</key>
    <true/>
    <key>KeepAlive</key>
    <true/>
</dict>
</plist>
```

### Production Checklist

- **Environment variables**: Store API keys (OpenAI, Together, etc.) in `.env` files or secret managers. Update `runtime.api_key` to reference them.
- **Data directory**: Set `LF_DATA_DIR` to a persistent location with adequate storage for models and vector databases.
- **Firewall**: Restrict access to ports 8000 (API) and 11540 (Universal Runtime) as needed.
- **TLS termination**: Use a reverse proxy (nginx, Caddy) for HTTPS in production.
- **Monitoring**: Enable logging and set up health checks against `/health` endpoint.

### Reverse Proxy Example (nginx)

```nginx
upstream llamafarm {
    server 127.0.0.1:8000;
}

server {
    listen 443 ssl;
    server_name api.yourcompany.com;

    ssl_certificate /etc/ssl/certs/api.yourcompany.com.crt;
    ssl_certificate_key /etc/ssl/private/api.yourcompany.com.key;

    location / {
        proxy_pass http://llamafarm;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # For streaming responses
        proxy_buffering off;
        proxy_cache off;
    }
}
```

## Scaling Considerations

- **Multiple instances**: Run multiple LlamaFarm instances behind a load balancer for horizontal scaling.
- **External model providers**: Use vLLM, Together, or OpenAI for model inference to offload compute.
- **Managed vector stores**: Swap `ChromaStore` for Qdrant Cloud, Pinecone, or another managed backend for larger deployments.
- **Shared storage**: Use NFS or object storage for `LF_DATA_DIR` when running multiple instances.

## Health Checks

LlamaFarm provides health endpoints for monitoring:

```bash
# Full health check
curl http://localhost:8000/health

# Liveness probe (for orchestrators)
curl http://localhost:8000/health/liveness
```

## Resources

- [Quickstart](../quickstart/index.md) â€“ Local installation steps
- [Configuration Guide](../configuration/index.md) â€“ Runtime/provider settings
- [Desktop App](../desktop-app/index.md) â€“ Bundled application
- [Extending LlamaFarm](../extending/index.md) â€“ Adapt to your infrastructure

---

# Industry Use Cases

LlamaFarm is designed to solve real-world problems across various industries. This section showcases how organizations are using LlamaFarm to streamline workflows, analyze complex documents, and extract actionable insights from their data.

## Featured Use Cases

### [Pharmaceutical & Therapeutics: FDA Document Analysis](./pharmaceutical-fda.md)

Learn how pharmaceutical and therapeutics companies use LlamaFarm to analyze FDA correspondence, track answered and unanswered regulatory questions, and ensure compliance during the approval process.

**Key Benefits:**
- Automated extraction of regulatory questions from FDA documents
- Cross-reference answers against existing correspondence databases
- Confidence scoring for answer validation
- Significant time savings in regulatory compliance workflows

---

## Why LlamaFarm for Enterprise Use Cases?

LlamaFarm provides several advantages for industry-specific applications:

- **Local-first architecture**: Keep sensitive documents on-premises
- **RAG-powered analysis**: Combine LLM reasoning with your proprietary document databases
- **Customizable agents**: Build specialized workflows for your industry
- **Batch processing**: Handle large document sets efficiently
- **Multi-model support**: Switch between models based on cost/performance needs

## Contributing Your Use Case

If you're using LlamaFarm in production and would like to share your experience, please reach out through our [GitHub repository](https://github.com/llama-farm/llamafarm). We're always looking to highlight innovative applications of the platform.

---

# Pharmaceutical & Therapeutics: FDA Document Analysis

## Overview

Pharmaceutical and therapeutics companies undergoing FDA approval face a critical challenge: tracking hundreds of questions and answers across multiple rounds of FDA correspondence. LlamaFarm provides an automated solution to identify unanswered questions, validate existing answers, and maintain compliance throughout the approval process.

This guide provides complete step-by-step instructions with code examples and configuration. All steps are self-contained and can be followed using only this documentation.

**TIP: Video Walkthrough Available**
A full video demonstration of this use case is available as a supplement to this guide: [FDA Document Analysis with LlamaFarm](https://loom.com/share/19b0f86d7e074025b12ca675c2257f25)

## Business Problem

During the FDA approval process, companies must:

- Track regulatory questions across multiple document types (Complete Response Letters, Information Requests, Meeting Minutes)
- Ensure all FDA questions have been adequately answered
- Validate answers against historical correspondence (COFUS database)
- Maintain confidence levels for answer authenticity
- Avoid missing critical unanswered questions that could delay approval

Manual review is time-consuming, error-prone, and doesn't scale as document volumes increase.

## Solution Architecture

LlamaFarm's agent-based approach automates this workflow:

```
FDA Documents â†’ Vector Database â†’ Agent Analysis â†’ Question Extraction â†’
Answer Validation â†’ Confidence Scoring â†’ Summary Report
```

### Key Components

1. **RAG-Enabled Document Store**: All FDA correspondence ingested into vector database
2. **Document Analysis Agent**: Recursively processes chunks to extract questions
3. **Answer Validation Agent**: Cross-references extracted questions against COFUS database
4. **Batch Orchestrator**: Manages processing of large document sets
5. **Confidence Scoring**: Provides reliability metrics for each answer

## Standard Operating Procedure (SOP)

### Prerequisites

- LlamaFarm installed and configured
- FDA documents in supported formats (PDF, Word, etc.)
- Access to COFUS or equivalent answer database

### Step 1: Ingest FDA Documents into Vector Database

Create a dataset and ingest all FDA correspondence:

```bash
# Create dataset for FDA documents
lf datasets create fda_correspondence -s universal_processor -b fda_db

# Upload documents (supports glob patterns)
lf datasets upload fda_correspondence ./fda_documents/*.pdf

# Process into vector database
lf datasets process fda_correspondence
```

**Best Practice**: Organize documents by submission cycle (e.g., `cycle_1/`, `cycle_2/`) for easier tracking.

### Step 2: Start Recursive Script for Document Analysis

Configure and run the FDA document analyzer agent:

```bash
# Run the FDA document analyzer
lf agents run fda_document_analyzer --input-file ./config/fda_input.json
```

The agent will:
- Break documents into manageable chunks
- Process each chunk independently
- Track progress across the entire corpus

**Configuration Example** (`fda_input.json`):

```json
{
  "database": "fda_db",
  "document_types": ["complete_response", "information_request", "meeting_minutes"],
  "chunk_size": 4000,
  "chunk_overlap": 200
}
```

### Step 3: Extract Questions from Documents

The agent sends document chunks to the LLM with specialized prompts to identify regulatory questions:

**System Prompt Example**:
```
You are analyzing FDA correspondence. Extract all regulatory questions
from the provided text. Focus on substantive questions about:
- Clinical data requirements
- Safety/efficacy concerns
- Manufacturing/quality controls
- Labeling requirements

Exclude administrative questions (meeting scheduling, contact info, etc.)

Return questions in this format:
{
  "question": "...",
  "category": "clinical|safety|manufacturing|labeling",
  "document_section": "..."
}
```

### Step 4: Validate Answers Against COFUS Database

For each extracted question, the agent:

1. Queries the COFUS database using RAG
2. Retrieves relevant passages
3. Validates if the question has been adequately answered
4. Assesses answer authenticity

```bash
# Query specific question against COFUS
lf rag query --database cofus_db \
  --score-threshold 0.7 \
  "Has clinical endpoint XYZ been addressed?"
```

### Step 5: Save Results and Confidence Scores

The agent generates structured output with confidence metrics:

```json
{
  "question_id": "Q_001",
  "question": "What additional clinical data is required for endpoint validation?",
  "status": "answered",
  "confidence_score": 0.95,
  "answer_source": "COFUS Letter 2024-03-15",
  "answer_summary": "Two additional Phase 3 studies required...",
  "validation_method": "semantic_match"
}
```

**Confidence Threshold**: Focus on scores â‰¥ 90% for reliable answers. Questions with lower scores may require manual review.

### Step 6: Review Summary of Findings

Generate an executive summary report:

```bash
lf agents run fda_summary_generator --input-file ./results/analysis_results.json
```

**Sample Summary Output**:

```
FDA Document Analysis Summary
=============================
Total Questions Identified: 47
Answered Questions: 42 (89%)
Unanswered Questions: 5 (11%)
Average Confidence Score: 0.93

High Priority Unanswered Questions:
1. [Clinical] What is the required duration for long-term safety follow-up?
2. [Manufacturing] Has the API impurity profile been fully characterized?
3. [Labeling] Are pediatric use restrictions required in Section 8?

Next Actions:
- Review 3 low-confidence answers (0.70-0.85 range)
- Prepare responses for 5 unanswered questions
- Submit supplemental information package
```

### Step 7: Utilize Batch Orchestrator for Processing

For large document sets, use the batch orchestrator:

```bash
# Process multiple documents in parallel
lf agents run batch_orchestrator \
  --agent fda_document_analyzer \
  --input-dir ./fda_documents/ \
  --concurrency 5 \
  --output-dir ./results/
```

**Monitoring**:
```bash
# Check processing status
lf agents status batch_orchestrator

# View logs
lf agents logs batch_orchestrator --tail 100
```

### Step 8: Adjust Agents and System Prompts as Needed

Customize agents based on your specific regulatory focus:

**Example: Prioritize Safety Questions**

Edit `llamafarm.yaml`:

```yaml
agents:
  fda_document_analyzer:
    system_prompt: |
      You are analyzing FDA correspondence with EXTRA FOCUS on safety concerns.
      Prioritize questions related to:
      - Adverse events
      - Safety signal monitoring
      - Risk mitigation strategies

      Mark safety questions with HIGH priority.

    parameters:
      temperature: 0.2  # Lower for consistency
      top_k: 5
      score_threshold: 0.85
```

Test configurations on a subset before scaling:

```bash
# Test on 10 documents first
lf agents run fda_document_analyzer \
  --input-file ./test_config.json \
  --limit 10 \
  --dry-run
```

## Cautionary Notes

âš ï¸ **Document Formatting**: Ensure documents are properly formatted before ingestion. Scanned PDFs may require OCR preprocessing.

âš ï¸ **Process Interruption**: The recursive process saves checkpoints. If stopped, it can resume from the last checkpoint, but plan for uninterrupted runs of 2-4 hours for large document sets.

âš ï¸ **Confidence Thresholds**: Don't rely solely on high-confidence scores. Critical questions should undergo manual review regardless of score.

âš ï¸ **Model Selection**: Use more powerful models (e.g., GPT-4, Claude) for regulatory analysis. Smaller models may miss nuanced questions.

## Tips for Efficiency

ğŸ’¡ **Overnight Processing**: Start the analysis at the end of the workday and let agents run independently for several hours. Review results the next morning.

ğŸ’¡ **Staged Rollout**: Test on a subset (10-20 documents) first to validate prompts and configuration before processing the full corpus.

ğŸ’¡ **Model Switching**: Use fast models for initial question extraction, then switch to powerful models for answer validation:

```bash
# Fast model for extraction
lf chat --model fast --agent question_extractor

# Powerful model for validation
lf chat --model powerful --agent answer_validator
```

ğŸ’¡ **Incremental Processing**: Process documents as they arrive rather than batch-processing at the end:

```bash
# Add to existing dataset
lf datasets upload fda_correspondence ./new_documents/*.pdf
lf datasets process fda_correspondence
```

## Results & ROI

Organizations using this workflow report:

- **Time Savings**: 80-90% reduction in manual document review time
- **Accuracy**: 95%+ question identification rate (when using appropriate models)
- **Risk Mitigation**: Earlier identification of unanswered questions
- **Audit Trail**: Complete tracking of all questions and answers for regulatory inspections

## Getting Started

1. **Review the example**: Check out `examples/fda_rag/` in the LlamaFarm repository
2. **Start small**: Begin with one submission cycle (5-10 documents)
3. **Iterate**: Refine prompts and configuration based on results
4. **Scale**: Expand to full document corpus once validated

**Optional**: [Watch the full video walkthrough](https://loom.com/share/19b0f86d7e074025b12ca675c2257f25) for a visual demonstration of these steps.

## Additional Resources

- [RAG Configuration Guide](../rag/index.md)
- [Agent Development Guide](../extending/index.md)
- [CLI Reference](../cli/index.md)

## Questions?

If you're implementing this workflow and need assistance, please reach out through our [GitHub Discussions](https://github.com/llama-farm/llamafarm/discussions).

---

# Examples

## ğŸŒŸ Featured: Personal Medical Assistant

**A privacy-first, 100% local medical records helper** that lets you understand your health data using AI and evidence-based medical knowledge.

### âœ¨ Highlights

- ğŸ”’ **Complete Privacy** â€“ All PDF processing in browser, no uploads, PHI stays on device
- ğŸ¤– **Multi-Hop Agentic RAG** â€“ AI orchestrates query generation and knowledge synthesis
- ğŸ“š **125,830 Medical Knowledge Chunks** â€“ From 18 authoritative textbooks (MedRAG dataset)
- âš¡ **Two-Tier AI** â€“ Fast model for queries, capable model for responses
- ğŸ’¬ **Streaming Chat** â€“ Real-time responses with transparent reasoning

Built with Next.js and LlamaFarm, this example demonstrates how to build privacy-first healthcare applications that keep sensitive data completely local while delivering intelligent, evidence-based insights.

**[ğŸ“– Read the Full Guide â†’](./medical-records-helper.md)**

---

## CLI-Based RAG Examples

The repository ships with interactive demos that highlight different retrieval scenarios. Each example lives under `examples/<folder>` and provides a configuration, sample data, and a script that uses the newest CLI commands (e.g., `lf datasets create`, `lf chat`).

| Folder | Use Case | Highlights |
|--------|----------|------------|
| `large_complex_rag/` | Multi-megabyte Raleigh UDO ordinance PDF | Long-running ingestion, zoning-focused prompts, unique DB/dataset per run. |
| `many_small_file_rag/` | FDA correspondence packet | Several shorter PDFs, quick iteration, letter-specific queries. |
| `mixed_format_rag/` | Blend of PDF/Markdown/HTML/text/code | Hybrid retrieval, multiple parsers/extractors in one pipeline. |
| `quick_rag/` | Two short engineering notes | Rapid smoke test for the environment and CLI. |

## How to Run an Example
```bash
# Build or install the CLI if needed
go build -o lf ./cli

# Run the interactive workflow (press Enter between steps).
# The script automatically scopes the CLI with `lf --cwd examples/<folder>`.
./examples/<folder>/run_example.sh

# Optional: point the script at a different directory that contains the lf binary
./examples/<folder>/run_example.sh /path/to/your/project

# Skip prompts if desired
NO_PAUSE=1 ./examples/<folder>/run_example.sh
```

Each script clones the relevant database entry, creates a unique dataset/database pair, uploads the sample documents, processes them, prints the CLI output verbatim, runs meaningful `lf rag query` and `lf chat` commands, and finishes with a baseline `--no-rag` comparison. Clean-up instructions are printed at the end of each script.

## Manual Command Reference
Use these commands if you prefer to run the workflows yourself (replace `<folder>` with the example you want to explore):
```bash
lf --cwd examples/<folder> datasets create -s <strategy> -b <database> <dataset>
lf --cwd examples/<folder> datasets upload <dataset> examples/<folder>/files/*
lf --cwd examples/<folder> datasets process <dataset>
lf --cwd examples/<folder> rag query --database <database> --top-k 3 --include-metadata --include-score "Your question"
lf --cwd examples/<folder> chat --database <database> "Prompt needing citations"
lf --cwd examples/<folder> chat --no-rag "Same prompt without RAG"
lf --cwd examples/<folder> datasets delete <dataset>
rm -rf examples/<folder>/data/<database>
```

Refer to each example folderâ€™s README for scenario-specific prompts, cleanup suggestions, and contextual background (e.g., why those documents were chosen and what use cases they simulate).

---

# Medical Records Helper

A 100% local, privacy-first medical assistant that helps you understand your medical records using AI and evidence-based medical knowledge. Built with Next.js and LlamaFarm, all PDF processing happens entirely in your browser â€“ your health data never leaves your device.

## Video Demo

Watch the full demonstration of the Medical Records Helper in action, showing how it processes medical documents locally and provides intelligent, evidence-based responses to your health questions.

## âœ¨ Key Features

### ğŸ”’ Complete Privacy
- **PDFs parsed client-side** â€“ All document processing happens in your browser
- **No server uploads** â€“ Your files never leave your device
- **PHI protection** â€“ Protected Health Information stays completely private
- **Local-first architecture** â€“ Full HIPAA-aligned approach to sensitive data

### ğŸ¤– Multi-Hop Agentic RAG
- **AI orchestration** â€“ Intelligent query generation and refinement
- **Knowledge retrieval** â€“ Semantic search across medical literature
- **Response synthesis** â€“ Combines multiple sources for comprehensive answers
- **Chain-of-thought reasoning** â€“ Transparent decision-making process

### ğŸ“š Medical Knowledge Base
- **125,830 knowledge chunks** from authoritative medical textbooks
- **18 medical textbooks** from the [MedRAG dataset](https://github.com/Teddy-XiongGZ/MedRAG)
- **Evidence-based information** â€“ Vetted medical knowledge sources
- **Semantic indexing** â€“ Fast, accurate retrieval of relevant information

### âš¡ Two-Tier AI Architecture
- **Fast model** for query generation and routing (e.g., Llama 3.2 3B)
- **Capable model** for comprehensive medical responses (e.g., Qwen 2.5 14B)
- **Optimized inference** â€“ Balance between speed and quality
- **Streaming responses** â€“ Real-time output for better UX

### ğŸ’¬ Streaming Chat Interface
- **Real-time streaming** â€“ See responses as they're generated
- **Collapsible agent reasoning** â€“ Inspect the AI's decision-making process
- **Conversation history** â€“ Maintain context across multiple queries
- **Citation support** â€“ Link responses to source documents

### ğŸ“„ Smart Document Analysis
- **Semantic chunking** â€“ Intelligent splitting of medical documents
- **Medical context awareness** â€“ Understands clinical terminology and structure
- **Cross-document synthesis** â€“ Correlate information across multiple records
- **Metadata extraction** â€“ Automatically identify key information

### âš™ï¸ Configurable Retrieval
- **Adjustable top-k** â€“ Control how many documents to retrieve
- **Score thresholds** â€“ Filter low-relevance results
- **Local document toggle** â€“ Choose between uploaded docs or knowledge base
- **Hybrid search** â€“ Combine keyword and semantic search

### ğŸ¨ Modern UI
- **Built with shadcn/ui** â€“ Beautiful, accessible components
- **Tailwind CSS** â€“ Responsive, mobile-friendly design
- **Dark mode support** â€“ Easy on the eyes for extended use
- **Intuitive workflow** â€“ Upload, chat, understand

## Architecture Overview

```mermaid
graph TB
    A[User's Browser] -->|Upload PDF| B[Client-Side PDF Parser]
    B -->|Extracted Text| C[LlamaFarm RAG Pipeline]
    C -->|Query| D[Fast Model - Query Generation]
    D -->|Refined Query| E[Vector Database]
    E -->|Relevant Chunks| F[Medical Knowledge Base<br/>125,830 chunks]
    E -->|User Documents| G[Uploaded Medical Records]
    F -->|Context| H[Capable Model - Response]
    G -->|Context| H
    H -->|Streaming Response| I[Chat Interface]
    I -->|User Question| D

    style A fill:#e1f5ff
    style C fill:#fff3cd
    style F fill:#d4edda
    style G fill:#f8d7da
    style H fill:#d1ecf1
```

### How It Works

1. **Document Upload** â€“ Drop your medical PDF into the browser interface
2. **Client-Side Parsing** â€“ PDF is parsed using PDF.js entirely in JavaScript
3. **Semantic Chunking** â€“ Text is split into meaningful chunks with medical context
4. **Query Processing** â€“ Your question is analyzed by a fast model to generate optimal search queries
5. **RAG Retrieval** â€“ Relevant information is retrieved from both your documents and the medical knowledge base
6. **Response Generation** â€“ A more capable model synthesizes the information into a comprehensive answer
7. **Stream & Display** â€“ Response is streamed in real-time with citations and reasoning

## Use Cases

### ğŸ©º Understanding Test Results
Ask questions like:
- "What does my hemoglobin A1c level of 6.8% mean?"
- "Should I be concerned about elevated liver enzymes?"
- "Explain my cholesterol panel results"

### ğŸ’Š Medication Information
Get context about prescriptions:
- "What are the side effects of metformin?"
- "Why was I prescribed this medication?"
- "Are there any drug interactions I should know about?"

### ğŸ¥ Procedure Preparation
Prepare for medical procedures:
- "What should I expect before my colonoscopy?"
- "How do I prepare for an MRI with contrast?"
- "What are the risks of this surgery?"

### ğŸ“‹ Medical History Synthesis
Consolidate your records:
- "Summarize my visits from the last 6 months"
- "What were my blood pressure trends over time?"
- "List all medications I've been prescribed"

### ğŸ” Second Opinion Research
Research conditions and treatments:
- "What are alternative treatments for hypertension?"
- "What does current research say about this diagnosis?"
- "Are there any clinical trials for my condition?"

## Getting Started

### Prerequisites

1. **LlamaFarm installed and running** â€“ Follow the [Quickstart Guide](../quickstart/index.md)
2. **Two models configured**:
   - Fast model (e.g., `llama3.2:3b`) for query generation
   - Capable model (e.g., `qwen2.5:14b`) for responses
3. **Medical knowledge base** â€“ Download and process the MedRAG dataset

### Installation

```bash
# Clone the local-ai-apps repository
git clone https://github.com/llama-farm/local-ai-apps.git
cd local-ai-apps/Medical-Records-Helper

# Install dependencies
npm install

# Configure environment variables
cp .env.example .env.local
# Edit .env.local with your LlamaFarm API endpoint

# Start the development server
npm run dev
```

### Configuration

Edit your `llamafarm.yaml` to configure the two-tier model setup:

```yaml
version: v1
name: medical-assistant
namespace: personal

runtime:
  models:
    fast-model:
      description: "Fast model for query generation"
      provider: ollama
      model: llama3.2:3b
      base_url: http://localhost:11434/v1

    capable-model:
      description: "Capable model for medical responses"
      provider: ollama
      model: qwen2.5:14b
      base_url: http://localhost:11434/v1

  default_model: capable-model

rag:
  databases:
    - name: medical_knowledge
      type: chroma
      path: ./data/medical_knowledge

  data_processing_strategies:
    - name: medical_processor
      parser: pdf
      extractors:
        - type: text
      chunker:
        type: semantic
        chunk_size: 1000
        chunk_overlap: 200
      embedder:
        model: all-MiniLM-L6-v2
```

### Setting Up the Medical Knowledge Base

```bash
# Download the MedRAG dataset
wget https://github.com/Teddy-XiongGZ/MedRAG/releases/download/v1.0/textbooks.zip
unzip textbooks.zip

# Create dataset
lf datasets create medical_texts -s medical_processor -b medical_knowledge

# Upload the textbooks
lf datasets upload medical_texts textbooks/*.pdf

# Process into vector database (this may take a while)
lf datasets process medical_texts
```

## Privacy & Security

### HIPAA Compliance Considerations

While this application is designed with privacy in mind, consider these factors:

âœ… **What's Private:**
- All PDF parsing happens client-side
- Documents are not stored on any server
- Queries are processed locally via LlamaFarm
- No data is sent to external APIs

âš ï¸ **Important Notes:**
- LlamaFarm must be running locally (not exposed to internet)
- Ensure your machine is secured (encrypted disk, screensaver lock)
- Consider using this on a dedicated, air-gapped machine for maximum security
- Review your local network security if accessing from multiple devices

### Data Flow

```
Your Device Only:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Browser (PDF Parsing) â”€â”€> LlamaFarm (Local) â”€â”€> Models â”‚
â”‚  No Internet Required     No Server Upload    Local RAM â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Performance Tips

### Hardware Recommendations

**Minimum Configuration:**
- **CPU**: 4 cores (8 recommended)
- **RAM**: 16GB (32GB recommended)
- **Storage**: 50GB free space for models and knowledge base
- **GPU**: Optional, but significantly speeds up inference

**Optimal Configuration:**
- **CPU**: 8+ cores
- **RAM**: 32GB+
- **GPU**: NVIDIA GPU with 8GB+ VRAM (for faster inference)
- **Storage**: SSD recommended

### Model Selection

Choose models based on your hardware:

| Hardware | Fast Model | Capable Model |
|----------|-----------|---------------|
| CPU Only | llama3.2:1b | llama3.2:3b |
| 8GB GPU | llama3.2:3b | qwen2.5:7b |
| 16GB GPU | llama3.2:3b | qwen2.5:14b |
| 24GB+ GPU | llama3.2:3b | qwen2.5:32b |

## Technical Details

### PDF Processing Pipeline

The client-side PDF processing uses PDF.js to:
1. Extract text content page by page
2. Preserve formatting and structure
3. Identify tables and lists
4. Extract metadata (dates, patient info)

### RAG Strategy

The multi-hop agentic RAG approach:
1. **Query Analysis** â€“ Fast model analyzes the user's question
2. **Query Generation** â€“ Generate multiple search queries to capture different aspects
3. **Retrieval** â€“ Semantic search across knowledge base + user documents
4. **Re-ranking** â€“ Score and filter results by relevance
5. **Synthesis** â€“ Capable model generates comprehensive response with citations

### Streaming Implementation

Responses stream using Server-Sent Events (SSE):
```javascript
// Client-side streaming handler
const eventSource = new EventSource('/api/chat');
eventSource.onmessage = (event) => {
  const chunk = JSON.parse(event.data);
  appendToChat(chunk.content);
};
```

## Limitations

### What This Tool Is NOT

âŒ **Not a replacement for medical professionals** â€“ Always consult qualified healthcare providers
âŒ **Not for emergencies** â€“ Call 911 or go to the ER for urgent medical issues
âŒ **Not diagnostic** â€“ Cannot diagnose conditions or prescribe treatments
âŒ **Not medical advice** â€“ For informational and educational purposes only

### Known Limitations

- **Language support** â€“ Currently optimized for English medical documents
- **Handwritten notes** â€“ Cannot process handwritten records (OCR not included)
- **Image analysis** â€“ Cannot interpret medical images (X-rays, CT scans, etc.)
- **Complex tables** â€“ May have difficulty with intricate tabular data
- **Real-time data** â€“ Cannot access current labs or vitals from healthcare systems

## Troubleshooting

### PDFs Not Parsing

**Problem:** Uploaded PDF shows no content
**Solutions:**
- Check if PDF is encrypted (password-protected)
- Ensure PDF contains text (not scanned images without OCR)
- Try a different PDF viewer to verify file integrity
- Check browser console for JavaScript errors

### Slow Response Times

**Problem:** AI responses take too long
**Solutions:**
- Switch to smaller/faster models
- Reduce RAG top-k setting (retrieve fewer chunks)
- Increase score threshold (more selective retrieval)
- Check system resources (CPU/RAM usage)
- Consider GPU acceleration

### Poor Answer Quality

**Problem:** Responses are vague or incorrect
**Solutions:**
- Use a more capable model for response generation
- Increase top-k to retrieve more context
- Ensure medical knowledge base is properly processed
- Refine your questions to be more specific
- Check that relevant documents are uploaded

### LlamaFarm Connection Issues

**Problem:** Cannot connect to LlamaFarm API
**Solutions:**
- Verify LlamaFarm is running: `lf start`
- Check API endpoint in `.env.local`
- Ensure correct port (default: 8000)
- Check firewall settings
- Review LlamaFarm logs for errors

## Contributing

This example is part of the [local-ai-apps](https://github.com/llama-farm/local-ai-apps) repository. Contributions welcome!

### Ideas for Enhancement

- ğŸ“Š **Visualization** â€“ Chart trends from lab results over time
- ğŸŒ **Multi-language** â€“ Support for non-English medical documents
- ğŸ”Š **Voice interface** â€“ Ask questions using speech-to-text
- ğŸ“± **Mobile app** â€“ Native iOS/Android versions
- ğŸ”— **EHR integration** â€“ Connect to healthcare system APIs (with proper authorization)
- ğŸ§ª **Lab result interpretation** â€“ Automated flagging of abnormal values
- ğŸ“… **Appointment preparation** â€“ Generate questions for your next doctor visit

## Resources

### Medical Knowledge Sources

- [MedRAG Dataset](https://github.com/Teddy-XiongGZ/MedRAG) â€“ 18 authoritative medical textbooks
- [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/) â€“ Free full-text archive of biomedical literature
- [UpToDate](https://www.uptodate.com/) â€“ Evidence-based clinical decision support (subscription)

### Related Projects

- [LlamaFarm Documentation](https://docs.llamafarm.ai) â€“ Full platform documentation
- [LlamaFarm GitHub](https://github.com/llama-farm/llamafarm) â€“ Main repository
- [Local AI Apps](https://github.com/llama-farm/local-ai-apps) â€“ Collection of privacy-first applications

### Further Reading

- [HIPAA Privacy Rule](https://www.hhs.gov/hipaa/for-professionals/privacy/index.html)
- [Patient Rights & Medical Records](https://www.hhs.gov/hipaa/for-individuals/medical-records/index.html)
- [Understanding Your Lab Results](https://medlineplus.gov/lab-tests/)

---

## Legal Disclaimer

**IMPORTANT:** This application is for educational and informational purposes only. It is not intended to be a substitute for professional medical advice, diagnosis, or treatment. Always seek the advice of your physician or other qualified health provider with any questions you may have regarding a medical condition. Never disregard professional medical advice or delay in seeking it because of something you have read through this application.

The medical knowledge base includes information from publicly available medical textbooks and is provided "as is" without warranty of any kind. The developers of this application are not liable for any damages or health issues arising from the use of this tool.

If you think you may have a medical emergency, call your doctor or 911 immediately.

---

## Next Steps

- Learn about [RAG configuration](../rag/index.md)
- Explore [multi-model setups](../models/index.md)
- Review [privacy best practices](../deployment/index.md)
- Check out [other examples](./index.md)

---

# Extending LlamaFarm

LlamaFarm is designed to be extended. This guide shows how to add new runtime providers, vector stores, parsers, extractors, and CLI commands while keeping schema validation intact.

## Extend Runtimes

1. **Update the schema**: add your provider to `runtime.provider` in `config/schema.yaml`.
2. **Regenerate types**:
   ```bash
   cd config
   uv run python generate_types.py
   cd ..
   ```
3. **Implement routing**:
   - Server: update runtime selection (e.g., `server/services/runtime_service.py`) to handle the new provider and base URL.
   - CLI: ensure `config/datamodel.py` exposes your provider; if special flags are needed, add them under `cli/cmd`.
4. **Document usage**: add a section to this guide and the Configuration doc showing sample `llamafarm.yaml`.

Example (vLLM running with OpenAI-compatible API):

```yaml
runtime:
  provider: openai
  model: mistral-small
  base_url: http://localhost:8000/v1
  api_key: sk-test
```

No code changes requiredâ€”just point `base_url` to your gateway.

## Extend RAG Components

### Add a Vector Store

1. Implement a store class (Python) that matches the existing store interface.
2. Register it with the RAG service so the server can instantiate it.
3. Add the store name to `rag/schema.yaml` under `databaseDefinition.type`.
4. Regenerate types and document configuration fields.

### Add a Parser or Extractor

1. Implement the parser/extractor (Python) with the required `process` signature.
2. Register it in the ingestion pipeline.
3. Append the new enum to `rag/schema.yaml` (`parsers` or `extractors` definitions) and define its config schema.
4. Regenerate types and update docs with usage examples.

## Extend the CLI

`lf` is built with Cobra.

1. Create a new file under `cli/cmd` (e.g., `backup.go`).
2. Define a `var myCmd = &cobra.Command{...}` and add it in `init()` with `rootCmd.AddCommand(myCmd)` or attach it to a namespace (datasets/rag).
3. Follow existing patterns for config resolution (`config.GetServerConfig`), auto-starting the server (`ensureServerAvailable`), and output formatting.
4. Write tests in `cli/cmd/..._test.go` if behaviour is complex.
5. Document the command in the CLI reference.

## Testing Extensions

- **Schema validation**: run `uv run --group test python -m pytest config/tests`.
- **CLI**: `cd cli && go test ./...`.
- **Server**: execute relevant pytest suites (e.g., `tests/test_project_chat_orchestrator.py`).
- **Docs**: update this page and run `nx build docs` to ensure navigation stays intact.

## Contribution Checklist

- [ ] Update schemas and regenerate types.
- [ ] Add or update server/CLI logic.
- [ ] Write tests covering new behaviour.
- [ ] Document configuration and usage.
- [ ] Note changes in `README.md` or example configs if appropriate.

Have questions? Open a [discussion](https://github.com/llama-farm/llamafarm/discussions) or join the [Discord](https://discord.gg/RrAUXTCVNF). Weâ€™re excited to see what you build.

---

# Contributing

We love contributionsâ€”from bug fixes and docs edits to new providers and RAG components. This page summarizes expectations and points you to detailed guides.

## Development Basics

- **Python**: format with `uv run ruff check --fix .`; run tests via `uv run --group test python -m pytest` in relevant packages.
- **Go (CLI)**: run `go fmt ./...`, `go vet ./...`, and `go test ./...` inside `cli/`.
- **Docs**: run `nx build docs` to ensure the site compiles without broken links.

## Workflow

1. Fork or create a branch (e.g., `feat/new-provider`).
2. Make changesâ€”code, schema, docs.
3. Run tests/lint commands; capture results for the PR description.
4. Open a draft PR with a summary, rationale, and testing notes.
5. Iterate with reviewers; keep commits tidy (Conventional Commit style is preferred).

## Updating Schemas

If you modify `config/schema.yaml` or `rag/schema.yaml`:

```bash
cd config
uv run python generate_types.py
cd ..
```

Re-run relevant tests to ensure generated code compiles and integrations still work.

## Adding Documentation

- Place new pages under the most relevant section (`quickstart`, `cli`, `configuration`, `examples`, etc.).
- Use `index.md` inside directories for Docusaurus categories.
- Keep tone concise and practical; update cross-links as needed.

## Issue Labels

- `good first issue` â€“ scoped tasks ideal for newcomers.
- `help wanted` â€“ contributions welcome; comment if you intend to pick one up.
- `breaking change` â€“ coordinate with maintainers before merging.

## Community

- [Discord](https://discord.gg/RrAUXTCVNF) for real-time chat.
- [Discussions](https://github.com/llama-farm/llamafarm/discussions) for RFCs or bigger feature proposals.
- Weekly sync details are pinned in Discord.

Thanks for helping grow LlamaFarm!

---

# _accessing Designer

<!--
  Shared snippet for Designer access instructions

  This file reduces duplication across documentation pages.

  Usage: Instead of repeating setup instructions, link to the Designer docs:
  See the [Designer documentation](../designer/index.md) for access instructions.

  The canonical setup instructions are in docs/website/docs/designer/index.md
  This file serves as a reference template if needed in the future.
-->

The easiest way to start the Designer is through the `lf start` command:

```bash
lf start
```

This automatically launches:

- The FastAPI server (port 8000)
- The RAG worker
- The Designer web UI (port **8000**)

Once started, open your browser to:

```
http://localhost:8000
```

The Designer is served by the same FastAPI server, so it shares port 8000 with the API.

---

# Designer Screenshots

This directory contains screenshots for the Designer documentation.

## Required Screenshots

Please add the following screenshots to this directory:

1. **designer-home.png** (referenced in `index.md`)

   - Shows the Designer home page with project selection and creation
   - Should show the project cards and the "Create new project" button
   - Capture: http://localhost:8000/

2. **dashboard.png** (referenced in `features.md`)

   - Shows the Dashboard view with project overview and key metrics
   - Should show project configuration summary, dataset stats, and quick actions
   - Capture: http://localhost:8000/chat/dashboard

3. **data-management.png** (referenced in `features.md`)

   - Shows the Data management interface
   - Should display processing strategies and datasets
   - Include both strategy cards and dataset cards if possible
   - Capture: http://localhost:8000/chat/data

4. **rag-config.png** (referenced in `features.md`)
   - Shows the RAG configuration interface
   - Should display databases, embedding strategies, or retrieval methods
   - Capture: http://localhost:8000/chat/databases

## Screenshot Guidelines

- **Format**: PNG (preferred) or JPG
- **Resolution**: At least 1200px wide for clarity
- **Browser**: Use a clean browser window (hide bookmarks bar, etc.)
- **Theme**: Either light or dark mode is fine, but be consistent
- **Content**: Use example/demo projects to avoid exposing private information
- **Cropping**: Include enough context to understand the UI, but remove unnecessary browser chrome

## How to Add Screenshots

1. Start the Designer: `lf start`
2. Open http://localhost:8000 in your browser
3. Navigate to each section
4. Take screenshots using your OS screenshot tool
5. Name them according to the list above
6. Place them in this directory
7. The documentation will automatically reference them

## Optional: Annotating Screenshots

If you want to highlight specific features, consider:

- Adding arrows or callouts using an image editor
- Circling important UI elements
- Adding brief text labels

Keep annotations minimal and professional.

---

# Documentation MCP Server

LlamaFarm includes an MCP server that exposes the documentation to AI models, allowing them to search, read, and navigate the docs programmatically.

## Why Use the MCP Server?

- **AI-Assisted Development**: Let Claude, GPT, or other AI assistants query LlamaFarm docs directly
- **IDE Integration**: Use with Cursor, Claude Code, or other MCP-enabled tools
- **Accurate Answers**: AI gets real-time access to current documentation
- **Context Retrieval**: Models can search for relevant configuration examples

## Quick Setup

### 1. Build the Server

```bash
cd docs/website/mcp
npm install
npm run build
```

### 2. Configure Your AI Tool

**Claude Desktop** (`~/Library/Application Support/Claude/claude_desktop_config.json`):

```json
{
  "mcpServers": {
    "llamafarm-docs": {
      "command": "node",
      "args": ["/path/to/llamafarm/docs/website/mcp/dist/index.js"]
    }
  }
}
```

**Claude Code** (`.mcp.json` in project root):

```json
{
  "mcpServers": {
    "llamafarm-docs": {
      "command": "node",
      "args": ["docs/website/mcp/dist/index.js"]
    }
  }
}
```

**Cursor** (`.cursor/mcp.json`):

```json
{
  "mcpServers": {
    "llamafarm-docs": {
      "type": "stdio",
      "command": "node",
      "args": ["docs/website/mcp/dist/index.js"]
    }
  }
}
```

## Available Tools

### list_docs

List all available documentation files:

```
Tool: list_docs
Arguments: { "category": "rag" }  // Optional filter
```

Returns file paths, titles, and descriptions.

### read_doc

Read a specific documentation file:

```
Tool: read_doc
Arguments: { "path": "rag/parsers.md" }
```

Returns the full markdown content.

### search_docs

Search across all documentation:

```
Tool: search_docs
Arguments: {
  "query": "PDFParser configuration",
  "max_results": 5
}
```

Returns matching files with line numbers and context.

### get_toc

Get the documentation structure:

```
Tool: get_toc
Arguments: {}
```

Returns a hierarchical table of contents.

## Example Interactions

Once configured, you can ask your AI assistant questions like:

> "Search the LlamaFarm docs for how to configure CrossEncoderRerankedStrategy"

The AI will use the `search_docs` tool to find relevant documentation, then `read_doc` to get the full content.

> "List all the RAG documentation pages"

The AI will call `list_docs` with `category: "rag"`.

> "Show me the parsers reference documentation"

The AI will call `read_doc` with `path: "rag/parsers.md"`.

## Using with LlamaFarm

You can also configure LlamaFarm itself to use the docs MCP server:

```yaml
mcp:
  servers:
    - name: llamafarm-docs
      transport: stdio
      command: node
      args:
        - docs/website/mcp/dist/index.js

runtime:
  models:
    - name: assistant
      provider: ollama
      model: llama3.1:8b
      mcp_servers: [llamafarm-docs]
```

This allows LlamaFarm's chat agents to query the documentation when answering questions.

## Development

To modify the MCP server:

```bash
cd docs/website/mcp

# Development mode with hot reload
npm run dev

# Inspect with MCP Inspector
npm run inspect

# Build for production
npm run build
```

## How It Works

1. The server scans `docs/website/docs/` for all markdown files
2. Extracts titles and descriptions from frontmatter
3. Provides full-text search with relevance scoring
4. Exposes docs as both MCP tools and resources

## Source Code

See `docs/website/mcp/` for the full implementation.

---
