"""
Blob-based document processor for LlamaFarm integration.
Handles iterative parser selection based on file patterns.
"""

import fnmatch
import sys
from pathlib import Path
from typing import Any, TypedDict

from components.extractors.base import BaseExtractor
from components.parsers.base.base_parser import BaseParser
from core.base import Document
from core.logging import RAGStructLogger
from utils.parsing_safety import (
    ParserFailedError,
    UnsupportedFileTypeError,
    get_file_extension,
)

repo_root = Path(__file__).parent.parent.parent.parent
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

try:
    from config.datamodel import (
        DataProcessingStrategyDefinition,
        Extractor,
        Parser,
    )
except ImportError as e:
    raise ImportError(
        f"Could not import config module. Make sure you're running from the repo root. Error: {e}"
    ) from e

logger = RAGStructLogger("rag.core.blob_processor")


class ExtractorOutput(TypedDict):
    name: str
    count: int
    new_fields: list[str]


class BlobProcessor:
    """
    Central processor for handling blob data with pattern-based parser/extractor routing.
    Implements centralized pattern matching using fnmatch for glob-style patterns.
    """

    def __init__(self, strategy_config: DataProcessingStrategyDefinition):
        """
        Initialize the blob processor with a strategy configuration.

        Args:
            strategy_config: Dictionary containing parsers and extractors config
        """
        self.strategy_config = strategy_config
        self.parsers = self._initialize_parsers(strategy_config.parsers or [])
        self.extractors = self._initialize_extractors(strategy_config.extractors or [])

    def _initialize_parsers(
        self, parser_configs: list[Parser]
    ) -> list[tuple[Parser, BaseParser]]:
        """
        Initialize parsers from configuration and sort by priority.

        Args:
            parser_configs: List of parser configurations

        Returns:
            List of tuples containing (config, parser_instance) sorted by priority
        """
        parsers: list[tuple[Parser, BaseParser]] = []
        for config in parser_configs:
            if not config.type:
                continue

            parser_type = config.type
            try:
                parser_class = self._get_parser_class(parser_type)
                # Pass the parser type name and config
                parser_instance = parser_class(
                    name=parser_type, config=config.config or {}
                )
                parsers.append((config, parser_instance))
            except Exception as e:
                logger.warning(f"Failed to initialize parser {parser_type}: {e}")

        # Sort by priority (lower numbers are higher priority)
        parsers.sort(key=lambda x: x[0].priority or 0)
        return parsers

    def _initialize_extractors(
        self, extractor_configs: list[Extractor]
    ) -> list[tuple[Extractor, BaseExtractor]]:
        """
        Initialize extractors from configuration and sort by priority.

        Args:
            extractor_configs: List of extractor configurations

        Returns:
            List of tuples containing (config, extractor_instance) sorted by priority
        """
        extractors = []
        for config in extractor_configs:
            try:
                extractor_type = config.type
                extractor_config = config.config or {}

                extractor_class = self._get_extractor_class(extractor_type)
                extractor_instance = extractor_class(extractor_config)
                extractors.append((config, extractor_instance))
            except Exception as e:
                logger.warning(f"Failed to initialize extractor {config.type}: {e}")

        # Sort by priority (lower numbers are higher priority)
        extractors.sort(key=lambda x: x[0].priority or 0)
        return extractors

    def _get_parser_class(self, parser_type: str) -> type:
        """
        Get parser class using the enhanced ToolAwareParserFactory.

        Args:
            parser_type: Name of the parser type (e.g., "PDFParser_LlamaIndex")

        Returns:
            Parser class
        """
        from components.parsers.parser_factory import ToolAwareParserFactory

        # Use the enhanced factory to load the parser class
        if parser_class := ToolAwareParserFactory.load_parser_class(parser_type):
            return parser_class

        raise ValueError(
            f"Parser {parser_type} not found. Check that the parser type is correct "
            f"and all required dependencies are installed."
        )

    def _get_extractor_class(self, extractor_type: str) -> type:
        """
        Dynamically discover and load extractor class by type name.

        Args:
            extractor_type: Name of the extractor type

        Returns:
            Extractor class
        """
        import importlib

        # Handle different naming conventions
        # ContentStatisticsExtractor -> statistics_extractor
        # EntityExtractor -> entity_extractor
        # KeywordExtractor -> keyword_extractor
        # Convert CamelCase to snake_case for directory name
        import re

        snake_name = re.sub("([A-Z]+)", r"_\1", extractor_type).lower().strip("_")
        snake_name = snake_name.replace("__", "_")  # Fix double underscores

        # Build potential module paths to try
        potential_paths = [
            # Try subdirectory first (most extractors are in subdirs)
            f"components.extractors.{snake_name}.{snake_name}",
            f"components.extractors.{snake_name}",
            # Try without 'extractor' suffix
            f"components.extractors.{snake_name.replace('_extractor', '')}.{snake_name.replace('_extractor', '')}_extractor",
            # Try in base module
            "components.extractors.base",
        ]

        # Special cases for known extractors
        if extractor_type == "ContentStatisticsExtractor":
            potential_paths.insert(
                0, "components.extractors.statistics_extractor.statistics_extractor"
            )
        elif extractor_type == "EntityExtractor":
            potential_paths.insert(
                0, "components.extractors.entity_extractor.entity_extractor"
            )
        elif extractor_type == "KeywordExtractor":
            potential_paths.insert(
                0, "components.extractors.keyword_extractor.keyword_extractor"
            )

        # Try to import from potential paths
        extractor_load_errors = []
        for module_path in potential_paths:
            try:
                logger.debug(f"Trying to import extractor from: {module_path}")
                module = importlib.import_module(module_path)

                # Try to get the class with the exact name first
                if hasattr(module, extractor_type):
                    extractor_class = getattr(module, extractor_type)
                    logger.debug(
                        f"Successfully loaded {extractor_type} from {module_path}"
                    )
                    return extractor_class

                # Try variations of the class name
                for attr_name in dir(module):
                    if attr_name.lower() == extractor_type.lower():
                        extractor_class = getattr(module, attr_name)
                        logger.debug(
                            f"Successfully loaded {attr_name} from {module_path}"
                        )
                        return extractor_class

            except (ImportError, AttributeError) as e:
                extractor_load_errors.append(f"Could not load from {module_path}: {e}")
                logger.debug(f"Could not load parser from {module_path}: {e}")
                continue

        # If we couldn't find the extractor, log a warning and return mock
        error_details = "; ".join(extractor_load_errors)
        logger.warning(
            f"Could not dynamically load extractor {extractor_type}. Falling back to mock extractor. Errors: {error_details}"
        )

        class MockExtractor(BaseExtractor):
            def __init__(self, config):
                super().__init__(name=extractor_type, config=config)

            def extract(self, documents):
                # Add mock metadata
                for doc in documents:
                    doc.metadata[f"extractor_{extractor_type}"] = True
                return documents

            def get_dependencies(self):
                return []

        return MockExtractor

    def _matches_patterns(self, filename: str, patterns: list[str]) -> bool:
        """
        Check if filename matches any of the glob patterns.

        Args:
            filename: Name of the file
            patterns: List of glob patterns to match against

        Returns:
            True if filename matches any pattern, False otherwise
        """
        for pattern in patterns:
            if fnmatch.fnmatch(filename.lower(), pattern.lower()):
                return True
        return False

    def _is_excluded(self, filename: str, exclude_patterns: list[str]) -> bool:
        """
        Check if filename matches any of the exclusion patterns.

        Args:
            filename: Name of the file
            exclude_patterns: List of glob patterns to exclude

        Returns:
            True if filename should be excluded, False otherwise
        """
        return (
            self._matches_patterns(filename, exclude_patterns)
            if exclude_patterns
            else False
        )

    def process_blob(
        self, blob_data: bytes, metadata: dict[str, Any]
    ) -> list[Document]:
        """
        Process a blob of data with automatic parser selection based on file patterns.

        Simplified logic - no fallback, explicit configuration required:
        1. Find parsers matching file extension/pattern
        2. If no match â†’ raise UnsupportedFileTypeError
        3. Try parsers in priority order
        4. If all fail â†’ raise ParserFailedError

        Args:
            blob_data: Raw bytes of the document
            metadata: Metadata including filename, content_type, etc.

        Returns:
            List of processed Document objects

        Raises:
            UnsupportedFileTypeError: If no parser is configured for this file type
            ParserFailedError: If all configured parsers fail to process the file
        """
        filename = metadata.get("filename", "unknown")
        extension = get_file_extension(filename)

        logger.info(f"Processing blob: {filename}")
        logger.debug(f"Blob metadata: {metadata}")

        # Find matching parsers based on file patterns
        matching_parsers = self._find_matching_parsers(filename)
        logger.debug(
            f"Found {len(matching_parsers)} matching parsers for {filename}: "
            f"{[p[0].type or None for p in matching_parsers]}"
        )

        # No parser configured for this file type â†’ fail immediately
        # NO FALLBACK LOGIC - explicit configuration required
        if not matching_parsers:
            error_msg = (
                f"No parser configured for file: {filename} (extension: {extension}). "
                f"Add an appropriate parser to your data_processing_strategy."
            )
            logger.error(error_msg)

            available_parser_types = [p[0].type for p in self.parsers if p[0].type]
            raise UnsupportedFileTypeError(
                filename=filename,
                extension=extension,
                available_parsers=available_parser_types,
            )

        # Try parsers in priority order until one succeeds
        documents: list[Document] = []
        tried_parsers: list[str] = []
        parser_errors: list[str] = []

        for config, parser in matching_parsers:
            if not config.type:
                logger.warning(
                    f"Parser config missing 'type': {config}. "
                    f"This may indicate a misconfiguration."
                )
                continue

            parser_type = config.type
            tried_parsers.append(parser_type)

            try:
                logger.debug(
                    f"Attempting to parse {filename} with {parser_type} "
                    f"(priority: {config.priority})"
                )
                documents = parser.parse_blob(blob_data, metadata)

                if documents:
                    # Calculate chunk statistics
                    chunk_sizes = [len(doc.content) for doc in documents]
                    avg_chunk_size = (
                        sum(chunk_sizes) // len(chunk_sizes) if chunk_sizes else 0
                    )

                    logger.info(
                        f"Successfully parsed {filename} with {parser_type} - "
                        f"got {len(documents)} chunks (avg size: {avg_chunk_size} chars)"
                    )

                    # Apply extractors to the documents
                    documents = self._apply_extractors(documents, filename)
                    return documents

            except Exception as e:
                error_msg = f"{parser_type}: {e}"
                parser_errors.append(error_msg)
                logger.warning(f"{parser_type} FAILED for {filename}: {e}")
                continue

        # All configured parsers failed - NO FALLBACK
        logger.error(f"All parsers failed for file: {filename}")
        raise ParserFailedError(
            filename=filename,
            tried_parsers=tried_parsers,
            errors=parser_errors,
        )

    def _find_matching_parsers(self, filename: str) -> list[tuple[Parser, BaseParser]]:
        """
        Find all parsers that match the given filename based on patterns.

        Args:
            filename: Name of the file to match

        Returns:
            List of matching (config, parser) tuples sorted by priority
        """
        matching: list[tuple[Parser, BaseParser]] = []

        for config, parser in self.parsers:
            include_patterns = config.file_include_patterns or []

            # Check if file matches include patterns and not exclude patterns
            if include_patterns:
                if self._matches_patterns(filename, include_patterns):
                    matching.append((config, parser))
            # If no include patterns specified, parser accepts all files (unless excluded)
            else:
                matching.append((config, parser))

        return matching

    def _apply_extractors(
        self, documents: list[Document], filename: str
    ) -> list[Document]:
        """
        Apply matching extractors to the documents based on file patterns.

        Args:
            documents: List of documents to process
            filename: Name of the file being processed

        Returns:
            List of documents with extracted metadata
        """
        # Find matching extractors
        matching_extractors = self._find_matching_extractors(filename)

        # Apply each matching extractor
        extractor_outputs: list[ExtractorOutput] = []
        for config, extractor in matching_extractors:
            try:
                extractor_type = config.type
                logger.debug(f"Applying extractor {extractor_type} to {filename}")

                # Count metadata before extraction
                before_keys: set = set()
                for doc in documents:
                    before_keys.update(doc.metadata.keys())

                # Extractors work on the list of documents
                documents = extractor.extract(documents)

                # Count metadata after extraction
                after_keys: set = set()
                for doc in documents:
                    after_keys.update(doc.metadata.keys())
                    # Mark that this extractor was applied
                    doc.metadata[f"extractor_{extractor_type}"] = True

                # Find what was extracted
                new_keys = after_keys - before_keys - {f"extractor_{extractor_type}"}

                # Count extracted items for specific extractors
                extraction_count = 0
                extractor_type_lower = extractor_type.lower() if extractor_type else ""

                if "keyword" in extractor_type_lower:
                    for doc in documents:
                        if "keywords" in doc.metadata:
                            extraction_count += len(doc.metadata.get("keywords", []))
                elif "entity" in extractor_type_lower:
                    for doc in documents:
                        if "entities" in doc.metadata:
                            extraction_count += len(doc.metadata.get("entities", []))
                elif "link" in extractor_type_lower:
                    for doc in documents:
                        if "links" in doc.metadata:
                            extraction_count += len(doc.metadata.get("links", []))
                elif "heading" in extractor_type_lower:
                    for doc in documents:
                        if "headings" in doc.metadata:
                            extraction_count += len(doc.metadata.get("headings", []))
                elif "table" in extractor_type_lower:
                    for doc in documents:
                        if "tables" in doc.metadata:
                            extraction_count += len(doc.metadata.get("tables", []))

                if extraction_count > 0 or new_keys:
                    extractor_outputs.append(
                        {
                            "name": extractor_type,
                            "count": extraction_count,
                            "new_fields": list(new_keys),
                        }
                    )

            except Exception as e:
                logger.warning(f"Extractor {extractor_type} failed for {filename}: {e}")
                continue

        # Log extractor outputs at debug level
        if extractor_outputs and len(extractor_outputs) > 0:
            logger.debug("\nðŸ” Extractors Applied:")
            for output in extractor_outputs:
                output_count = output["count"] or 0
                output_fields = output.get("new_fields", [])
                if output_count > 0:
                    logger.debug(
                        f"   â”œâ”€ {output['name']}: extracted {output_count} items"
                    )
                elif output_fields:
                    logger.debug(
                        f"   â”œâ”€ {output['name']}: added fields {output_fields}"
                    )
                else:
                    logger.debug(f"   â”œâ”€ {output['name']}: applied")

        return documents

    def _find_matching_extractors(
        self, filename: str
    ) -> list[tuple[Extractor, BaseExtractor]]:
        """
        Find all extractors that match the given filename based on patterns.

        Args:
            filename: Name of the file to match

        Returns:
            List of matching (config, extractor) tuples sorted by priority
        """
        matching: list[tuple[Extractor, BaseExtractor]] = []

        for config, extractor in self.extractors:
            include_patterns = config.file_include_patterns or []

            # Check if file matches include patterns and not exclude patterns
            if include_patterns:
                if self._matches_patterns(filename, include_patterns):
                    matching.append((config, extractor))
            # If no include patterns specified, extractor applies to all files
            else:
                matching.append((config, extractor))

        return matching

    def get_supported_extensions(self) -> list[str]:
        """
        Get list of all supported file extensions from all parsers.

        Returns:
            List of supported extensions
        """
        extensions = set()
        for config, _ in self.parsers:
            patterns = config.file_include_patterns or []
            for pattern in patterns:
                # Extract extensions from patterns like "*.pdf"
                if pattern.startswith("*."):
                    extensions.add(pattern[1:])  # Remove the "*"
        return sorted(extensions)
