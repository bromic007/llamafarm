"""Baseline chat API behaviour highlighting current session issues."""

import time
import uuid
from types import SimpleNamespace

import pytest
from fastapi.testclient import TestClient

from api.main import llama_farm_api
from config.datamodel import (
    LlamaFarmConfig,
    PromptSet,
    PromptMessage,
    Provider,
    Runtime,
    Model,
)
from services.project_chat_service import FALLBACK_ECHO_RESPONSE


@pytest.fixture(autouse=True)
def reset_agent_sessions():
    from api.routers.projects import projects

    with projects._agent_sessions_lock:  # noqa: SLF001
        projects.agent_sessions.clear()
    yield
    with projects._agent_sessions_lock:  # noqa: SLF001
        projects.agent_sessions.clear()


@pytest.fixture()
def app_client(mocker):
    default_config = LlamaFarmConfig(
        version="v1",
        name="llamafarm-1",
        namespace="default",
        prompts=[
            PromptSet(
                name="default",
                messages=[
                    PromptMessage(
                        role="system", content="You are the default project assistant."
                    )
                ],
            )
        ],
        runtime=Runtime(
            models=[
                Model(
                    name="default",
                    provider=Provider.ollama,
                    model="dummy-model",
                )
            ]
        ),
    )
    seed_config = LlamaFarmConfig(
        version="v1",
        name="project_seed",
        namespace="llamafarm",
        prompts=[
            PromptSet(
                name="default",
                messages=[
                    PromptMessage(
                        role="system", content="You are the seed project assistant."
                    )
                ],
            )
        ],
        runtime=Runtime(
            models=[
                Model(
                    name="default",
                    provider=Provider.ollama,
                    model="dummy-model",
                )
            ]
        ),
    )

    def load_config(namespace: str, project_id: str):
        if namespace == "llamafarm" and project_id == "project_seed":
            return seed_config
        if namespace == "default" and project_id == "llamafarm-1":
            return default_config
        raise AssertionError(f"Unexpected project request: {namespace}/{project_id}")

    mocker.patch(
        "api.routers.projects.projects.ProjectService.load_config",
        side_effect=load_config,
    )
    mocker.patch(
        "api.routers.projects.projects.ProjectService.get_project_dir",
        return_value="/tmp",
    )

    class StubAgent:
        def __init__(self, tag: str):
            self.tag = tag
            self.model_name = "stub-model"
            self.context_providers = {}
            self.history = []
            self._persist_enabled = False
            self._session_id = None

        def register_context_provider(self, name: str, provider):
            self.context_providers[name] = provider

        def remove_context_provider(self, name: str):
            if name in self.context_providers:
                del self.context_providers[name]

        def enable_persistence(self, *, session_id: str):
            self._persist_enabled = True
            self._session_id = session_id

        async def run_async(
            self,
            *,
            user_input: dict | None = None,
            input_schema=None,
            **_,
        ):
            if user_input is not None:
                chat_message = user_input
            elif input_schema is not None:
                chat_message = input_schema.chat_message
            else:
                chat_message = {"role": "user", "content": ""}
            self.history.append(chat_message)
            return {
                "id": f"chatcmpl-{uuid.uuid4().hex}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": self.model_name,
                "choices": [
                    {
                        "index": 0,
                        "finish_reason": "stop",
                        "message": {
                            "role": "assistant",
                            "content": chat_message["content"],
                        },
                    }
                ],
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0,
                },
            }

        async def run_async_stream(self, *, user_input=None, **_):
            # Match the LFAgent signature with user_input keyword arg
            message_content = (user_input or {}).get("content")
            self.history.append(message_content)

            class FakeChunk:
                def __init__(self, chunk_content: str | None):
                    self._content = chunk_content

                def model_dump(self, exclude_none: bool = True) -> dict:
                    delta: dict[str, str] = {}
                    if self._content is not None:
                        delta["content"] = self._content
                    if exclude_none:
                        delta = {k: v for k, v in delta.items() if v is not None}
                    return {
                        "choices": [
                            {
                                "delta": delta,
                                "finish_reason": "stop",
                            }
                        ]
                    }

            if message_content == "no-content":
                # Simulate providers that stream no usable content
                return

            # Echo back the user input to trigger fallback detection
            # (simulates a model that just echoes without being helpful)
            yield FakeChunk(message_content)

    def make_agent(
        project_config: LlamaFarmConfig,
        project_dir: str,
        model_name: str | None = None,
        session_id: str | None = None,
    ):
        tag = f"{project_config.namespace}/{project_config.name}"
        agent = StubAgent(tag)
        # Add model_name attribute for fallback testing
        agent.model_name = model_name or "test-model"
        return agent

    mocker.patch(
        "api.routers.projects.projects.ChatOrchestratorAgentFactory.create_agent",
        side_effect=make_agent,
    )

    return TestClient(llama_farm_api())


def _post_chat(
    client: TestClient,
    namespace: str,
    project: str,
    payload: dict,
    session: str | None = None,
):
    headers = {"Content-Type": "application/json"}
    if session:
        headers["X-Session-ID"] = session
    return client.post(
        f"/v1/projects/{namespace}/{project}/chat/completions",
        json=payload,
        headers=headers,
    )


def _delete_session(client: TestClient, namespace: str, project: str, session: str):
    return client.delete(f"/v1/projects/{namespace}/{project}/chat/sessions/{session}")


def _delete_all_sessions(client: TestClient, namespace: str, project: str):
    return client.delete(f"/v1/projects/{namespace}/{project}/chat/sessions")


def _stream_chat(
    client: TestClient,
    namespace: str,
    project: str,
    payload: dict,
    session: str | None = None,
) -> str:
    headers = {"Content-Type": "application/json"}
    if session:
        headers["X-Session-ID"] = session
    chunks: list[str] = []
    with client.stream(
        "POST",
        f"/v1/projects/{namespace}/{project}/chat/completions",
        json=payload,
        headers=headers,
    ) as resp:
        assert resp.status_code == 200, resp.text
        for line in resp.iter_lines():
            if isinstance(line, bytes):
                if not line or not line.startswith(b"data:"):
                    continue
                data_bytes = line[len(b"data:") :].strip()
            else:
                if not line or not line.startswith("data:"):
                    continue
                data_bytes = line[len("data:") :].strip().encode()
            if data_bytes == b"[DONE]":
                break
            try:
                import json

                payload_json = json.loads(data_bytes.decode())
            except ValueError:
                continue
            delta = payload_json.get("choices", [{}])[0].get("delta", {})
            content = delta.get("content")
            if content:
                chunks.append(content)
    return "".join(chunks)


def test_default_project_chat_should_not_use_seed_session(app_client):
    """Test that different projects maintain separate sessions."""
    payload = {"messages": [{"role": "user", "content": "hello"}]}
    shared_session = "sess-123"

    seed_resp = _post_chat(
        app_client, "llamafarm", "project_seed", payload, session=shared_session
    )
    assert seed_resp.status_code == 200
    seed_content = seed_resp.json()["choices"][0]["message"]["content"]
    assert "hello" in seed_content

    default_resp = _post_chat(
        app_client, "default", "llamafarm-1", payload, session=shared_session
    )
    assert default_resp.status_code == 200
    default_content = default_resp.json()["choices"][0]["message"]["content"]
    assert "hello" in default_content
    # Verify sessions are separate (different agents handle them)


def test_seed_project_chat_creates_session(app_client):
    """Test that chat creates a session ID."""
    payload = {"messages": [{"role": "user", "content": "hello"}]}
    resp = _post_chat(app_client, "llamafarm", "project_seed", payload)
    assert resp.status_code == 200
    assert resp.headers.get("X-Session-ID")
    content = resp.json()["choices"][0]["message"]["content"]
    assert "hello" in content


def test_delete_specific_session(app_client):
    payload = {"messages": [{"role": "user", "content": "reset"}]}
    resp = _post_chat(app_client, "default", "llamafarm-1", payload)
    assert resp.status_code == 200
    session = resp.headers.get("X-Session-ID")
    assert session

    delete_resp = _delete_session(app_client, "default", "llamafarm-1", session)
    assert delete_resp.status_code == 200
    assert delete_resp.json()["message"].startswith("Session")

    resp_again = _post_chat(
        app_client, "default", "llamafarm-1", payload, session=session
    )
    assert resp_again.status_code == 200
    assert resp_again.headers.get("X-Session-ID") == session


def test_delete_all_project_sessions(app_client):
    payload = {"messages": [{"role": "user", "content": "hello"}]}
    first = _post_chat(app_client, "llamafarm", "project_seed", payload)
    second = _post_chat(app_client, "llamafarm", "project_seed", payload)
    assert first.headers.get("X-Session-ID")
    assert second.headers.get("X-Session-ID")

    delete_resp = _delete_all_sessions(app_client, "llamafarm", "project_seed")
    assert delete_resp.status_code == 200
    assert delete_resp.json()["count"] >= 1

    third = _post_chat(app_client, "llamafarm", "project_seed", payload)
    third_session = third.headers.get("X-Session-ID")
    assert third_session
    assert third_session != first.headers.get("X-Session-ID")


def test_stream_default_project_returns_content(app_client):
    """Test that streaming returns agent output."""
    payload = {"messages": [{"role": "user", "content": "hello"}], "stream": True}
    streamed = _stream_chat(app_client, "default", "llamafarm-1", payload)
    # The stub agent echoes back the input
    assert streamed == "hello"


def test_stream_dev_project_returns_content(app_client):
    """Test that streaming returns agent output for seed project."""
    payload = {"messages": [{"role": "user", "content": "hello"}], "stream": True}
    streamed = _stream_chat(app_client, "llamafarm", "project_seed", payload)
    # The stub agent echoes back the input
    assert streamed == "hello"


def test_stream_empty_output_uses_fallback(app_client):
    """Test that empty streaming output triggers fallback message."""
    payload = {"messages": [{"role": "user", "content": "no-content"}], "stream": True}
    streamed = _stream_chat(app_client, "default", "llamafarm-1", payload)
    # Empty output from stub agent should trigger fallback
    assert streamed == FALLBACK_ECHO_RESPONSE
