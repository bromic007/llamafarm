# generated by datamodel-codegen:
#   filename:  schema.deref.yaml
#   timestamp: 2025-09-10T12:52:46+00:00

from __future__ import annotations
from enum import Enum
from typing import Any, Literal, Optional, Union
from pydantic import AnyUrl, BaseModel, ConfigDict, Field, RootModel, confloat, conint, constr


class Version(Enum):
    v1 = "v1"


class Prompt(BaseModel):
    role: Optional[str] = Field(
        None, description='Prompt role (e.g., "system", "user", "assistant")'
    )
    content: str = Field(..., description='Prompt content (e.g., "You are a helpful assistant.")')


class Tag(RootModel[constr(pattern=r"^[a-z0-9_-]+$")]):
    root: constr(pattern=r"^[a-z0-9_-]+$")


class UseCase(RootModel[constr(min_length=3)]):
    root: constr(min_length=3)


class Type(Enum):
    PDFParser_PyPDF2 = "PDFParser_PyPDF2"
    PDFParser_LlamaIndex = "PDFParser_LlamaIndex"
    CSVParser_Pandas = "CSVParser_Pandas"
    CSVParser_Python = "CSVParser_Python"
    CSVParser_LlamaIndex = "CSVParser_LlamaIndex"
    ExcelParser_OpenPyXL = "ExcelParser_OpenPyXL"
    ExcelParser_Pandas = "ExcelParser_Pandas"
    ExcelParser_LlamaIndex = "ExcelParser_LlamaIndex"
    DocxParser_PythonDocx = "DocxParser_PythonDocx"
    DocxParser_LlamaIndex = "DocxParser_LlamaIndex"
    MarkdownParser_Python = "MarkdownParser_Python"
    MarkdownParser_LlamaIndex = "MarkdownParser_LlamaIndex"
    TextParser_Python = "TextParser_Python"
    TextParser_LlamaIndex = "TextParser_LlamaIndex"


class ChunkStrategy(Enum):
    paragraphs = "paragraphs"
    sentences = "sentences"
    characters = "characters"


class Config(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        1000, description="Chunk size in characters"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        100, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy] = Field(
        "paragraphs", description="Chunking strategy using PyPDF2 text structure"
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract PDF metadata using PyPDF2")
    preserve_layout: Optional[bool] = Field(
        True, description="Use PyPDF2 layout-preserving extraction mode"
    )
    extract_page_info: Optional[bool] = Field(
        True, description="Extract page numbers and rotation info"
    )
    extract_annotations: Optional[bool] = Field(
        False, description="Extract PDF annotations using PyPDF2"
    )
    extract_links: Optional[bool] = Field(False, description="Extract hyperlinks")
    extract_form_fields: Optional[bool] = Field(
        False, description="Extract form fields using PyPDF2"
    )
    extract_outlines: Optional[bool] = Field(
        False, description="Extract document outlines/bookmarks"
    )
    extract_images: Optional[bool] = Field(
        False, description="Extract embedded images using PyPDF2"
    )
    extract_xmp_metadata: Optional[bool] = Field(
        False, description="Extract XMP metadata using PyPDF2"
    )
    clean_text: Optional[bool] = Field(True, description="Clean extracted text")


class ChunkStrategy1(Enum):
    sentences = "sentences"
    paragraphs = "paragraphs"
    pages = "pages"
    semantic = "semantic"


class FallbackStrategy(Enum):
    llama_pdf_reader = "llama_pdf_reader"
    llama_pymupdf_reader = "llama_pymupdf_reader"
    direct_pymupdf = "direct_pymupdf"
    pypdf2_fallback = "pypdf2_fallback"


class Config1(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        1000, description="Chunk size in characters"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        100, description="Overlap between chunks"
    )
    chunk_strategy: Optional[ChunkStrategy1] = Field(
        "sentences", description="Chunking strategy for PDF content"
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract PDF metadata")
    extract_images: Optional[bool] = Field(False, description="Extract images from PDF")
    extract_tables: Optional[bool] = Field(True, description="Extract tables from PDF")
    fallback_strategies: Optional[list[FallbackStrategy]] = Field(
        ["llama_pdf_reader", "llama_pymupdf_reader", "direct_pymupdf", "pypdf2_fallback"],
        description="Fallback strategies to try in order",
    )


class ChunkStrategy2(Enum):
    rows = "rows"
    columns = "columns"
    full = "full"


class Config2(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100)] = Field(1000, description="Number of rows per chunk")
    chunk_strategy: Optional[ChunkStrategy2] = Field(
        "rows", description="How to chunk the CSV data"
    )
    extract_metadata: Optional[bool] = Field(
        True, description="Extract data statistics and metadata"
    )
    encoding: Optional[str] = Field("utf-8", description="File encoding")
    delimiter: Optional[str] = Field(",", description="CSV delimiter")
    na_values: Optional[list[str]] = Field(
        ["", "NA", "N/A", "null", "None"], description="Values to treat as NaN"
    )


class Config3(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100)] = Field(1000, description="Number of rows per chunk")
    encoding: Optional[str] = Field("utf-8", description="File encoding")
    delimiter: Optional[str] = Field(",", description="CSV delimiter")
    quotechar: Optional[str] = Field('"', description="Quote character")


class ChunkStrategy3(Enum):
    rows = "rows"
    semantic = "semantic"
    full = "full"


class Config4(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        1000, description="Number of rows per chunk"
    )
    chunk_strategy: Optional[ChunkStrategy3] = Field("rows", description="Chunking strategy")
    field_mapping: Optional[dict[str, str]] = Field(
        None, description="Map CSV columns to standard fields"
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract metadata from CSV")
    combine_fields: Optional[bool] = Field(True, description="Combine fields into text content")
    skiprows: Optional[conint(ge=0)] = Field(
        None, description="Number of rows to skip at beginning"
    )
    na_values: Optional[list[str]] = Field(
        ["", "NA", "N/A", "null", "None"], description="Values to treat as missing"
    )


class Config5(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100)] = Field(1000, description="Number of rows per chunk")
    extract_formulas: Optional[bool] = Field(
        False, description="Extract cell formulas using OpenPyXL"
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract workbook metadata")
    sheets: Optional[list[str]] = Field(None, description="Specific sheets to process (null = all)")
    data_only: Optional[bool] = Field(True, description="Extract values instead of formulas")


class Config6(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100)] = Field(1000, description="Number of rows per chunk")
    sheets: Optional[list[str]] = Field(None, description="Specific sheets to process (null = all)")
    extract_metadata: Optional[bool] = Field(True, description="Extract data statistics")
    skiprows: Optional[int] = Field(None, description="Rows to skip at beginning")
    na_values: Optional[list[str]] = Field(
        ["", "NA", "N/A", "null", "None"], description="Values to treat as NaN"
    )


class Config7(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        1000, description="Number of rows per chunk"
    )
    chunk_strategy: Optional[ChunkStrategy3] = Field("rows", description="Chunking strategy")
    sheets: Optional[list[str]] = Field(None, description="Specific sheets to parse (null for all)")
    combine_sheets: Optional[bool] = Field(
        False, description="Combine all sheets into one document"
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract metadata from Excel")
    extract_formulas: Optional[bool] = Field(
        False, description="Extract formulas instead of values"
    )
    header_row: Optional[conint(ge=0)] = Field(0, description="Row index for headers")
    skiprows: Optional[conint(ge=0)] = Field(None, description="Number of rows to skip")
    na_values: Optional[list[str]] = Field(
        ["", "NA", "N/A", "null", "None"], description="Values to treat as missing"
    )


class ChunkStrategy5(Enum):
    paragraphs = "paragraphs"
    sentences = "sentences"
    characters = "characters"


class Config8(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100)] = Field(1000, description="Chunk size in characters")
    chunk_strategy: Optional[ChunkStrategy5] = Field("paragraphs", description="Chunking strategy")
    extract_metadata: Optional[bool] = Field(True, description="Extract document metadata")
    extract_tables: Optional[bool] = Field(True, description="Extract tables using python-docx")
    extract_headers: Optional[bool] = Field(True, description="Extract headers")
    extract_footers: Optional[bool] = Field(False, description="Extract footers")
    extract_comments: Optional[bool] = Field(False, description="Extract comments")


class ChunkStrategy6(Enum):
    paragraphs = "paragraphs"
    sentences = "sentences"
    semantic = "semantic"


class Config9(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        1000, description="Chunk size in characters"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        100, description="Overlap between chunks"
    )
    chunk_strategy: Optional[ChunkStrategy6] = Field("paragraphs", description="Chunking strategy")
    extract_metadata: Optional[bool] = Field(True, description="Extract document metadata")
    extract_tables: Optional[bool] = Field(True, description="Extract tables from document")
    extract_images: Optional[bool] = Field(False, description="Extract images from document")
    preserve_formatting: Optional[bool] = Field(True, description="Preserve text formatting")
    include_header_footer: Optional[bool] = Field(
        False, description="Include header and footer content"
    )


class ChunkStrategy7(Enum):
    sections = "sections"
    paragraphs = "paragraphs"
    characters = "characters"


class Config10(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100)] = Field(1000, description="Chunk size in characters")
    chunk_strategy: Optional[ChunkStrategy7] = Field(
        "sections", description="Chunking strategy - sections uses markdown headers"
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract YAML frontmatter")
    extract_code_blocks: Optional[bool] = Field(True, description="Extract code blocks")
    extract_links: Optional[bool] = Field(True, description="Extract markdown links")


class ChunkStrategy8(Enum):
    headings = "headings"
    paragraphs = "paragraphs"
    sentences = "sentences"
    semantic = "semantic"


class Config11(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        1000, description="Chunk size in characters"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        100, description="Overlap between chunks"
    )
    chunk_strategy: Optional[ChunkStrategy8] = Field(
        "headings", description="Chunking strategy for markdown"
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract frontmatter metadata")
    extract_code_blocks: Optional[bool] = Field(True, description="Extract code blocks separately")
    extract_tables: Optional[bool] = Field(True, description="Extract markdown tables")
    extract_links: Optional[bool] = Field(True, description="Extract links and references")
    preserve_structure: Optional[bool] = Field(True, description="Preserve heading hierarchy")


class ChunkStrategy9(Enum):
    sentences = "sentences"
    paragraphs = "paragraphs"
    characters = "characters"


class Config12(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100)] = Field(1000, description="Chunk size in characters")
    chunk_overlap: Optional[conint(ge=0)] = Field(100, description="Overlap between chunks")
    chunk_strategy: Optional[ChunkStrategy9] = Field(
        "sentences", description="Text chunking strategy"
    )
    encoding: Optional[str] = Field("utf-8", description="Text encoding (utf-8 or auto-detect)")
    clean_text: Optional[bool] = Field(True, description="Remove excessive whitespace")
    extract_metadata: Optional[bool] = Field(True, description="Extract file statistics")


class ChunkStrategy10(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"
    tokens = "tokens"
    semantic = "semantic"
    code = "code"


class Config13(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        1000, description="Chunk size in characters"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        100, description="Overlap between chunks"
    )
    chunk_strategy: Optional[ChunkStrategy10] = Field(
        "semantic",
        description="Advanced chunking strategy - semantic uses content-based splitting, code preserves syntax",
    )
    encoding: Optional[str] = Field("utf-8", description="Text encoding")
    clean_text: Optional[bool] = Field(True, description="Clean extracted text")
    extract_metadata: Optional[bool] = Field(
        True, description="Extract comprehensive file and content metadata"
    )
    semantic_buffer_size: Optional[conint(ge=1, le=10)] = Field(
        1, description="Buffer size for semantic chunking"
    )
    semantic_breakpoint_percentile_threshold: Optional[conint(ge=50, le=99)] = Field(
        95, description="Percentile threshold for semantic breakpoints"
    )
    token_model: Optional[str] = Field(
        "gpt-3.5-turbo", description="Tokenizer model for token-based chunking"
    )
    preserve_code_structure: Optional[bool] = Field(
        True, description="Preserve code syntax and structure when parsing code files"
    )
    detect_language: Optional[bool] = Field(
        True, description="Automatically detect programming language for code files"
    )
    include_prev_next_rel: Optional[bool] = Field(
        True, description="Include relationships between chunks for better context"
    )


class Config14(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    recursive: Optional[bool] = Field(True, description="Recursively parse subdirectories")
    include_patterns: Optional[list[str]] = Field(None, description="File patterns to include")
    exclude_patterns: Optional[list[str]] = Field(None, description="File patterns to exclude")
    parser_map: Optional[dict[str, str]] = Field(
        None, description="Mapping of file extensions to parser types"
    )
    parser_configs: Optional[dict[str, dict[str, Any]]] = Field(
        None, description="Configuration for each parser type"
    )


class Config15(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    content_fields: Optional[list[str]] = Field(
        ["subject", "body"], description="CSV columns to use as document content", min_length=1
    )
    metadata_fields: Optional[list[str]] = Field(
        [], description="CSV columns to include as metadata"
    )
    id_field: Optional[str] = Field(None, description="CSV column to use as document ID")
    combine_content: Optional[bool] = Field(
        True, description="Combine multiple content fields into one document"
    )
    content_separator: Optional[str] = Field(
        "\n\n", description="Separator when combining content fields"
    )
    priority_mapping: Optional[dict[str, Union[str, float]]] = Field(
        None,
        description="Mapping for priority field values",
        examples=[{"Critical": 1, "High": 2, "Medium": 3, "Low": 4}],
    )


class ChunkStrategy11(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"
    pages = "pages"


class Config16(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract PDF metadata")
    extract_page_structure: Optional[bool] = Field(True, description="Extract page-level structure")
    combine_pages: Optional[bool] = Field(
        True, description="Combine all pages into single document"
    )
    page_separator: Optional[str] = Field(
        "\n\n--- Page Break ---\n\n", description="Separator between pages"
    )
    min_text_length: Optional[conint(ge=0)] = Field(
        10, description="Minimum text length to include page"
    )
    include_page_numbers: Optional[bool] = Field(True, description="Include page numbers in text")
    extract_outline: Optional[bool] = Field(True, description="Extract document outline")
    extract_images: Optional[bool] = Field(False, description="Extract embedded images")
    ocr_enabled: Optional[bool] = Field(False, description="Enable OCR for scanned documents")
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy11] = Field(
        "characters", description="Chunking strategy - pages splits at PDF page boundaries"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle when using character chunking"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        False,
        description="Avoid splitting paragraphs in the middle (overrides sentence boundaries)",
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        50, description="Minimum chunk size to avoid creating tiny chunks"
    )


class ChunkStrategy12(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"
    headings = "headings"


class Config17(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract YAML frontmatter")
    extract_headings: Optional[bool] = Field(True, description="Extract heading structure")
    extract_links: Optional[bool] = Field(True, description="Extract all links")
    extract_code_blocks: Optional[bool] = Field(True, description="Extract code blocks")
    chunk_by_headings: Optional[bool] = Field(False, description="Split by headings")
    preserve_formatting: Optional[bool] = Field(False, description="Preserve Markdown formatting")
    heading_level_split: Optional[conint(ge=1, le=6)] = Field(
        2, description="Heading level for splitting"
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None,
        description="Chunk size in characters (null for no chunking, overrides chunk_by_headings)",
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy12] = Field(
        "headings", description="Chunking strategy - headings preserves markdown structure"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle when using character chunking"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting paragraphs in the middle"
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        100, description="Minimum chunk size to avoid creating tiny chunks"
    )


class ChunkStrategy13(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"
    elements = "elements"


class Config18(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract meta tags")
    extract_links: Optional[bool] = Field(True, description="Extract hyperlinks")
    extract_images: Optional[bool] = Field(True, description="Extract image sources")
    preserve_structure: Optional[bool] = Field(False, description="Preserve HTML structure")
    remove_scripts: Optional[bool] = Field(True, description="Remove JavaScript")
    remove_styles: Optional[bool] = Field(True, description="Remove CSS")
    text_only: Optional[bool] = Field(False, description="Extract only text")
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy13] = Field(
        "elements", description="Chunking strategy - elements uses HTML semantic structure"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle when using character chunking"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting paragraphs in the middle"
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        100, description="Minimum chunk size to avoid creating tiny chunks"
    )


class ChunkStrategy14(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"
    sections = "sections"


class Config19(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract document properties")
    extract_headers_footers: Optional[bool] = Field(True, description="Include headers and footers")
    extract_comments: Optional[bool] = Field(True, description="Extract comments")
    extract_tables: Optional[bool] = Field(True, description="Extract tables")
    extract_images: Optional[bool] = Field(False, description="Extract images")
    preserve_formatting: Optional[bool] = Field(False, description="Preserve text formatting")
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy14] = Field(
        "paragraphs", description="Chunking strategy - sections uses Word's built-in structure"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle when using character chunking"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting paragraphs in the middle"
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        100, description="Minimum chunk size to avoid creating tiny chunks"
    )


class TableFormat(Enum):
    csv = "csv"
    markdown = "markdown"
    json = "json"
    text = "text"


class Config20(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    sheet_names: Optional[list[str]] = Field(None, description="Specific sheets to parse")
    combine_sheets: Optional[bool] = Field(False, description="Combine all sheets")
    extract_formulas: Optional[bool] = Field(False, description="Extract cell formulas")
    extract_charts: Optional[bool] = Field(False, description="Extract chart metadata")
    table_format: Optional[TableFormat] = Field("markdown", description="Table output format")
    header_row: Optional[conint(ge=0)] = Field(0, description="Header row index")


class ChunkStrategy15(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"


class SplitBy(Enum):
    characters = "characters"
    words = "words"
    sentences = "sentences"
    paragraphs = "paragraphs"
    lines = "lines"


class Config21(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    encoding: Optional[str] = Field(
        "auto", description="Text encoding (auto-detect or specify like utf-8, latin1, etc.)"
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy15] = Field(
        "characters", description="Chunking strategy - how to split the text"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle when using character chunking"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        False,
        description="Avoid splitting paragraphs in the middle (overrides sentence boundaries)",
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        50, description="Minimum chunk size to avoid creating tiny chunks"
    )
    preserve_line_breaks: Optional[bool] = Field(
        True, description="Preserve line breaks in the text"
    )
    strip_empty_lines: Optional[bool] = Field(
        True, description="Remove empty lines while preserving paragraph structure"
    )
    detect_structure: Optional[bool] = Field(
        True, description="Detect and extract structural elements (headers, lists, code blocks)"
    )
    split_by: Optional[SplitBy] = Field("characters", description="Splitting method")
    preserve_whitespace: Optional[bool] = Field(False, description="Preserve whitespace")


class Config22(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    fallback_chain: Optional[list[str]] = Field(
        ["PlainTextParser", "MarkdownParser"], description="Fallback parsers to try in order"
    )
    content_detection_enabled: Optional[bool] = Field(
        True, description="Enable content-based file type detection"
    )
    max_file_size_mb: Optional[conint(ge=1, le=1000)] = Field(
        100, description="Maximum file size in MB to process"
    )
    enable_magic: Optional[bool] = Field(
        True, description="Enable python-magic for file type detection"
    )


class ChunkStrategy16(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"
    tokens = "tokens"


class Config23(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    encoding: Optional[str] = Field("auto", description="Text encoding (auto-detect or specify)")
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy16] = Field(
        "characters", description="Chunking strategy for LlamaIndex"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        False, description="Avoid splitting paragraphs in the middle"
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        50, description="Minimum chunk size to avoid creating tiny chunks"
    )
    preserve_line_breaks: Optional[bool] = Field(
        True, description="Preserve line breaks in the text"
    )
    strip_empty_lines: Optional[bool] = Field(
        True, description="Remove empty lines while preserving paragraph structure"
    )
    detect_structure: Optional[bool] = Field(
        True, description="Detect and extract structural elements"
    )


class Config24(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract PDF metadata")
    extract_page_structure: Optional[bool] = Field(True, description="Extract page-level structure")
    combine_pages: Optional[bool] = Field(
        True, description="Combine all pages into single document"
    )
    page_separator: Optional[str] = Field(
        "\n\n--- Page Break ---\n\n", description="Separator between pages"
    )
    min_text_length: Optional[conint(ge=0)] = Field(
        10, description="Minimum text length to include page"
    )
    include_page_numbers: Optional[bool] = Field(True, description="Include page numbers in text")
    extract_outline: Optional[bool] = Field(True, description="Extract document outline")
    fallback_strategies: Optional[list[FallbackStrategy]] = Field(
        ["llama_pdf_reader", "llama_pymupdf_reader", "direct_pymupdf", "pypdf2_fallback"],
        description="PDF parsing strategies to try in order",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy16] = Field(
        "characters", description="Chunking strategy for LlamaIndex"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        False, description="Avoid splitting paragraphs in the middle"
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        50, description="Minimum chunk size to avoid creating tiny chunks"
    )


class Config25(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    content_fields: Optional[list[str]] = Field(
        ["subject", "body", "content", "description"],
        description="Columns to use as document content",
    )
    metadata_fields: Optional[list[str]] = Field([], description="Columns to include as metadata")
    id_field: Optional[str] = Field(None, description="Column to use as document ID")
    combine_content: Optional[bool] = Field(
        True, description="Combine multiple content fields into one document"
    )
    content_separator: Optional[str] = Field(
        "\n\n", description="Separator when combining content fields"
    )
    priority_mapping: Optional[dict[str, Union[str, float]]] = Field(
        None, description="Mapping for priority field values"
    )
    table_format: Optional[TableFormat] = Field(
        "markdown", description="Table output format for Excel"
    )
    sheet_names: Optional[list[str]] = Field(
        None, description="Specific sheets to parse (Excel only)"
    )
    combine_sheets: Optional[bool] = Field(False, description="Combine all sheets (Excel only)")
    header_row: Optional[conint(ge=0)] = Field(0, description="Header row index (Excel only)")


class Config26(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract document properties")
    extract_headers_footers: Optional[bool] = Field(True, description="Include headers and footers")
    extract_comments: Optional[bool] = Field(False, description="Extract comments")
    extract_tables: Optional[bool] = Field(True, description="Extract tables")
    extract_images: Optional[bool] = Field(False, description="Extract images")
    preserve_formatting: Optional[bool] = Field(False, description="Preserve text formatting")
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy16] = Field(
        "paragraphs", description="Chunking strategy for LlamaIndex"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting paragraphs in the middle"
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        100, description="Minimum chunk size to avoid creating tiny chunks"
    )


class ChunkStrategy19(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"
    headings = "headings"
    tokens = "tokens"


class Config27(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract YAML frontmatter")
    extract_headings: Optional[bool] = Field(True, description="Extract heading structure")
    extract_links: Optional[bool] = Field(True, description="Extract all links")
    extract_code_blocks: Optional[bool] = Field(True, description="Extract code blocks")
    chunk_by_headings: Optional[bool] = Field(False, description="Split by headings")
    preserve_formatting: Optional[bool] = Field(False, description="Preserve Markdown formatting")
    heading_level_split: Optional[conint(ge=1, le=6)] = Field(
        2, description="Heading level for splitting"
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy19] = Field(
        "headings", description="Chunking strategy - headings preserves markdown structure"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting paragraphs in the middle"
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        100, description="Minimum chunk size to avoid creating tiny chunks"
    )


class ChunkStrategy20(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"
    elements = "elements"
    tokens = "tokens"


class Config28(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract meta tags")
    extract_links: Optional[bool] = Field(True, description="Extract hyperlinks")
    extract_images: Optional[bool] = Field(True, description="Extract image sources")
    preserve_structure: Optional[bool] = Field(False, description="Preserve HTML structure")
    remove_scripts: Optional[bool] = Field(True, description="Remove JavaScript")
    remove_styles: Optional[bool] = Field(True, description="Remove CSS")
    text_only: Optional[bool] = Field(False, description="Extract only text")
    request_timeout: Optional[conint(ge=5, le=300)] = Field(
        30, description="Request timeout for URLs in seconds"
    )
    user_agent: Optional[str] = Field(
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        description="User agent for web requests",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy20] = Field(
        "elements", description="Chunking strategy - elements uses HTML semantic structure"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting paragraphs in the middle"
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        100, description="Minimum chunk size to avoid creating tiny chunks"
    )


class Parser(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    type: Type
    config: Union[
        Config,
        Config1,
        Config2,
        Config3,
        Config4,
        Config5,
        Config6,
        Config7,
        Config8,
        Config9,
        Config10,
        Config11,
        Config12,
        Config13,
        Config14,
        Config15,
        Config16,
        Config17,
        Config18,
        Config19,
        Config20,
        Config21,
        Config22,
        Config23,
        Config24,
        Config25,
        Config26,
        Config27,
        Config28,
    ]


class Type1(Enum):
    KeywordExtractor = "KeywordExtractor"
    EntityExtractor = "EntityExtractor"
    DateTimeExtractor = "DateTimeExtractor"
    HeadingExtractor = "HeadingExtractor"
    LinkExtractor = "LinkExtractor"
    PathExtractor = "PathExtractor"
    PatternExtractor = "PatternExtractor"
    StatisticsExtractor = "StatisticsExtractor"
    SummaryExtractor = "SummaryExtractor"
    TableExtractor = "TableExtractor"
    SentimentExtractor = "SentimentExtractor"
    YAKEExtractor = "YAKEExtractor"
    ContentStatisticsExtractor = "ContentStatisticsExtractor"


class Algorithm(Enum):
    rake = "rake"
    yake = "yake"
    tfidf = "tfidf"
    textrank = "textrank"


class Config29(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extractor_type: Literal["keyword"] = Field(
        "keyword", description="Extractor type discriminator"
    )
    algorithm: Optional[Algorithm] = Field("rake", description="Extraction algorithm")
    max_keywords: Optional[conint(ge=1, le=100)] = Field(
        10, description="Maximum keywords to extract"
    )
    min_length: Optional[conint(ge=1)] = Field(1, description="Minimum word length for keywords")
    max_length: Optional[conint(ge=1)] = Field(4, description="Maximum word length for keywords")
    min_frequency: Optional[conint(ge=1)] = Field(1, description="Minimum frequency for keywords")
    stop_words: Optional[list[str]] = Field(None, description="Custom stop words")
    language: Optional[str] = Field("en", description="Language for YAKE algorithm")
    max_ngram_size: Optional[conint(ge=1, le=5)] = Field(
        3, description="Maximum n-gram size for YAKE"
    )
    deduplication_threshold: Optional[confloat(ge=0.0, le=1.0)] = Field(
        0.9, description="Deduplication threshold for YAKE"
    )


class EntityType(Enum):
    PERSON = "PERSON"
    ORG = "ORG"
    GPE = "GPE"
    DATE = "DATE"
    TIME = "TIME"
    MONEY = "MONEY"
    EMAIL = "EMAIL"
    PHONE = "PHONE"
    URL = "URL"
    LAW = "LAW"
    PERCENT = "PERCENT"
    PRODUCT = "PRODUCT"
    EVENT = "EVENT"
    VERSION = "VERSION"
    FAC = "FAC"
    LOC = "LOC"


class Config30(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    model: Optional[str] = Field("en_core_web_sm", description="NER model name")
    entity_types: Optional[list[EntityType]] = Field(
        [
            "PERSON",
            "ORG",
            "GPE",
            "DATE",
            "TIME",
            "MONEY",
            "EMAIL",
            "PHONE",
            "URL",
            "PERCENT",
            "PRODUCT",
            "EVENT",
        ],
        description="Entity types to extract",
    )
    use_fallback: Optional[bool] = Field(True, description="Use regex fallback")
    min_entity_length: Optional[conint(ge=1)] = Field(2, description="Minimum entity length")
    merge_entities: Optional[bool] = Field(True, description="Merge adjacent entities")
    confidence_threshold: Optional[confloat(ge=0.0, le=1.0)] = Field(
        0.7, description="Minimum confidence score"
    )


class PreferDatesFrom(Enum):
    past = "past"
    future = "future"
    current = "current"


class Config31(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    fuzzy_parsing: Optional[bool] = Field(True, description="Enable fuzzy parsing")
    extract_relative: Optional[bool] = Field(True, description="Extract relative dates")
    extract_times: Optional[bool] = Field(True, description="Extract time expressions")
    extract_durations: Optional[bool] = Field(True, description="Extract durations")
    default_timezone: Optional[str] = Field("UTC", description="Default timezone")
    date_format: Optional[str] = Field("ISO", description="Output date format")
    prefer_dates_from: Optional[PreferDatesFrom] = Field(
        "current", description="Preference for ambiguous dates"
    )


class Config32(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    max_level: Optional[conint(ge=1, le=6)] = Field(6, description="Maximum heading level")
    include_hierarchy: Optional[bool] = Field(True, description="Include hierarchy structure")
    extract_outline: Optional[bool] = Field(True, description="Generate document outline")
    min_heading_length: Optional[conint(ge=1)] = Field(3, description="Minimum heading length")
    enabled: Optional[bool] = Field(True, description="Enable this extractor")


class Config33(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_urls: Optional[bool] = Field(True, description="Extract URLs")
    extract_emails: Optional[bool] = Field(True, description="Extract email addresses")
    extract_domains: Optional[bool] = Field(True, description="Extract unique domains")
    validate_urls: Optional[bool] = Field(False, description="Validate URL format")
    resolve_redirects: Optional[bool] = Field(False, description="Resolve URL redirects")
    enabled: Optional[bool] = Field(True, description="Enable this extractor")


class Config34(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_file_paths: Optional[bool] = Field(True, description="Extract file paths")
    extract_urls: Optional[bool] = Field(True, description="Extract URL paths")
    extract_s3_paths: Optional[bool] = Field(True, description="Extract S3 paths")
    validate_paths: Optional[bool] = Field(False, description="Validate path existence")
    normalize_paths: Optional[bool] = Field(True, description="Normalize path formats")
    enabled: Optional[bool] = Field(True, description="Enable this extractor")


class PredefinedPattern(Enum):
    email = "email"
    phone = "phone"
    url = "url"
    ip = "ip"
    ip_address = "ip_address"
    ssn = "ssn"
    credit_card = "credit_card"
    zip_code = "zip_code"
    file_path = "file_path"
    version = "version"
    date = "date"


class CustomPattern(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    name: str = Field(..., description="Pattern name")
    pattern: str = Field(..., description="Regex pattern")
    description: Optional[str] = Field(None, description="Pattern description")


class Pattern(BaseModel):
    name: str = Field(..., description="Pattern name")
    pattern: str = Field(..., description="Regex pattern")


class Config35(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    predefined_patterns: Optional[list[PredefinedPattern]] = Field(
        [],
        description="Use built-in patterns for common data types (e.g., email, phone). Takes precedence over 'patterns' field for matching pattern names",
    )
    custom_patterns: Optional[list[CustomPattern]] = Field([], description="Custom regex patterns")
    case_sensitive: Optional[bool] = Field(False, description="Case-sensitive matching")
    return_positions: Optional[bool] = Field(False, description="Return match positions")
    include_context: Optional[bool] = Field(
        False, description="Include surrounding context in results"
    )
    patterns: Optional[list[Pattern]] = Field([], description="Custom patterns to match")
    max_matches_per_pattern: Optional[conint(ge=1)] = Field(
        100, description="Maximum matches per pattern"
    )
    deduplicate_matches: Optional[bool] = Field(True, description="Remove duplicate matches")


class Config36(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    include_readability: Optional[bool] = Field(True, description="Calculate readability scores")
    include_vocabulary: Optional[bool] = Field(True, description="Analyze vocabulary")
    include_structure: Optional[bool] = Field(True, description="Analyze text structure")
    include_sentiment: Optional[bool] = Field(False, description="Basic sentiment analysis")
    include_sentiment_indicators: Optional[bool] = Field(
        False, description="Include detailed sentiment indicators"
    )
    include_language: Optional[bool] = Field(True, description="Detect language")


class Algorithm1(Enum):
    textrank = "textrank"
    lsa = "lsa"
    luhn = "luhn"
    lexrank = "lexrank"


class Config37(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    summary_sentences: Optional[conint(ge=1, le=10)] = Field(
        3, description="Number of summary sentences"
    )
    algorithm: Optional[Algorithm1] = Field("textrank", description="Summarization algorithm")
    include_key_phrases: Optional[bool] = Field(True, description="Extract key phrases")
    include_statistics: Optional[bool] = Field(True, description="Include text statistics")
    min_sentence_length: Optional[conint(ge=1)] = Field(
        10, description="Minimum sentence length for summary"
    )
    max_sentence_length: Optional[conint(ge=10)] = Field(
        500, description="Maximum sentence length for summary"
    )


class OutputFormat(Enum):
    dict = "dict"
    list = "list"
    csv = "csv"
    markdown = "markdown"


class Config38(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    output_format: Optional[OutputFormat] = Field("dict", description="Output format")
    extract_headers: Optional[bool] = Field(True, description="Extract table headers")
    merge_cells: Optional[bool] = Field(True, description="Handle merged cells")
    min_rows: Optional[conint(ge=1)] = Field(2, description="Minimum rows for table")
    enabled: Optional[bool] = Field(True, description="Enable this extractor")


class Category(Enum):
    positive = "positive"
    negative = "negative"
    neutral = "neutral"
    mixed = "mixed"


class Config39(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    analyze_business_tone: Optional[bool] = Field(
        False, description="Analyze business-specific tone"
    )
    extract_confidence: Optional[bool] = Field(True, description="Extract confidence scores")
    categories: Optional[list[Category]] = Field(
        ["positive", "negative", "neutral"], description="Sentiment categories to analyze"
    )


class Config40(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extractor_type: Literal["yake"] = Field("yake", description="Extractor type discriminator")
    max_keywords: Optional[conint(ge=1, le=100)] = Field(
        10, description="Maximum keywords to extract"
    )
    language: Optional[str] = Field("en", description="Language for YAKE algorithm")
    max_ngram_size: Optional[conint(ge=1, le=5)] = Field(
        3, description="Maximum n-gram size for YAKE"
    )
    deduplication_threshold: Optional[confloat(ge=0.0, le=1.0)] = Field(
        0.9, description="Deduplication threshold for YAKE"
    )


class Config41(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    include_readability: Optional[bool] = Field(True, description="Calculate readability scores")
    include_vocabulary: Optional[bool] = Field(True, description="Analyze vocabulary")
    include_structure: Optional[bool] = Field(True, description="Analyze text structure")
    include_sentiment_indicators: Optional[bool] = Field(
        False, description="Include detailed sentiment indicators"
    )


class Extractor(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    type: Type1 = Field(..., description="Extractor type identifier")
    priority: Optional[conint(ge=1, le=1000)] = Field(
        None, description="Execution priority (higher numbers run first)"
    )
    config: Union[
        Config29,
        Config30,
        Config31,
        Config32,
        Config33,
        Config34,
        Config35,
        Config36,
        Config37,
        Config38,
        Config39,
        Config40,
        Config41,
    ]


class Type2(Enum):
    OllamaEmbedder = "OllamaEmbedder"
    HuggingFaceEmbedder = "HuggingFaceEmbedder"
    OpenAIEmbedder = "OpenAIEmbedder"
    SentenceTransformerEmbedder = "SentenceTransformerEmbedder"


class Config42(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    model: Optional[str] = Field("nomic-embed-text", description="Ollama model name")
    base_url: Optional[AnyUrl] = Field(
        "http://localhost:11434", description="Ollama API endpoint (preferred)"
    )
    api_base: Optional[AnyUrl] = Field(
        None,
        description="Alias for base_url (deprecated - use base_url instead). If both are set, base_url takes precedence",
    )
    dimension: Optional[conint(ge=128, le=4096)] = Field(768, description="Embedding dimension")
    batch_size: Optional[conint(ge=1, le=128)] = Field(16, description="Batch processing size")
    timeout: Optional[conint(ge=10)] = Field(60, description="Request timeout (seconds)")
    auto_pull: Optional[bool] = Field(True, description="Auto-pull missing models")


class Device(Enum):
    cpu = "cpu"
    cuda = "cuda"
    mps = "mps"
    auto = "auto"


class Config43(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    model_name: Optional[str] = Field(
        "sentence-transformers/all-MiniLM-L6-v2", description="HuggingFace model ID"
    )
    device: Optional[Device] = Field("auto", description="Computation device")
    batch_size: Optional[conint(ge=1, le=256)] = Field(32, description="Batch size")
    normalize_embeddings: Optional[bool] = Field(True, description="L2 normalize embeddings")
    show_progress_bar: Optional[bool] = Field(False, description="Show progress bar")
    cache_folder: Optional[str] = Field(None, description="Model cache directory")


class Config44(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    model: Optional[str] = Field("text-embedding-3-small", description="OpenAI embedding model")
    api_key: Optional[str] = Field(None, description="OpenAI API key")
    base_url: Optional[str] = Field(None, description="OpenAI API base URL")
    organization: Optional[str] = Field(None, description="OpenAI organization ID")
    batch_size: Optional[conint(ge=1, le=2048)] = Field(100, description="Batch size for API calls")
    max_retries: Optional[conint(ge=0, le=10)] = Field(3, description="Maximum retry attempts")
    timeout: Optional[conint(ge=10)] = Field(60, description="Request timeout in seconds")


class Device1(Enum):
    cpu = "cpu"
    cuda = "cuda"
    mps = "mps"


class Config45(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    model_name: Optional[str] = Field(
        "sentence-transformers/all-MiniLM-L6-v2", description="Model name"
    )
    device: Optional[Device1] = Field("cpu", description="Computation device")


class Embedder(BaseModel):
    type: Type2
    config: Union[Config42, Config43, Config44, Config45]


class Type3(Enum):
    ChromaStore = "ChromaStore"
    FAISSStore = "FAISSStore"
    PineconeStore = "PineconeStore"
    QdrantStore = "QdrantStore"


class DistanceFunction(Enum):
    cosine = "cosine"
    l2 = "l2"
    ip = "ip"


class DistanceMetric(Enum):
    cosine = "cosine"
    l2 = "l2"
    ip = "ip"


class Config46(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    collection_name: Optional[constr(pattern=r"^[a-zA-Z0-9_-]+$")] = Field(
        "documents", description="Collection name"
    )
    persist_directory: Optional[str] = Field(
        "./data/chroma_db", description="Persistence directory"
    )
    host: Optional[str] = Field(None, description="Server host")
    port: Optional[conint(ge=1, le=65535)] = Field(8000, description="Server port")
    distance_function: Optional[DistanceFunction] = Field("cosine", description="Distance metric")
    distance_metric: Optional[DistanceMetric] = Field(
        "cosine", description="Alternative distance metric name"
    )
    embedding_dimension: Optional[conint(ge=1, le=4096)] = Field(
        768, description="Embedding dimension"
    )
    enable_deduplication: Optional[bool] = Field(True, description="Enable document deduplication")
    embedding_function: Optional[str] = Field(None, description="Built-in embedding function")


class IndexType(Enum):
    Flat = "Flat"
    IVF = "IVF"
    HNSW = "HNSW"
    LSH = "LSH"


class Metric(Enum):
    L2 = "L2"
    IP = "IP"
    Cosine = "Cosine"


class Config47(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    dimension: conint(ge=1, le=4096) = Field(..., description="Vector dimension")
    index_type: Optional[IndexType] = Field("Flat", description="Index type")
    metric: Optional[Metric] = Field("L2", description="Distance metric")
    nlist: Optional[conint(ge=1)] = Field(100, description="Number of clusters (IVF)")
    nprobe: Optional[conint(ge=1)] = Field(10, description="Clusters to search (IVF)")
    use_gpu: Optional[bool] = Field(False, description="Enable GPU acceleration")


class Metric1(Enum):
    euclidean = "euclidean"
    cosine = "cosine"
    dotproduct = "dotproduct"


class Config48(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    api_key: str = Field(..., description="Pinecone API key")
    environment: Optional[str] = Field("us-east-1-aws", description="Pinecone environment")
    index_name: constr(pattern=r"^[a-z0-9-]+$") = Field(..., description="Index name")
    dimension: conint(ge=1, le=20000) = Field(..., description="Vector dimension")
    metric: Optional[Metric1] = Field("cosine", description="Distance metric")
    namespace: Optional[str] = Field("", description="Namespace for isolation")
    replicas: Optional[conint(ge=1, le=20)] = Field(1, description="Number of replicas")


class Distance(Enum):
    Cosine = "Cosine"
    Euclid = "Euclid"
    Dot = "Dot"


class Config49(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    host: Optional[str] = Field("localhost", description="Server host")
    port: Optional[conint(ge=1, le=65535)] = Field(6333, description="Server port")
    grpc_port: Optional[conint(ge=1, le=65535)] = Field(6334, description="gRPC port")
    api_key: Optional[str] = Field(None, description="API key")
    collection_name: Optional[constr(pattern=r"^[a-zA-Z0-9_-]+$")] = Field(
        "documents", description="Collection name"
    )
    vector_size: conint(ge=1, le=65536) = Field(..., description="Vector dimension")
    distance: Optional[Distance] = Field("Cosine", description="Distance metric")
    on_disk: Optional[bool] = Field(False, description="Store vectors on disk")


class VectorStore(BaseModel):
    type: Type3
    config: Union[Config46, Config47, Config48, Config49]


class DistanceMetric1(Enum):
    cosine = "cosine"
    euclidean = "euclidean"
    manhattan = "manhattan"
    dot = "dot"


class Config50(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    top_k: Optional[conint(ge=1, le=1000)] = Field(10, description="Number of results")
    distance_metric: Optional[DistanceMetric1] = Field("cosine", description="Distance metric")
    score_threshold: Optional[confloat(ge=0.0, le=1.0)] = Field(
        None, description="Minimum similarity score"
    )


class RetrievalStrategy(BaseModel):
    type: Literal["BasicSimilarityStrategy"] = Field(
        ..., description="Retrieval strategy type identifier"
    )
    config: Config50 = Field(..., title="Basic Similarity Configuration")


class FilterMode(Enum):
    pre = "pre"
    post = "post"


class Config51(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    top_k: Optional[conint(ge=1, le=1000)] = Field(10, description="Number of results")
    filters: Optional[dict[str, Union[str, float, bool, list[Union[str, float, bool]]]]] = Field(
        {}, description="Metadata filters"
    )
    filter_mode: Optional[FilterMode] = Field("pre", description="When to apply filters")
    fallback_multiplier: Optional[conint(ge=1, le=10)] = Field(
        3, description="Multiplier for post-filtering"
    )


class RetrievalStrategy1(BaseModel):
    type: Literal["MetadataFilteredStrategy"] = Field(
        ..., description="Retrieval strategy type identifier"
    )
    config: Config51 = Field(..., title="Metadata Filtered Configuration")


class AggregationMethod(Enum):
    max = "max"
    mean = "mean"
    weighted = "weighted"
    reciprocal_rank = "reciprocal_rank"


class Config52(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    num_queries: Optional[conint(ge=1, le=10)] = Field(3, description="Number of query variations")
    top_k: Optional[conint(ge=1, le=1000)] = Field(10, description="Results per query")
    aggregation_method: Optional[AggregationMethod] = Field(
        "weighted", description="Result aggregation method"
    )
    query_weights: Optional[list[confloat(ge=0.0, le=1.0)]] = Field(
        None, description="Weights for each query"
    )


class RetrievalStrategy2(BaseModel):
    type: Literal["MultiQueryStrategy"] = Field(
        ..., description="Retrieval strategy type identifier"
    )
    config: Config52 = Field(..., title="Multi Query Configuration")


class RerankFactors(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    similarity_weight: Optional[confloat(ge=0.0, le=1.0)] = 0.7
    recency_weight: Optional[confloat(ge=0.0, le=1.0)] = 0.1
    length_weight: Optional[confloat(ge=0.0, le=1.0)] = 0.1
    metadata_weight: Optional[confloat(ge=0.0, le=1.0)] = 0.1


class Config53(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    initial_k: Optional[conint(ge=10, le=1000)] = Field(30, description="Initial candidates")
    final_k: Optional[conint(ge=1, le=100)] = Field(10, description="Final results")
    rerank_factors: Optional[RerankFactors] = Field(None, description="Reranking factor weights")
    normalize_scores: Optional[bool] = Field(True, description="Normalize scores before combining")


class RetrievalStrategy3(BaseModel):
    type: Literal["RerankedStrategy"] = Field(..., description="Retrieval strategy type identifier")
    config: Config53 = Field(..., title="Reranked Configuration")


class Type4(Enum):
    BasicSimilarityStrategy = "BasicSimilarityStrategy"
    MetadataFilteredStrategy = "MetadataFilteredStrategy"
    MultiQueryStrategy = "MultiQueryStrategy"
    RerankedStrategy = "RerankedStrategy"


class Strategy1(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    type: Type4
    weight: Optional[confloat(ge=0.0, le=1.0)] = 1.0
    config: Optional[dict[str, Any]] = None


class CombinationMethod(Enum):
    weighted_average = "weighted_average"
    rank_fusion = "rank_fusion"
    score_fusion = "score_fusion"


class Config54(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    strategies: Optional[list[Strategy1]] = Field(
        None, description="Sub-strategies to combine", max_length=5, min_length=2
    )
    combination_method: Optional[CombinationMethod] = Field(
        "weighted_average", description="Combination method"
    )
    final_k: Optional[conint(ge=1, le=1000)] = Field(10, description="Final number of results")


class RetrievalStrategy4(BaseModel):
    type: Literal["HybridUniversalStrategy"] = Field(
        ..., description="Retrieval strategy type identifier"
    )
    config: Config54 = Field(..., title="Hybrid Universal Configuration")


class Components(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    parser: Parser = Field(..., description="Parser configuration", title="Parser Configuration")
    extractors: Optional[list[Extractor]] = Field(None, max_length=10)
    embedder: Embedder = Field(..., title="Embedder Configuration")
    vector_store: VectorStore = Field(..., title="Vector Store Configuration")
    retrieval_strategy: Union[
        RetrievalStrategy,
        RetrievalStrategy1,
        RetrievalStrategy2,
        RetrievalStrategy3,
        RetrievalStrategy4,
    ] = Field(..., title="Retrieval Strategy Configuration")


class Strategy(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    name: constr(pattern=r"^[a-z][a-z0-9_]*$", min_length=1, max_length=50) = Field(
        ...,
        description="Unique strategy identifier",
        examples=["simple", "customer_support", "legal_documents"],
    )
    description: constr(min_length=10, max_length=500) = Field(
        ..., description="Clear description of strategy purpose"
    )
    tags: Optional[list[Tag]] = Field(
        None,
        description="Tags for categorization",
        examples=[["production", "optimized"], ["development", "testing"]],
    )
    use_cases: Optional[list[UseCase]] = Field(None, description="Specific use cases", min_length=1)
    components: Components = Field(..., title="Strategy Components")
    optimization: Optional[dict[str, Any]] = Field(
        None,
        description="Performance and resource optimization settings",
        title="Optimization Settings",
    )
    validation: Optional[dict[str, Any]] = Field(
        None, description="Data validation and quality rules", title="Validation Rules"
    )
    monitoring: Optional[dict[str, Any]] = Field(
        None, description="Monitoring and metrics settings", title="Monitoring Configuration"
    )


class Type5(Enum):
    PDFParser_PyPDF2 = "PDFParser_PyPDF2"
    PDFParser_LlamaIndex = "PDFParser_LlamaIndex"
    CSVParser_Pandas = "CSVParser_Pandas"
    CSVParser_Python = "CSVParser_Python"
    CSVParser_LlamaIndex = "CSVParser_LlamaIndex"
    ExcelParser_OpenPyXL = "ExcelParser_OpenPyXL"
    ExcelParser_Pandas = "ExcelParser_Pandas"
    ExcelParser_LlamaIndex = "ExcelParser_LlamaIndex"
    DocxParser_PythonDocx = "DocxParser_PythonDocx"
    DocxParser_LlamaIndex = "DocxParser_LlamaIndex"
    MarkdownParser_Python = "MarkdownParser_Python"
    MarkdownParser_LlamaIndex = "MarkdownParser_LlamaIndex"
    TextParser_Python = "TextParser_Python"
    TextParser_LlamaIndex = "TextParser_LlamaIndex"


class ChunkStrategy21(Enum):
    paragraphs = "paragraphs"
    sentences = "sentences"
    characters = "characters"


class Config55(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        1000, description="Chunk size in characters"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        100, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy21] = Field(
        "paragraphs", description="Chunking strategy using PyPDF2 text structure"
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract PDF metadata using PyPDF2")
    preserve_layout: Optional[bool] = Field(
        True, description="Use PyPDF2 layout-preserving extraction mode"
    )
    extract_page_info: Optional[bool] = Field(
        True, description="Extract page numbers and rotation info"
    )
    extract_annotations: Optional[bool] = Field(
        False, description="Extract PDF annotations using PyPDF2"
    )
    extract_links: Optional[bool] = Field(False, description="Extract hyperlinks")
    extract_form_fields: Optional[bool] = Field(
        False, description="Extract form fields using PyPDF2"
    )
    extract_outlines: Optional[bool] = Field(
        False, description="Extract document outlines/bookmarks"
    )
    extract_images: Optional[bool] = Field(
        False, description="Extract embedded images using PyPDF2"
    )
    extract_xmp_metadata: Optional[bool] = Field(
        False, description="Extract XMP metadata using PyPDF2"
    )
    clean_text: Optional[bool] = Field(True, description="Clean extracted text")


class ChunkStrategy22(Enum):
    sentences = "sentences"
    paragraphs = "paragraphs"
    pages = "pages"
    semantic = "semantic"


class Config56(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        1000, description="Chunk size in characters"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        100, description="Overlap between chunks"
    )
    chunk_strategy: Optional[ChunkStrategy22] = Field(
        "sentences", description="Chunking strategy for PDF content"
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract PDF metadata")
    extract_images: Optional[bool] = Field(False, description="Extract images from PDF")
    extract_tables: Optional[bool] = Field(True, description="Extract tables from PDF")
    fallback_strategies: Optional[list[FallbackStrategy]] = Field(
        ["llama_pdf_reader", "llama_pymupdf_reader", "direct_pymupdf", "pypdf2_fallback"],
        description="Fallback strategies to try in order",
    )


class ChunkStrategy23(Enum):
    rows = "rows"
    columns = "columns"
    full = "full"


class Config57(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100)] = Field(1000, description="Number of rows per chunk")
    chunk_strategy: Optional[ChunkStrategy23] = Field(
        "rows", description="How to chunk the CSV data"
    )
    extract_metadata: Optional[bool] = Field(
        True, description="Extract data statistics and metadata"
    )
    encoding: Optional[str] = Field("utf-8", description="File encoding")
    delimiter: Optional[str] = Field(",", description="CSV delimiter")
    na_values: Optional[list[str]] = Field(
        ["", "NA", "N/A", "null", "None"], description="Values to treat as NaN"
    )


class Config58(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100)] = Field(1000, description="Number of rows per chunk")
    encoding: Optional[str] = Field("utf-8", description="File encoding")
    delimiter: Optional[str] = Field(",", description="CSV delimiter")
    quotechar: Optional[str] = Field('"', description="Quote character")


class ChunkStrategy24(Enum):
    rows = "rows"
    semantic = "semantic"
    full = "full"


class Config59(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        1000, description="Number of rows per chunk"
    )
    chunk_strategy: Optional[ChunkStrategy24] = Field("rows", description="Chunking strategy")
    field_mapping: Optional[dict[str, str]] = Field(
        None, description="Map CSV columns to standard fields"
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract metadata from CSV")
    combine_fields: Optional[bool] = Field(True, description="Combine fields into text content")
    skiprows: Optional[conint(ge=0)] = Field(
        None, description="Number of rows to skip at beginning"
    )
    na_values: Optional[list[str]] = Field(
        ["", "NA", "N/A", "null", "None"], description="Values to treat as missing"
    )


class Config60(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100)] = Field(1000, description="Number of rows per chunk")
    extract_formulas: Optional[bool] = Field(
        False, description="Extract cell formulas using OpenPyXL"
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract workbook metadata")
    sheets: Optional[list[str]] = Field(None, description="Specific sheets to process (null = all)")
    data_only: Optional[bool] = Field(True, description="Extract values instead of formulas")


class Config61(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100)] = Field(1000, description="Number of rows per chunk")
    sheets: Optional[list[str]] = Field(None, description="Specific sheets to process (null = all)")
    extract_metadata: Optional[bool] = Field(True, description="Extract data statistics")
    skiprows: Optional[int] = Field(None, description="Rows to skip at beginning")
    na_values: Optional[list[str]] = Field(
        ["", "NA", "N/A", "null", "None"], description="Values to treat as NaN"
    )


class Config62(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        1000, description="Number of rows per chunk"
    )
    chunk_strategy: Optional[ChunkStrategy24] = Field("rows", description="Chunking strategy")
    sheets: Optional[list[str]] = Field(None, description="Specific sheets to parse (null for all)")
    combine_sheets: Optional[bool] = Field(
        False, description="Combine all sheets into one document"
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract metadata from Excel")
    extract_formulas: Optional[bool] = Field(
        False, description="Extract formulas instead of values"
    )
    header_row: Optional[conint(ge=0)] = Field(0, description="Row index for headers")
    skiprows: Optional[conint(ge=0)] = Field(None, description="Number of rows to skip")
    na_values: Optional[list[str]] = Field(
        ["", "NA", "N/A", "null", "None"], description="Values to treat as missing"
    )


class ChunkStrategy26(Enum):
    paragraphs = "paragraphs"
    sentences = "sentences"
    characters = "characters"


class Config63(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100)] = Field(1000, description="Chunk size in characters")
    chunk_strategy: Optional[ChunkStrategy26] = Field("paragraphs", description="Chunking strategy")
    extract_metadata: Optional[bool] = Field(True, description="Extract document metadata")
    extract_tables: Optional[bool] = Field(True, description="Extract tables using python-docx")
    extract_headers: Optional[bool] = Field(True, description="Extract headers")
    extract_footers: Optional[bool] = Field(False, description="Extract footers")
    extract_comments: Optional[bool] = Field(False, description="Extract comments")


class ChunkStrategy27(Enum):
    paragraphs = "paragraphs"
    sentences = "sentences"
    semantic = "semantic"


class Config64(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        1000, description="Chunk size in characters"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        100, description="Overlap between chunks"
    )
    chunk_strategy: Optional[ChunkStrategy27] = Field("paragraphs", description="Chunking strategy")
    extract_metadata: Optional[bool] = Field(True, description="Extract document metadata")
    extract_tables: Optional[bool] = Field(True, description="Extract tables from document")
    extract_images: Optional[bool] = Field(False, description="Extract images from document")
    preserve_formatting: Optional[bool] = Field(True, description="Preserve text formatting")
    include_header_footer: Optional[bool] = Field(
        False, description="Include header and footer content"
    )


class ChunkStrategy28(Enum):
    sections = "sections"
    paragraphs = "paragraphs"
    characters = "characters"


class Config65(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100)] = Field(1000, description="Chunk size in characters")
    chunk_strategy: Optional[ChunkStrategy28] = Field(
        "sections", description="Chunking strategy - sections uses markdown headers"
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract YAML frontmatter")
    extract_code_blocks: Optional[bool] = Field(True, description="Extract code blocks")
    extract_links: Optional[bool] = Field(True, description="Extract markdown links")


class ChunkStrategy29(Enum):
    headings = "headings"
    paragraphs = "paragraphs"
    sentences = "sentences"
    semantic = "semantic"


class Config66(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        1000, description="Chunk size in characters"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        100, description="Overlap between chunks"
    )
    chunk_strategy: Optional[ChunkStrategy29] = Field(
        "headings", description="Chunking strategy for markdown"
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract frontmatter metadata")
    extract_code_blocks: Optional[bool] = Field(True, description="Extract code blocks separately")
    extract_tables: Optional[bool] = Field(True, description="Extract markdown tables")
    extract_links: Optional[bool] = Field(True, description="Extract links and references")
    preserve_structure: Optional[bool] = Field(True, description="Preserve heading hierarchy")


class ChunkStrategy30(Enum):
    sentences = "sentences"
    paragraphs = "paragraphs"
    characters = "characters"


class Config67(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100)] = Field(1000, description="Chunk size in characters")
    chunk_overlap: Optional[conint(ge=0)] = Field(100, description="Overlap between chunks")
    chunk_strategy: Optional[ChunkStrategy30] = Field(
        "sentences", description="Text chunking strategy"
    )
    encoding: Optional[str] = Field("utf-8", description="Text encoding (utf-8 or auto-detect)")
    clean_text: Optional[bool] = Field(True, description="Remove excessive whitespace")
    extract_metadata: Optional[bool] = Field(True, description="Extract file statistics")


class ChunkStrategy31(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"
    tokens = "tokens"
    semantic = "semantic"
    code = "code"


class Config68(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        1000, description="Chunk size in characters"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        100, description="Overlap between chunks"
    )
    chunk_strategy: Optional[ChunkStrategy31] = Field(
        "semantic",
        description="Advanced chunking strategy - semantic uses content-based splitting, code preserves syntax",
    )
    encoding: Optional[str] = Field("utf-8", description="Text encoding")
    clean_text: Optional[bool] = Field(True, description="Clean extracted text")
    extract_metadata: Optional[bool] = Field(
        True, description="Extract comprehensive file and content metadata"
    )
    semantic_buffer_size: Optional[conint(ge=1, le=10)] = Field(
        1, description="Buffer size for semantic chunking"
    )
    semantic_breakpoint_percentile_threshold: Optional[conint(ge=50, le=99)] = Field(
        95, description="Percentile threshold for semantic breakpoints"
    )
    token_model: Optional[str] = Field(
        "gpt-3.5-turbo", description="Tokenizer model for token-based chunking"
    )
    preserve_code_structure: Optional[bool] = Field(
        True, description="Preserve code syntax and structure when parsing code files"
    )
    detect_language: Optional[bool] = Field(
        True, description="Automatically detect programming language for code files"
    )
    include_prev_next_rel: Optional[bool] = Field(
        True, description="Include relationships between chunks for better context"
    )


class Config69(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    recursive: Optional[bool] = Field(True, description="Recursively parse subdirectories")
    include_patterns: Optional[list[str]] = Field(None, description="File patterns to include")
    exclude_patterns: Optional[list[str]] = Field(None, description="File patterns to exclude")
    parser_map: Optional[dict[str, str]] = Field(
        None, description="Mapping of file extensions to parser types"
    )
    parser_configs: Optional[dict[str, dict[str, Any]]] = Field(
        None, description="Configuration for each parser type"
    )


class Config70(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    content_fields: Optional[list[str]] = Field(
        ["subject", "body"], description="CSV columns to use as document content", min_length=1
    )
    metadata_fields: Optional[list[str]] = Field(
        [], description="CSV columns to include as metadata"
    )
    id_field: Optional[str] = Field(None, description="CSV column to use as document ID")
    combine_content: Optional[bool] = Field(
        True, description="Combine multiple content fields into one document"
    )
    content_separator: Optional[str] = Field(
        "\n\n", description="Separator when combining content fields"
    )
    priority_mapping: Optional[dict[str, Union[str, float]]] = Field(
        None,
        description="Mapping for priority field values",
        examples=[{"Critical": 1, "High": 2, "Medium": 3, "Low": 4}],
    )


class ChunkStrategy32(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"
    pages = "pages"


class Config71(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract PDF metadata")
    extract_page_structure: Optional[bool] = Field(True, description="Extract page-level structure")
    combine_pages: Optional[bool] = Field(
        True, description="Combine all pages into single document"
    )
    page_separator: Optional[str] = Field(
        "\n\n--- Page Break ---\n\n", description="Separator between pages"
    )
    min_text_length: Optional[conint(ge=0)] = Field(
        10, description="Minimum text length to include page"
    )
    include_page_numbers: Optional[bool] = Field(True, description="Include page numbers in text")
    extract_outline: Optional[bool] = Field(True, description="Extract document outline")
    extract_images: Optional[bool] = Field(False, description="Extract embedded images")
    ocr_enabled: Optional[bool] = Field(False, description="Enable OCR for scanned documents")
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy32] = Field(
        "characters", description="Chunking strategy - pages splits at PDF page boundaries"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle when using character chunking"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        False,
        description="Avoid splitting paragraphs in the middle (overrides sentence boundaries)",
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        50, description="Minimum chunk size to avoid creating tiny chunks"
    )


class ChunkStrategy33(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"
    headings = "headings"


class Config72(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract YAML frontmatter")
    extract_headings: Optional[bool] = Field(True, description="Extract heading structure")
    extract_links: Optional[bool] = Field(True, description="Extract all links")
    extract_code_blocks: Optional[bool] = Field(True, description="Extract code blocks")
    chunk_by_headings: Optional[bool] = Field(False, description="Split by headings")
    preserve_formatting: Optional[bool] = Field(False, description="Preserve Markdown formatting")
    heading_level_split: Optional[conint(ge=1, le=6)] = Field(
        2, description="Heading level for splitting"
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None,
        description="Chunk size in characters (null for no chunking, overrides chunk_by_headings)",
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy33] = Field(
        "headings", description="Chunking strategy - headings preserves markdown structure"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle when using character chunking"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting paragraphs in the middle"
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        100, description="Minimum chunk size to avoid creating tiny chunks"
    )


class ChunkStrategy34(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"
    elements = "elements"


class Config73(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract meta tags")
    extract_links: Optional[bool] = Field(True, description="Extract hyperlinks")
    extract_images: Optional[bool] = Field(True, description="Extract image sources")
    preserve_structure: Optional[bool] = Field(False, description="Preserve HTML structure")
    remove_scripts: Optional[bool] = Field(True, description="Remove JavaScript")
    remove_styles: Optional[bool] = Field(True, description="Remove CSS")
    text_only: Optional[bool] = Field(False, description="Extract only text")
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy34] = Field(
        "elements", description="Chunking strategy - elements uses HTML semantic structure"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle when using character chunking"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting paragraphs in the middle"
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        100, description="Minimum chunk size to avoid creating tiny chunks"
    )


class ChunkStrategy35(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"
    sections = "sections"


class Config74(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract document properties")
    extract_headers_footers: Optional[bool] = Field(True, description="Include headers and footers")
    extract_comments: Optional[bool] = Field(True, description="Extract comments")
    extract_tables: Optional[bool] = Field(True, description="Extract tables")
    extract_images: Optional[bool] = Field(False, description="Extract images")
    preserve_formatting: Optional[bool] = Field(False, description="Preserve text formatting")
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy35] = Field(
        "paragraphs", description="Chunking strategy - sections uses Word's built-in structure"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle when using character chunking"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting paragraphs in the middle"
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        100, description="Minimum chunk size to avoid creating tiny chunks"
    )


class Config75(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    sheet_names: Optional[list[str]] = Field(None, description="Specific sheets to parse")
    combine_sheets: Optional[bool] = Field(False, description="Combine all sheets")
    extract_formulas: Optional[bool] = Field(False, description="Extract cell formulas")
    extract_charts: Optional[bool] = Field(False, description="Extract chart metadata")
    table_format: Optional[TableFormat] = Field("markdown", description="Table output format")
    header_row: Optional[conint(ge=0)] = Field(0, description="Header row index")


class ChunkStrategy36(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"


class Config76(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    encoding: Optional[str] = Field(
        "auto", description="Text encoding (auto-detect or specify like utf-8, latin1, etc.)"
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy36] = Field(
        "characters", description="Chunking strategy - how to split the text"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle when using character chunking"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        False,
        description="Avoid splitting paragraphs in the middle (overrides sentence boundaries)",
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        50, description="Minimum chunk size to avoid creating tiny chunks"
    )
    preserve_line_breaks: Optional[bool] = Field(
        True, description="Preserve line breaks in the text"
    )
    strip_empty_lines: Optional[bool] = Field(
        True, description="Remove empty lines while preserving paragraph structure"
    )
    detect_structure: Optional[bool] = Field(
        True, description="Detect and extract structural elements (headers, lists, code blocks)"
    )
    split_by: Optional[SplitBy] = Field("characters", description="Splitting method")
    preserve_whitespace: Optional[bool] = Field(False, description="Preserve whitespace")


class Config77(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    fallback_chain: Optional[list[str]] = Field(
        ["PlainTextParser", "MarkdownParser"], description="Fallback parsers to try in order"
    )
    content_detection_enabled: Optional[bool] = Field(
        True, description="Enable content-based file type detection"
    )
    max_file_size_mb: Optional[conint(ge=1, le=1000)] = Field(
        100, description="Maximum file size in MB to process"
    )
    enable_magic: Optional[bool] = Field(
        True, description="Enable python-magic for file type detection"
    )


class ChunkStrategy37(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"
    tokens = "tokens"


class Config78(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    encoding: Optional[str] = Field("auto", description="Text encoding (auto-detect or specify)")
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy37] = Field(
        "characters", description="Chunking strategy for LlamaIndex"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        False, description="Avoid splitting paragraphs in the middle"
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        50, description="Minimum chunk size to avoid creating tiny chunks"
    )
    preserve_line_breaks: Optional[bool] = Field(
        True, description="Preserve line breaks in the text"
    )
    strip_empty_lines: Optional[bool] = Field(
        True, description="Remove empty lines while preserving paragraph structure"
    )
    detect_structure: Optional[bool] = Field(
        True, description="Detect and extract structural elements"
    )


class Config79(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract PDF metadata")
    extract_page_structure: Optional[bool] = Field(True, description="Extract page-level structure")
    combine_pages: Optional[bool] = Field(
        True, description="Combine all pages into single document"
    )
    page_separator: Optional[str] = Field(
        "\n\n--- Page Break ---\n\n", description="Separator between pages"
    )
    min_text_length: Optional[conint(ge=0)] = Field(
        10, description="Minimum text length to include page"
    )
    include_page_numbers: Optional[bool] = Field(True, description="Include page numbers in text")
    extract_outline: Optional[bool] = Field(True, description="Extract document outline")
    fallback_strategies: Optional[list[FallbackStrategy]] = Field(
        ["llama_pdf_reader", "llama_pymupdf_reader", "direct_pymupdf", "pypdf2_fallback"],
        description="PDF parsing strategies to try in order",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy37] = Field(
        "characters", description="Chunking strategy for LlamaIndex"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        False, description="Avoid splitting paragraphs in the middle"
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        50, description="Minimum chunk size to avoid creating tiny chunks"
    )


class Config80(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    content_fields: Optional[list[str]] = Field(
        ["subject", "body", "content", "description"],
        description="Columns to use as document content",
    )
    metadata_fields: Optional[list[str]] = Field([], description="Columns to include as metadata")
    id_field: Optional[str] = Field(None, description="Column to use as document ID")
    combine_content: Optional[bool] = Field(
        True, description="Combine multiple content fields into one document"
    )
    content_separator: Optional[str] = Field(
        "\n\n", description="Separator when combining content fields"
    )
    priority_mapping: Optional[dict[str, Union[str, float]]] = Field(
        None, description="Mapping for priority field values"
    )
    table_format: Optional[TableFormat] = Field(
        "markdown", description="Table output format for Excel"
    )
    sheet_names: Optional[list[str]] = Field(
        None, description="Specific sheets to parse (Excel only)"
    )
    combine_sheets: Optional[bool] = Field(False, description="Combine all sheets (Excel only)")
    header_row: Optional[conint(ge=0)] = Field(0, description="Header row index (Excel only)")


class Config81(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract document properties")
    extract_headers_footers: Optional[bool] = Field(True, description="Include headers and footers")
    extract_comments: Optional[bool] = Field(False, description="Extract comments")
    extract_tables: Optional[bool] = Field(True, description="Extract tables")
    extract_images: Optional[bool] = Field(False, description="Extract images")
    preserve_formatting: Optional[bool] = Field(False, description="Preserve text formatting")
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy37] = Field(
        "paragraphs", description="Chunking strategy for LlamaIndex"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting paragraphs in the middle"
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        100, description="Minimum chunk size to avoid creating tiny chunks"
    )


class ChunkStrategy40(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"
    headings = "headings"
    tokens = "tokens"


class Config82(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract YAML frontmatter")
    extract_headings: Optional[bool] = Field(True, description="Extract heading structure")
    extract_links: Optional[bool] = Field(True, description="Extract all links")
    extract_code_blocks: Optional[bool] = Field(True, description="Extract code blocks")
    chunk_by_headings: Optional[bool] = Field(False, description="Split by headings")
    preserve_formatting: Optional[bool] = Field(False, description="Preserve Markdown formatting")
    heading_level_split: Optional[conint(ge=1, le=6)] = Field(
        2, description="Heading level for splitting"
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy40] = Field(
        "headings", description="Chunking strategy - headings preserves markdown structure"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting paragraphs in the middle"
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        100, description="Minimum chunk size to avoid creating tiny chunks"
    )


class ChunkStrategy41(Enum):
    characters = "characters"
    sentences = "sentences"
    paragraphs = "paragraphs"
    elements = "elements"
    tokens = "tokens"


class Config83(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_metadata: Optional[bool] = Field(True, description="Extract meta tags")
    extract_links: Optional[bool] = Field(True, description="Extract hyperlinks")
    extract_images: Optional[bool] = Field(True, description="Extract image sources")
    preserve_structure: Optional[bool] = Field(False, description="Preserve HTML structure")
    remove_scripts: Optional[bool] = Field(True, description="Remove JavaScript")
    remove_styles: Optional[bool] = Field(True, description="Remove CSS")
    text_only: Optional[bool] = Field(False, description="Extract only text")
    request_timeout: Optional[conint(ge=5, le=300)] = Field(
        30, description="Request timeout for URLs in seconds"
    )
    user_agent: Optional[str] = Field(
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        description="User agent for web requests",
    )
    chunk_size: Optional[conint(ge=100, le=50000)] = Field(
        None, description="Chunk size in characters (null for no chunking)"
    )
    chunk_overlap: Optional[conint(ge=0, le=5000)] = Field(
        0, description="Overlap between chunks in characters"
    )
    chunk_strategy: Optional[ChunkStrategy41] = Field(
        "elements", description="Chunking strategy - elements uses HTML semantic structure"
    )
    respect_sentence_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting sentences in the middle"
    )
    respect_paragraph_boundaries: Optional[bool] = Field(
        True, description="Avoid splitting paragraphs in the middle"
    )
    min_chunk_size: Optional[conint(ge=10, le=1000)] = Field(
        100, description="Minimum chunk size to avoid creating tiny chunks"
    )


class Parser1(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    type: Type5
    config: Union[
        Config55,
        Config56,
        Config57,
        Config58,
        Config59,
        Config60,
        Config61,
        Config62,
        Config63,
        Config64,
        Config65,
        Config66,
        Config67,
        Config68,
        Config69,
        Config70,
        Config71,
        Config72,
        Config73,
        Config74,
        Config75,
        Config76,
        Config77,
        Config78,
        Config79,
        Config80,
        Config81,
        Config82,
        Config83,
    ]


class Type6(Enum):
    KeywordExtractor = "KeywordExtractor"
    EntityExtractor = "EntityExtractor"
    DateTimeExtractor = "DateTimeExtractor"
    HeadingExtractor = "HeadingExtractor"
    LinkExtractor = "LinkExtractor"
    PathExtractor = "PathExtractor"
    PatternExtractor = "PatternExtractor"
    StatisticsExtractor = "StatisticsExtractor"
    SummaryExtractor = "SummaryExtractor"
    TableExtractor = "TableExtractor"
    SentimentExtractor = "SentimentExtractor"
    YAKEExtractor = "YAKEExtractor"
    ContentStatisticsExtractor = "ContentStatisticsExtractor"


class Algorithm2(Enum):
    rake = "rake"
    yake = "yake"
    tfidf = "tfidf"
    textrank = "textrank"


class Config84(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extractor_type: Literal["keyword"] = Field(
        "keyword", description="Extractor type discriminator"
    )
    algorithm: Optional[Algorithm2] = Field("rake", description="Extraction algorithm")
    max_keywords: Optional[conint(ge=1, le=100)] = Field(
        10, description="Maximum keywords to extract"
    )
    min_length: Optional[conint(ge=1)] = Field(1, description="Minimum word length for keywords")
    max_length: Optional[conint(ge=1)] = Field(4, description="Maximum word length for keywords")
    min_frequency: Optional[conint(ge=1)] = Field(1, description="Minimum frequency for keywords")
    stop_words: Optional[list[str]] = Field(None, description="Custom stop words")
    language: Optional[str] = Field("en", description="Language for YAKE algorithm")
    max_ngram_size: Optional[conint(ge=1, le=5)] = Field(
        3, description="Maximum n-gram size for YAKE"
    )
    deduplication_threshold: Optional[confloat(ge=0.0, le=1.0)] = Field(
        0.9, description="Deduplication threshold for YAKE"
    )


class Config85(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    model: Optional[str] = Field("en_core_web_sm", description="NER model name")
    entity_types: Optional[list[EntityType]] = Field(
        [
            "PERSON",
            "ORG",
            "GPE",
            "DATE",
            "TIME",
            "MONEY",
            "EMAIL",
            "PHONE",
            "URL",
            "PERCENT",
            "PRODUCT",
            "EVENT",
        ],
        description="Entity types to extract",
    )
    use_fallback: Optional[bool] = Field(True, description="Use regex fallback")
    min_entity_length: Optional[conint(ge=1)] = Field(2, description="Minimum entity length")
    merge_entities: Optional[bool] = Field(True, description="Merge adjacent entities")
    confidence_threshold: Optional[confloat(ge=0.0, le=1.0)] = Field(
        0.7, description="Minimum confidence score"
    )


class Config86(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    fuzzy_parsing: Optional[bool] = Field(True, description="Enable fuzzy parsing")
    extract_relative: Optional[bool] = Field(True, description="Extract relative dates")
    extract_times: Optional[bool] = Field(True, description="Extract time expressions")
    extract_durations: Optional[bool] = Field(True, description="Extract durations")
    default_timezone: Optional[str] = Field("UTC", description="Default timezone")
    date_format: Optional[str] = Field("ISO", description="Output date format")
    prefer_dates_from: Optional[PreferDatesFrom] = Field(
        "current", description="Preference for ambiguous dates"
    )


class Config87(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    max_level: Optional[conint(ge=1, le=6)] = Field(6, description="Maximum heading level")
    include_hierarchy: Optional[bool] = Field(True, description="Include hierarchy structure")
    extract_outline: Optional[bool] = Field(True, description="Generate document outline")
    min_heading_length: Optional[conint(ge=1)] = Field(3, description="Minimum heading length")
    enabled: Optional[bool] = Field(True, description="Enable this extractor")


class Config88(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_urls: Optional[bool] = Field(True, description="Extract URLs")
    extract_emails: Optional[bool] = Field(True, description="Extract email addresses")
    extract_domains: Optional[bool] = Field(True, description="Extract unique domains")
    validate_urls: Optional[bool] = Field(False, description="Validate URL format")
    resolve_redirects: Optional[bool] = Field(False, description="Resolve URL redirects")
    enabled: Optional[bool] = Field(True, description="Enable this extractor")


class Config89(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extract_file_paths: Optional[bool] = Field(True, description="Extract file paths")
    extract_urls: Optional[bool] = Field(True, description="Extract URL paths")
    extract_s3_paths: Optional[bool] = Field(True, description="Extract S3 paths")
    validate_paths: Optional[bool] = Field(False, description="Validate path existence")
    normalize_paths: Optional[bool] = Field(True, description="Normalize path formats")
    enabled: Optional[bool] = Field(True, description="Enable this extractor")


class Config90(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    predefined_patterns: Optional[list[PredefinedPattern]] = Field(
        [],
        description="Use built-in patterns for common data types (e.g., email, phone). Takes precedence over 'patterns' field for matching pattern names",
    )
    custom_patterns: Optional[list[CustomPattern]] = Field([], description="Custom regex patterns")
    case_sensitive: Optional[bool] = Field(False, description="Case-sensitive matching")
    return_positions: Optional[bool] = Field(False, description="Return match positions")
    include_context: Optional[bool] = Field(
        False, description="Include surrounding context in results"
    )
    patterns: Optional[list[Pattern]] = Field([], description="Custom patterns to match")
    max_matches_per_pattern: Optional[conint(ge=1)] = Field(
        100, description="Maximum matches per pattern"
    )
    deduplicate_matches: Optional[bool] = Field(True, description="Remove duplicate matches")


class Config91(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    include_readability: Optional[bool] = Field(True, description="Calculate readability scores")
    include_vocabulary: Optional[bool] = Field(True, description="Analyze vocabulary")
    include_structure: Optional[bool] = Field(True, description="Analyze text structure")
    include_sentiment: Optional[bool] = Field(False, description="Basic sentiment analysis")
    include_sentiment_indicators: Optional[bool] = Field(
        False, description="Include detailed sentiment indicators"
    )
    include_language: Optional[bool] = Field(True, description="Detect language")


class Algorithm3(Enum):
    textrank = "textrank"
    lsa = "lsa"
    luhn = "luhn"
    lexrank = "lexrank"


class Config92(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    summary_sentences: Optional[conint(ge=1, le=10)] = Field(
        3, description="Number of summary sentences"
    )
    algorithm: Optional[Algorithm3] = Field("textrank", description="Summarization algorithm")
    include_key_phrases: Optional[bool] = Field(True, description="Extract key phrases")
    include_statistics: Optional[bool] = Field(True, description="Include text statistics")
    min_sentence_length: Optional[conint(ge=1)] = Field(
        10, description="Minimum sentence length for summary"
    )
    max_sentence_length: Optional[conint(ge=10)] = Field(
        500, description="Maximum sentence length for summary"
    )


class Config93(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    output_format: Optional[OutputFormat] = Field("dict", description="Output format")
    extract_headers: Optional[bool] = Field(True, description="Extract table headers")
    merge_cells: Optional[bool] = Field(True, description="Handle merged cells")
    min_rows: Optional[conint(ge=1)] = Field(2, description="Minimum rows for table")
    enabled: Optional[bool] = Field(True, description="Enable this extractor")


class Config94(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    analyze_business_tone: Optional[bool] = Field(
        False, description="Analyze business-specific tone"
    )
    extract_confidence: Optional[bool] = Field(True, description="Extract confidence scores")
    categories: Optional[list[Category]] = Field(
        ["positive", "negative", "neutral"], description="Sentiment categories to analyze"
    )


class Config95(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    extractor_type: Literal["yake"] = Field("yake", description="Extractor type discriminator")
    max_keywords: Optional[conint(ge=1, le=100)] = Field(
        10, description="Maximum keywords to extract"
    )
    language: Optional[str] = Field("en", description="Language for YAKE algorithm")
    max_ngram_size: Optional[conint(ge=1, le=5)] = Field(
        3, description="Maximum n-gram size for YAKE"
    )
    deduplication_threshold: Optional[confloat(ge=0.0, le=1.0)] = Field(
        0.9, description="Deduplication threshold for YAKE"
    )


class Config96(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    include_readability: Optional[bool] = Field(True, description="Calculate readability scores")
    include_vocabulary: Optional[bool] = Field(True, description="Analyze vocabulary")
    include_structure: Optional[bool] = Field(True, description="Analyze text structure")
    include_sentiment_indicators: Optional[bool] = Field(
        False, description="Include detailed sentiment indicators"
    )


class Extractor1(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    type: Type6 = Field(..., description="Extractor type identifier")
    priority: Optional[conint(ge=1, le=1000)] = Field(
        None, description="Execution priority (higher numbers run first)"
    )
    config: Union[
        Config84,
        Config85,
        Config86,
        Config87,
        Config88,
        Config89,
        Config90,
        Config91,
        Config92,
        Config93,
        Config94,
        Config95,
        Config96,
    ]


class Type7(Enum):
    OllamaEmbedder = "OllamaEmbedder"
    HuggingFaceEmbedder = "HuggingFaceEmbedder"
    OpenAIEmbedder = "OpenAIEmbedder"
    SentenceTransformerEmbedder = "SentenceTransformerEmbedder"


class Config97(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    model: Optional[str] = Field("nomic-embed-text", description="Ollama model name")
    base_url: Optional[AnyUrl] = Field(
        "http://localhost:11434", description="Ollama API endpoint (preferred)"
    )
    api_base: Optional[AnyUrl] = Field(
        None,
        description="Alias for base_url (deprecated - use base_url instead). If both are set, base_url takes precedence",
    )
    dimension: Optional[conint(ge=128, le=4096)] = Field(768, description="Embedding dimension")
    batch_size: Optional[conint(ge=1, le=128)] = Field(16, description="Batch processing size")
    timeout: Optional[conint(ge=10)] = Field(60, description="Request timeout (seconds)")
    auto_pull: Optional[bool] = Field(True, description="Auto-pull missing models")


class Device2(Enum):
    cpu = "cpu"
    cuda = "cuda"
    mps = "mps"
    auto = "auto"


class Config98(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    model_name: Optional[str] = Field(
        "sentence-transformers/all-MiniLM-L6-v2", description="HuggingFace model ID"
    )
    device: Optional[Device2] = Field("auto", description="Computation device")
    batch_size: Optional[conint(ge=1, le=256)] = Field(32, description="Batch size")
    normalize_embeddings: Optional[bool] = Field(True, description="L2 normalize embeddings")
    show_progress_bar: Optional[bool] = Field(False, description="Show progress bar")
    cache_folder: Optional[str] = Field(None, description="Model cache directory")


class Config99(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    model: Optional[str] = Field("text-embedding-3-small", description="OpenAI embedding model")
    api_key: Optional[str] = Field(None, description="OpenAI API key")
    base_url: Optional[str] = Field(None, description="OpenAI API base URL")
    organization: Optional[str] = Field(None, description="OpenAI organization ID")
    batch_size: Optional[conint(ge=1, le=2048)] = Field(100, description="Batch size for API calls")
    max_retries: Optional[conint(ge=0, le=10)] = Field(3, description="Maximum retry attempts")
    timeout: Optional[conint(ge=10)] = Field(60, description="Request timeout in seconds")


class Device3(Enum):
    cpu = "cpu"
    cuda = "cuda"
    mps = "mps"


class Config100(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    model_name: Optional[str] = Field(
        "sentence-transformers/all-MiniLM-L6-v2", description="Model name"
    )
    device: Optional[Device3] = Field("cpu", description="Computation device")


class Embedder1(BaseModel):
    type: Type7
    config: Union[Config97, Config98, Config99, Config100]


class Type8(Enum):
    ChromaStore = "ChromaStore"
    FAISSStore = "FAISSStore"
    PineconeStore = "PineconeStore"
    QdrantStore = "QdrantStore"


class DistanceMetric2(Enum):
    cosine = "cosine"
    l2 = "l2"
    ip = "ip"


class Config101(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    collection_name: Optional[constr(pattern=r"^[a-zA-Z0-9_-]+$")] = Field(
        "documents", description="Collection name"
    )
    persist_directory: Optional[str] = Field(
        "./data/chroma_db", description="Persistence directory"
    )
    host: Optional[str] = Field(None, description="Server host")
    port: Optional[conint(ge=1, le=65535)] = Field(8000, description="Server port")
    distance_function: Optional[DistanceFunction] = Field("cosine", description="Distance metric")
    distance_metric: Optional[DistanceMetric2] = Field(
        "cosine", description="Alternative distance metric name"
    )
    embedding_dimension: Optional[conint(ge=1, le=4096)] = Field(
        768, description="Embedding dimension"
    )
    enable_deduplication: Optional[bool] = Field(True, description="Enable document deduplication")
    embedding_function: Optional[str] = Field(None, description="Built-in embedding function")


class Metric2(Enum):
    L2 = "L2"
    IP = "IP"
    Cosine = "Cosine"


class Config102(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    dimension: conint(ge=1, le=4096) = Field(..., description="Vector dimension")
    index_type: Optional[IndexType] = Field("Flat", description="Index type")
    metric: Optional[Metric2] = Field("L2", description="Distance metric")
    nlist: Optional[conint(ge=1)] = Field(100, description="Number of clusters (IVF)")
    nprobe: Optional[conint(ge=1)] = Field(10, description="Clusters to search (IVF)")
    use_gpu: Optional[bool] = Field(False, description="Enable GPU acceleration")


class Metric3(Enum):
    euclidean = "euclidean"
    cosine = "cosine"
    dotproduct = "dotproduct"


class Config103(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    api_key: str = Field(..., description="Pinecone API key")
    environment: Optional[str] = Field("us-east-1-aws", description="Pinecone environment")
    index_name: constr(pattern=r"^[a-z0-9-]+$") = Field(..., description="Index name")
    dimension: conint(ge=1, le=20000) = Field(..., description="Vector dimension")
    metric: Optional[Metric3] = Field("cosine", description="Distance metric")
    namespace: Optional[str] = Field("", description="Namespace for isolation")
    replicas: Optional[conint(ge=1, le=20)] = Field(1, description="Number of replicas")


class Config104(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    host: Optional[str] = Field("localhost", description="Server host")
    port: Optional[conint(ge=1, le=65535)] = Field(6333, description="Server port")
    grpc_port: Optional[conint(ge=1, le=65535)] = Field(6334, description="gRPC port")
    api_key: Optional[str] = Field(None, description="API key")
    collection_name: Optional[constr(pattern=r"^[a-zA-Z0-9_-]+$")] = Field(
        "documents", description="Collection name"
    )
    vector_size: conint(ge=1, le=65536) = Field(..., description="Vector dimension")
    distance: Optional[Distance] = Field("Cosine", description="Distance metric")
    on_disk: Optional[bool] = Field(False, description="Store vectors on disk")


class VectorStore1(BaseModel):
    type: Type8
    config: Union[Config101, Config102, Config103, Config104]


class DistanceMetric3(Enum):
    cosine = "cosine"
    euclidean = "euclidean"
    manhattan = "manhattan"
    dot = "dot"


class Config105(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    top_k: Optional[conint(ge=1, le=1000)] = Field(10, description="Number of results")
    distance_metric: Optional[DistanceMetric3] = Field("cosine", description="Distance metric")
    score_threshold: Optional[confloat(ge=0.0, le=1.0)] = Field(
        None, description="Minimum similarity score"
    )


class RetrievalStrategy5(BaseModel):
    type: Literal["BasicSimilarityStrategy"] = Field(
        ..., description="Retrieval strategy type identifier"
    )
    config: Config105 = Field(..., title="Basic Similarity Configuration")


class Config106(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    top_k: Optional[conint(ge=1, le=1000)] = Field(10, description="Number of results")
    filters: Optional[dict[str, Union[str, float, bool, list[Union[str, float, bool]]]]] = Field(
        {}, description="Metadata filters"
    )
    filter_mode: Optional[FilterMode] = Field("pre", description="When to apply filters")
    fallback_multiplier: Optional[conint(ge=1, le=10)] = Field(
        3, description="Multiplier for post-filtering"
    )


class RetrievalStrategy6(BaseModel):
    type: Literal["MetadataFilteredStrategy"] = Field(
        ..., description="Retrieval strategy type identifier"
    )
    config: Config106 = Field(..., title="Metadata Filtered Configuration")


class Config107(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    num_queries: Optional[conint(ge=1, le=10)] = Field(3, description="Number of query variations")
    top_k: Optional[conint(ge=1, le=1000)] = Field(10, description="Results per query")
    aggregation_method: Optional[AggregationMethod] = Field(
        "weighted", description="Result aggregation method"
    )
    query_weights: Optional[list[confloat(ge=0.0, le=1.0)]] = Field(
        None, description="Weights for each query"
    )


class RetrievalStrategy7(BaseModel):
    type: Literal["MultiQueryStrategy"] = Field(
        ..., description="Retrieval strategy type identifier"
    )
    config: Config107 = Field(..., title="Multi Query Configuration")


class Config108(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    initial_k: Optional[conint(ge=10, le=1000)] = Field(30, description="Initial candidates")
    final_k: Optional[conint(ge=1, le=100)] = Field(10, description="Final results")
    rerank_factors: Optional[RerankFactors] = Field(None, description="Reranking factor weights")
    normalize_scores: Optional[bool] = Field(True, description="Normalize scores before combining")


class RetrievalStrategy8(BaseModel):
    type: Literal["RerankedStrategy"] = Field(..., description="Retrieval strategy type identifier")
    config: Config108 = Field(..., title="Reranked Configuration")


class Type9(Enum):
    BasicSimilarityStrategy = "BasicSimilarityStrategy"
    MetadataFilteredStrategy = "MetadataFilteredStrategy"
    MultiQueryStrategy = "MultiQueryStrategy"
    RerankedStrategy = "RerankedStrategy"


class Strategy2(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    type: Type9
    weight: Optional[confloat(ge=0.0, le=1.0)] = 1.0
    config: Optional[dict[str, Any]] = None


class Config109(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    strategies: Optional[list[Strategy2]] = Field(
        None, description="Sub-strategies to combine", max_length=5, min_length=2
    )
    combination_method: Optional[CombinationMethod] = Field(
        "weighted_average", description="Combination method"
    )
    final_k: Optional[conint(ge=1, le=1000)] = Field(10, description="Final number of results")


class RetrievalStrategy9(BaseModel):
    type: Literal["HybridUniversalStrategy"] = Field(
        ..., description="Retrieval strategy type identifier"
    )
    config: Config109 = Field(..., title="Hybrid Universal Configuration")


class Components1(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    parser: Parser1 = Field(..., description="Parser configuration", title="Parser Configuration")
    extractors: Optional[list[Extractor1]] = Field(None, max_length=10)
    embedder: Embedder1 = Field(..., title="Embedder Configuration")
    vector_store: VectorStore1 = Field(..., title="Vector Store Configuration")
    retrieval_strategy: Union[
        RetrievalStrategy5,
        RetrievalStrategy6,
        RetrievalStrategy7,
        RetrievalStrategy8,
        RetrievalStrategy9,
    ] = Field(..., title="Retrieval Strategy Configuration")


class StrategyTemplates(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    name: constr(pattern=r"^[a-z][a-z0-9_]*$", min_length=1, max_length=50) = Field(
        ...,
        description="Unique strategy identifier",
        examples=["simple", "customer_support", "legal_documents"],
    )
    description: constr(min_length=10, max_length=500) = Field(
        ..., description="Clear description of strategy purpose"
    )
    tags: Optional[list[Tag]] = Field(
        None,
        description="Tags for categorization",
        examples=[["production", "optimized"], ["development", "testing"]],
    )
    use_cases: Optional[list[UseCase]] = Field(None, description="Specific use cases", min_length=1)
    components: Components1 = Field(..., title="Strategy Components")
    optimization: Optional[dict[str, Any]] = Field(
        None,
        description="Performance and resource optimization settings",
        title="Optimization Settings",
    )
    validation: Optional[dict[str, Any]] = Field(
        None, description="Data validation and quality rules", title="Validation Rules"
    )
    monitoring: Optional[dict[str, Any]] = Field(
        None, description="Monitoring and metrics settings", title="Monitoring Configuration"
    )


class Dependencies(BaseModel):
    required: Optional[list[str]] = None
    optional: Optional[list[str]] = None


class Parsers(BaseModel):
    description: Optional[str] = None
    class_name: Optional[str] = None
    module: Optional[str] = None
    use_cases: Optional[list[str]] = None
    dependencies: Optional[Dependencies] = None


class ComponentMetadata(BaseModel):
    parsers: Optional[dict[str, Parsers]] = None
    extractors: Optional[dict[str, dict[str, Any]]] = None
    embedders: Optional[dict[str, dict[str, Any]]] = None
    vector_stores: Optional[dict[str, dict[str, Any]]] = None
    retrieval_strategies: Optional[dict[str, dict[str, Any]]] = None


class CompatibilityRules(BaseModel):
    component_compatibility: Optional[dict[str, Any]] = None
    embedder_store_compatibility: Optional[dict[str, Any]] = None
    strategy_store_optimization: Optional[dict[str, Any]] = None


class ValidationRules(BaseModel):
    required_fields: Optional[dict[str, Any]] = None
    field_constraints: Optional[dict[str, Any]] = None
    performance_warnings: Optional[list[dict[str, Any]]] = None


class Rag(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    strategies: list[Strategy] = Field(
        ...,
        description="Array of strategy configurations",
        min_length=1,
        title="Strategy Definitions",
    )
    strategy_templates: Optional[dict[str, StrategyTemplates]] = Field(
        {},
        description="Ready-to-use strategy configurations",
        title="Predefined Strategy Templates",
    )
    component_metadata: Optional[ComponentMetadata] = Field(
        None, description="Metadata about available components", title="Component Metadata"
    )
    compatibility_rules: Optional[CompatibilityRules] = Field(
        None, description="Rules for component compatibility", title="Component Compatibility Rules"
    )
    validation_rules: Optional[ValidationRules] = Field(
        None, description="System-wide validation rules", title="Validation Rules"
    )
    best_practices: Optional[dict[str, list[str]]] = Field(
        None, description="Recommended configuration patterns", title="Best Practices"
    )


class Dataset(BaseModel):
    name: str = Field(..., description="Dataset name")
    rag_strategy: Optional[str] = Field("auto", description="RAG strategy to use for the dataset")
    files: list[str] = Field(..., description="List of file hashes")


class Provider(Enum):
    openai = "openai"
    ollama = "ollama"


class Runtime(BaseModel):
    provider: Provider
    model: str = Field(..., description="Model name or ID")
    base_url: Optional[str] = Field(None, description="Base URL for the provider")
    api_key: Optional[str] = Field(None, description="API key for the provider")
    instructor_mode: Optional[str] = Field(
        None, description="Instructor mode to use for the provider"
    )
    model_api_parameters: Optional[Any] = Field(
        None, description="Additional parameters passed to the API provider"
    )


class LlamaFarmConfig(BaseModel):
    version: Version = Field(..., description='Config version, must be "v1"')
    name: str = Field(..., description="Project name", examples=["my-project"])
    namespace: str = Field(..., description="Project namespace", examples=["my-namespace"])
    prompts: Optional[list[Prompt]] = []
    rag: Optional[Rag] = Field(
        None,
        description="Schema for RAG system strategy configurations",
        title="RAG Strategy Configuration Schema",
    )
    datasets: Optional[list[Dataset]] = Field([], description="List of dataset configurations")
    runtime: Runtime
