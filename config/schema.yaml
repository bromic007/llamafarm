# yaml-language-server: $schema=http://json-schema.org/draft-07/schema#

$schema: http://json-schema.org/draft-07/schema#
title: LlamaFarm Config
type: object
required:
  - version
  - name
  - namespace
  - runtime
properties:
  version:
    type: string
    enum: [v1]
    default: v1
    description: Config version, must be "v1"
  name:
    type: string
    description: Project name
    example: my-project
  namespace:
    type: string
    description: Project namespace
    example: my-namespace
  prompts:
    type: array
    description: List of named prompt sets
    items:
      title: PromptSet
      type: object
      required:
        - name
        - messages
      properties:
        name:
          type: string
          pattern: "^[a-z][a-z0-9_]*$"
          description: Unique prompt set identifier
        messages:
          type: array
          description: List of messages in this prompt set
          items:
            type: object
            title: PromptMessage
            required:
              - role
              - content
            properties:
              role:
                type: string
                description: 'Message role (e.g., "system", "user", "assistant", "tool")'
              content:
                type: string
                description: Message content
              tool_call_id:
                type: string
                description: Tool call ID
          default: []
    default: []

  rag:
    $ref: "../rag/schema.yaml"

  datasets:
    type: array
    description: List of dataset configurations
    items:
      type: object
      required:
        - name
        - data_processing_strategy
        - database
      properties:
        name:
          type: string
          description: Dataset name
        auto_process:
          type: boolean
          description: Whether to automatically process uploads into the vector store
          default: true
        data_processing_strategy:
          type: string
          description: RAG data processing strategy to use for the dataset
        database:
          type: string
          description: RAG database to use for the dataset
    default: []

  runtime:
    type: object
    properties:
      default_model:
        type: string
        description: Name of the default model to use (references a model name in the models list). If not specified, uses first model.

      models:
        type: array
        description: List of model configurations for multi-model support
        items:
          type: object
          required:
            - name
            - provider
            - model
          properties:
            name:
              type: string
              description: Model identifier (unique name)
            description:
              type: string
              description: Human-readable description of this model configuration
            provider:
              type: string
              enum: [openai, ollama, lemonade, universal]
              description: Runtime provider for this model
            model:
              type: string
              description: Model name or ID
            base_url:
              type: string
              description: Base URL for the provider
            api_key:
              type: string
              description: API key for the provider
            instructor_mode:
              type: string
              description: "Instructor mode to use for structured output (e.g., tools, json, md_json)"
            model_api_parameters:
              type: object
              description: |
                Additional parameters passed directly to the API provider as request parameters.
                Common examples: temperature, top_p, max_tokens, frequency_penalty, etc.
              additionalProperties: true
            extra_body:
              type: object
              description: |
                Provider-specific parameters passed in the request body's extra_body field.
                These parameters are sent directly to the underlying provider API.

                Note: For GGUF quantization, specify it in the model name using the format
                "model_id:quantization" (e.g., "unsloth/Qwen3-4B-GGUF:Q4_K_M").

                The properties below document common GGUF/llama.cpp parameters, but any
                additional parameters will be passed through to the provider.
              properties:
                n_ctx:
                  type: integer
                  description: |
                    Context window size for GGUF models (Universal runtime only).
                    If not specified, automatically computed based on available memory,
                    model training context, and pattern-based defaults.
                    See Universal runtime documentation for auto-detection behavior.
                n_batch:
                  type: integer
                  minimum: 1
                  description: |
                    Batch size for prompt processing. Controls compute buffer memory usage.
                    Lower values (e.g., 512) reduce memory significantly at slight speed cost.
                    Critical for memory-constrained devices like Jetson (8GB shared memory).
                n_gpu_layers:
                  type: integer
                  minimum: -1
                  description: |
                    Number of layers to offload to GPU. Use -1 to offload all layers.
                    For models like Qwen3-1.7B with 29 layers, use 29 for full GPU offload.
                n_threads:
                  type: integer
                  minimum: 1
                  description: |
                    Number of CPU threads to use. If not specified, auto-detected.
                    For Jetson Orin Nano, use 6 (number of CPU cores).
                flash_attn:
                  type: boolean
                  description: |
                    Enable flash attention for faster inference. Requires GPU support.
                    Recommended for Ampere+ GPUs (RTX 30xx, A100, Jetson Orin).
                use_mmap:
                  type: boolean
                  description: |
                    Memory-map model file for efficient memory usage. Default: true.
                    Enables OS to swap model pages efficiently on memory-constrained devices.
                use_mlock:
                  type: boolean
                  description: |
                    Lock model in RAM to prevent swapping. Default: false.
                    Set to false on memory-constrained devices (e.g., 8GB Jetson) to allow
                    OS memory management.
                cache_type_k:
                  type: string
                  description: |
                    KV cache key quantization type. Lower precision reduces memory usage.
                    Common values: f32 (full), f16 (half), q8_0, q5_1, q5_0, q4_1, q4_0.
                    Using q4_0 can reduce KV cache memory by ~4x vs f16.
                cache_type_v:
                  type: string
                  description: |
                    KV cache value quantization type. Lower precision reduces memory usage.
                    Common values: f32 (full), f16 (half), q8_0, q5_1, q5_0, q4_1, q4_0.
                    Using q4_0 can reduce KV cache memory by ~4x vs f16.
              additionalProperties: true
            encoder_config:
              type: object
              description: |
                Configuration for BERT-style encoder models (Universal runtime only).
                Used for embeddings, classification, reranking, and NER endpoints.
              properties:
                max_length:
                  type: integer
                  minimum: 1
                  description: |
                    Maximum sequence length for tokenization. Auto-detected if not specified.
                    ModernBERT supports up to 8,192 tokens, classic BERT supports 512.
                    Must be a positive integer if specified.
                use_flash_attention:
                  type: boolean
                  description: |
                    Enable Flash Attention 2 for faster inference on CUDA devices.
                    Requires flash_attn package and compatible GPU.
                  default: true
                task:
                  type: string
                  enum: [embedding, classification, reranking, ner]
                  description: |
                    Task type for the encoder model:
                    - embedding: Generate dense vector representations
                    - classification: Sentiment, spam detection, intent routing
                    - reranking: Cross-encoder document reranking
                    - ner: Named entity recognition
                  default: embedding
            prompts:
              type: array
              description: List of prompt set names to use for this model (merged in order)
              items:
                type: string
              default: []
            mcp_servers:
              type: array
              description: List of MCP server names to use for this model (omit to use all servers, empty list for none)
              items:
                type: string
            tool_call_strategy:
              type: string
              enum: [native_api, prompt_based]
              description: |
                Strategy to use for tool calls. `native_api` uses native tool calling through the client
                library (e.g. setting the `tools` parameter on the chat completions OpenAI request);
                `prompt_based` uses system prompting to inject tool definitions and instructions to guide
                any model to use tools. This is universal, but may not be as effective as native tool calling.
                Use `prompt_based` for models that do not support native tool calling.
              default: native_api
            tools:
              type: array
              description: List of tools to use for this model
              items:
                $ref: "#/definitions/Tool"
              default: []
            keep_loaded:
              type: boolean
              description: |
                Keep this model loaded in memory to prevent cache eviction.
                When true, the runtime will not purge this model from cache
                even under memory pressure. Useful for models that need fast
                response times (e.g., voice chat LLMs).
              default: false

  mcp:
    type: object
    description: Model Context Protocol (MCP) client configuration
    properties:
      servers:
        type: array
        description: List of MCP Servers available to the project during inference
        items:
          type: object
          required:
            - name
            - transport
          properties:
            name:
              type: string
              description: MCP server identifier
            transport:
              type: string
              enum: [stdio, http, sse]
              description: Connection transport to the MCP server
            # stdio transport
            command:
              type: string
              description: Command/binary to launch the MCP server (stdio)
            args:
              type: array
              description: Optional args for the stdio command
              items:
                type: string
            env:
              type: object
              description: Environment variables for the stdio command
              additionalProperties:
                type: string
            # http transport
            base_url:
              type: string
              description: Base URL of the MCP server (http)
            headers:
              type: object
              description: HTTP headers for the MCP server
              additionalProperties:
                type: string
              example:
                Authorization: Bearer ${env:MCP_SERVER_TOKEN}

  voice:
    type: object
    description: Voice chat configuration for real-time speech interaction via WebSocket
    properties:
      enabled:
        type: boolean
        description: Enable or disable the voice chat endpoint
        default: true
      llm_model:
        type: string
        description: |
          Reference to a model name in runtime.models[] to use for voice chat.
          The model's prompts array will be applied to voice conversations.
      tts:
        type: object
        description: Text-to-speech configuration
        properties:
          model:
            type: string
            description: |
              TTS model/backend ID:
              - kokoro: High-quality neural TTS with built-in voices (default)
              - chatterbox-turbo: Fast voice cloning TTS (350M params, sub-200ms)
                Requires voice_profiles configuration for voice cloning.
              - pocket-tts: Lightweight CPU TTS from Kyutai (100M params, ~6x realtime)
                Fast and efficient, no GPU required.
            default: kokoro
          voice:
            type: string
            description: |
              Voice identifier for synthesis.

              For Kokoro, use built-in voice IDs:
              - af_heart (American Female, default)
              - af_bella, af_nicole, af_sarah, af_sky (American Female)
              - am_adam, am_michael (American Male)
              - bf_emma, bf_isabella (British Female)
              - bm_george, bm_lewis (British Male)

              For Pocket TTS, use built-in voice IDs:
              - alba (default), marius, javert, jean
              - fantine, cosette, eponine, azelma

              For Chatterbox Turbo, use a voice profile name defined in voice_profiles.
            default: af_heart
          speed:
            type: number
            minimum: 0.5
            maximum: 2.0
            description: |
              Speech speed multiplier (0.5-2.0). For more natural-sounding
              speech, try 0.9-0.95 (slightly slower than default).
              - 0.8-0.9: Slower, deliberate (good for complex topics)
              - 0.9-0.95: Natural conversational pace (recommended)
              - 1.0: Default speed
              - 1.1-1.2: Faster, energetic
            default: 0.95
          keep_loaded:
            type: boolean
            description: |
              Keep TTS model loaded in memory to prevent cache eviction.
              When true, the runtime will not purge this model from cache
              even under memory pressure. Reduces latency for voice responses.
            default: false
          voice_profiles:
            type: object
            description: |
              Named voice profiles for voice cloning (Chatterbox Turbo only).
              Each profile maps a name to a reference audio file (~10s of speech).
            additionalProperties:
              type: object
              properties:
                audio_path:
                  type: string
                  description: |
                    Path to reference audio file for voice cloning.
                    Can be relative to project directory or absolute.
                description:
                  type: string
                  description: Human-readable description of this voice
              required:
                - audio_path
          temperature:
            type: number
            minimum: 0.1
            maximum: 2.0
            description: |
              Chatterbox Turbo temperature (ignored for Kokoro).
              Controls randomness in generation. Higher = more varied output.
              - 0.5: More deterministic
              - 0.8: Balanced (default)
              - 1.2: More creative/varied
            default: 0.8
          top_k:
            type: integer
            minimum: 1
            maximum: 5000
            description: |
              Chatterbox Turbo top-k sampling (ignored for Kokoro).
              Number of highest probability tokens to consider.
            default: 1000
          top_p:
            type: number
            minimum: 0.0
            maximum: 1.0
            description: |
              Chatterbox Turbo nucleus sampling threshold (ignored for Kokoro).
              Cumulative probability cutoff for token selection.
            default: 0.95
          repetition_penalty:
            type: number
            minimum: 1.0
            maximum: 2.0
            description: |
              Chatterbox Turbo repetition penalty (ignored for Kokoro).
              Penalty applied to repeating tokens. Higher = less repetition.
            default: 1.2
      stt:
        type: object
        description: Speech-to-text configuration
        properties:
          model:
            type: string
            enum: [tiny, base, small, medium, large-v3, distil-large-v3-turbo]
            description: |
              Whisper model size for transcription:
              - tiny (39M, fastest, lower accuracy)
              - base (74M, fast, good accuracy, default)
              - small (244M, medium speed, better accuracy)
              - medium (769M, slower, high accuracy)
              - large-v3 (1.5B, slowest, highest accuracy)
              - distil-large-v3-turbo (~800M, fast & accurate, recommended)
            default: base
          language:
            type: string
            description: Language code for transcription (e.g., "en", "es", "fr")
            default: en
          keep_loaded:
            type: boolean
            description: |
              Keep STT model loaded in memory to prevent cache eviction.
              When true, the runtime will not purge this model from cache
              even under memory pressure. Reduces latency for voice transcription.
            default: false
      enable_thinking:
        type: boolean
        description: |
          Enable thinking/reasoning mode for the LLM. When false (default),
          the LLM is instructed to skip chain-of-thought reasoning and respond
          directly. This is recommended for voice chat since thinking output
          would be spoken aloud by TTS.
        default: false
      turn_detection:
        type: object
        description: |
          End-of-turn detection configuration. Controls how the system detects
          when a user has finished speaking vs when they're just pausing to think.
          This prevents the LLM from responding prematurely during thinking pauses.
        properties:
          enabled:
            type: boolean
            description: |
              Enable smart end-of-turn detection using linguistic analysis.
              When enabled, the system analyzes partial transcriptions to detect
              thinking pauses vs actual end of utterance, preventing premature
              LLM responses. When disabled, uses fixed silence threshold only.
            default: true
          base_silence_duration:
            type: number
            minimum: 0.1
            maximum: 2.0
            description: |
              Base silence duration for complete utterances (seconds).
              Used when the transcription appears linguistically complete
              (ends with punctuation or a complete phrase like "yes", "thanks").
            default: 0.4
          thinking_silence_duration:
            type: number
            minimum: 0.3
            maximum: 5.0
            description: |
              Extended silence duration for incomplete utterances (seconds).
              Used when linguistic analysis suggests the user is mid-thought
              (trailing conjunctions like "and", "but", prepositions like "to",
              "with", or filler words like "um", "uh").
            default: 1.2
          max_silence_duration:
            type: number
            minimum: 0.5
            maximum: 10.0
            description: |
              Maximum silence before forcing end-of-turn (seconds).
              Even if the utterance seems incomplete, processing starts after
              this timeout to ensure responsiveness.
            default: 2.5
      emotion:
        type: object
        description: |
          Speech emotion recognition configuration. Analyzes user audio to detect
          emotional tone (angry, happy, sad, etc.) and provides context to the LLM.
        properties:
          enabled:
            type: boolean
            description: |
              Enable speech emotion recognition. When enabled, analyzes audio to
              detect user emotional tone and includes it in the LLM context. This
              allows the model to respond appropriately to user mood.
            default: true
          model:
            type: string
            description: |
              Emotion recognition model ID:
              - wav2vec2-lg-xlsr-en: English emotion recognition (default, recommended)
              - wav2vec2-base-superb: Alternative model from SUPERB benchmark
              Custom HuggingFace model IDs are also supported.
            default: wav2vec2-lg-xlsr-en
          confidence_threshold:
            type: number
            minimum: 0.0
            maximum: 1.0
            description: |
              Minimum confidence threshold for emotion detection.
              Predictions below this threshold are reported as 'neutral'.
              Lower values increase sensitivity but may produce false positives.
            default: 0.4

definitions:
  Tool:
    type: object
    required:
      - type
      - name
      - description
      - parameters
    properties:
      type:
        type: string
        enum: [function]
        description: Type of tool
      name:
        type: string
        description: Name of the tool
      description:
        type: string
        description: Description of the tool
      parameters:
        type: object
        description: Parameters of the tool
        additionalProperties: true
