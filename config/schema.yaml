# yaml-language-server: $schema=http://json-schema.org/draft-07/schema#

$schema: http://json-schema.org/draft-07/schema#
title: LlamaFarm Config
type: object
required:
  - version
  - name
  - namespace
  - runtime
properties:
  version:
    type: string
    enum: [v1]
    default: v1
    description: Config version, must be "v1"
  name:
    type: string
    description: Project name
    example: my-project
  namespace:
    type: string
    description: Project namespace
    example: my-namespace
  prompts:
    type: array
    description: List of named prompt sets
    items:
      title: PromptSet
      type: object
      required:
        - name
        - messages
      properties:
        name:
          type: string
          pattern: "^[a-z][a-z0-9_]*$"
          description: Unique prompt set identifier
        messages:
          type: array
          description: List of messages in this prompt set
          items:
            type: object
            title: PromptMessage
            required:
              - role
              - content
            properties:
              role:
                type: string
                description: 'Message role (e.g., "system", "user", "assistant", "tool")'
              content:
                type: string
                description: Message content
              tool_call_id:
                type: string
                description: Tool call ID
          default: []
    default: []

  rag:
    $ref: "../rag/schema.yaml"

  datasets:
    type: array
    description: List of dataset configurations
    items:
      type: object
      required:
        - name
        - data_processing_strategy
        - database
      properties:
        name:
          type: string
          description: Dataset name
        auto_process:
          type: boolean
          description: Whether to automatically process uploads into the vector store
          default: true
        data_processing_strategy:
          type: string
          description: RAG data processing strategy to use for the dataset
        database:
          type: string
          description: RAG database to use for the dataset
    default: []

  runtime:
    type: object
    properties:
      default_model:
        type: string
        description: Name of the default model to use (references a model name in the models list). If not specified, uses first model.

      models:
        type: array
        description: List of model configurations for multi-model support
        items:
          type: object
          required:
            - name
            - provider
            - model
          properties:
            name:
              type: string
              description: Model identifier (unique name)
            description:
              type: string
              description: Human-readable description of this model configuration
            provider:
              type: string
              enum: [openai, ollama, lemonade, universal]
              description: Runtime provider for this model
            model:
              type: string
              description: Model name or ID
            base_url:
              type: string
              description: Base URL for the provider
            api_key:
              type: string
              description: API key for the provider
            instructor_mode:
              type: string
              description: "Instructor mode to use for structured output (e.g., tools, json, md_json)"
            model_api_parameters:
              type: object
              description: |
                Additional parameters passed directly to the API provider as request parameters.
                Common examples: temperature, top_p, max_tokens, frequency_penalty, etc.
              additionalProperties: true
            extra_body:
              type: object
              description: |
                Provider-specific parameters passed in the request body's extra_body field.
                These parameters are sent directly to the underlying provider API.

                Note: For GGUF quantization, specify it in the model name using the format
                "model_id:quantization" (e.g., "unsloth/Qwen3-4B-GGUF:Q4_K_M").

                The properties below document common GGUF/llama.cpp parameters, but any
                additional parameters will be passed through to the provider.
              properties:
                n_ctx:
                  type: integer
                  description: |
                    Context window size for GGUF models (Universal runtime only).
                    If not specified, automatically computed based on available memory,
                    model training context, and pattern-based defaults.
                    See Universal runtime documentation for auto-detection behavior.
                n_batch:
                  type: integer
                  minimum: 1
                  description: |
                    Batch size for prompt processing. Controls compute buffer memory usage.
                    Lower values (e.g., 512) reduce memory significantly at slight speed cost.
                    Critical for memory-constrained devices like Jetson (8GB shared memory).
                n_gpu_layers:
                  type: integer
                  minimum: -1
                  description: |
                    Number of layers to offload to GPU. Use -1 to offload all layers.
                    For models like Qwen3-1.7B with 29 layers, use 29 for full GPU offload.
                n_threads:
                  type: integer
                  minimum: 1
                  description: |
                    Number of CPU threads to use. If not specified, auto-detected.
                    For Jetson Orin Nano, use 6 (number of CPU cores).
                flash_attn:
                  type: boolean
                  description: |
                    Enable flash attention for faster inference. Requires GPU support.
                    Recommended for Ampere+ GPUs (RTX 30xx, A100, Jetson Orin).
                use_mmap:
                  type: boolean
                  description: |
                    Memory-map model file for efficient memory usage. Default: true.
                    Enables OS to swap model pages efficiently on memory-constrained devices.
                use_mlock:
                  type: boolean
                  description: |
                    Lock model in RAM to prevent swapping. Default: false.
                    Set to false on memory-constrained devices (e.g., 8GB Jetson) to allow
                    OS memory management.
                cache_type_k:
                  type: string
                  description: |
                    KV cache key quantization type. Lower precision reduces memory usage.
                    Common values: f32 (full), f16 (half), q8_0, q5_1, q5_0, q4_1, q4_0.
                    Using q4_0 can reduce KV cache memory by ~4x vs f16.
                cache_type_v:
                  type: string
                  description: |
                    KV cache value quantization type. Lower precision reduces memory usage.
                    Common values: f32 (full), f16 (half), q8_0, q5_1, q5_0, q4_1, q4_0.
                    Using q4_0 can reduce KV cache memory by ~4x vs f16.
              additionalProperties: true
            encoder_config:
              type: object
              description: |
                Configuration for BERT-style encoder models (Universal runtime only).
                Used for embeddings, classification, reranking, and NER endpoints.
              properties:
                max_length:
                  type: integer
                  minimum: 1
                  description: |
                    Maximum sequence length for tokenization. Auto-detected if not specified.
                    ModernBERT supports up to 8,192 tokens, classic BERT supports 512.
                    Must be a positive integer if specified.
                use_flash_attention:
                  type: boolean
                  description: |
                    Enable Flash Attention 2 for faster inference on CUDA devices.
                    Requires flash_attn package and compatible GPU.
                  default: true
                task:
                  type: string
                  enum: [embedding, classification, reranking, ner]
                  description: |
                    Task type for the encoder model:
                    - embedding: Generate dense vector representations
                    - classification: Sentiment, spam detection, intent routing
                    - reranking: Cross-encoder document reranking
                    - ner: Named entity recognition
                  default: embedding
            prompts:
              type: array
              description: List of prompt set names to use for this model (merged in order)
              items:
                type: string
              default: []
            mcp_servers:
              type: array
              description: List of MCP server names to use for this model (omit to use all servers, empty list for none)
              items:
                type: string
            tool_call_strategy:
              type: string
              enum: [native_api, prompt_based]
              description: |
                Strategy to use for tool calls. `native_api` uses native tool calling through the client
                library (e.g. setting the `tools` parameter on the chat completions OpenAI request);
                `prompt_based` uses system prompting to inject tool definitions and instructions to guide
                any model to use tools. This is universal, but may not be as effective as native tool calling.
                Use `prompt_based` for models that do not support native tool calling.
              default: native_api
            tools:
              type: array
              description: List of tools to use for this model
              items:
                $ref: "#/definitions/Tool"
              default: []

  mcp:
    type: object
    description: Model Context Protocol (MCP) client configuration
    properties:
      servers:
        type: array
        description: List of MCP Servers available to the project during inference
        items:
          type: object
          required:
            - name
            - transport
          properties:
            name:
              type: string
              description: MCP server identifier
            transport:
              type: string
              enum: [stdio, http, sse]
              description: Connection transport to the MCP server
            # stdio transport
            command:
              type: string
              description: Command/binary to launch the MCP server (stdio)
            args:
              type: array
              description: Optional args for the stdio command
              items:
                type: string
            env:
              type: object
              description: Environment variables for the stdio command
              additionalProperties:
                type: string
            # http transport
            base_url:
              type: string
              description: Base URL of the MCP server (http)
            headers:
              type: object
              description: HTTP headers for the MCP server
              additionalProperties:
                type: string
              example:
                Authorization: Bearer ${env:MCP_SERVER_TOKEN}

definitions:
  Tool:
    type: object
    required:
      - type
      - name
      - description
      - parameters
    properties:
      type:
        type: string
        enum: [function]
        description: Type of tool
      name:
        type: string
        description: Name of the tool
      description:
        type: string
        description: Description of the tool
      parameters:
        type: object
        description: Parameters of the tool
        additionalProperties: true
