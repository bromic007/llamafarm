name: End-to-End Tests
permissions:
  contents: read
  actions: read # Required to download artifacts and check workflow status
  pull-requests: write # Can post PR comments (only works for non-fork PRs)

on:
  push:
    branches: [main]
  pull_request:
    types: [opened, synchronize, reopened]
    branches: [main]
  # Allow manual trigger for debugging
  workflow_dispatch:

env:
  NX_NO_CLOUD: true

concurrency:
  group: e2e-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  # ============================================================
  # Notify PR that E2E tests are starting
  # Note: This job is skipped for fork PRs due to GitHub's security model
  # which limits GITHUB_TOKEN to read-only for workflows triggered by forks.
  # ============================================================
  notify-start:
    name: Notify E2E Starting
    # Only run for non-fork PRs (fork PRs have head.repo.fork = true)
    if: github.event_name == 'pull_request' && github.event.pull_request.head.repo.fork == false
    runs-on: ubuntu-latest
    steps:
      - name: Get PR info
        id: pr-info
        run: |
          echo "pr_number=${{ github.event.pull_request.number }}" >> $GITHUB_OUTPUT
          echo "head_sha=${{ github.event.pull_request.head.sha }}" >> $GITHUB_OUTPUT

      - name: Build in-progress comment
        env:
          HEAD_SHA: ${{ steps.pr-info.outputs.head_sha }}
        run: |
          WORKFLOW_RUN_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

          {
            echo "## :hourglass_flowing_sand: **E2E Tests Running...**"
            echo ""
            echo "### Test Results by Platform"
            echo ""
            echo "| OS | Mode | Status |"
            echo "|---|---|---|"
            echo "| ubuntu-latest | source | :hourglass: Pending |"
            echo "| macos-latest | source | :hourglass: Pending |"
            echo "| windows-latest | source | :hourglass: Pending |"
            echo "| ubuntu-latest | binary | :hourglass: Pending |"
            echo "| macos-latest | binary | :hourglass: Pending |"
            echo "| windows-latest | binary | :hourglass: Pending |"
            echo ""
            echo "### Summary"
            echo ""
            echo "- **Status**: Tests in progress..."
            echo "- **Workflow Run**: [View Details](${WORKFLOW_RUN_URL})"
            echo "- **Commit**: \`${HEAD_SHA:0:7}\`"
            echo ""
            echo "---"
            echo "*This comment was automatically generated by the E2E Tests workflow.*"
          } > comment-body.txt

          echo "Comment body prepared:"
          cat comment-body.txt

      - name: Find existing comment
        id: find-comment
        uses: actions/github-script@v7
        env:
          PR_NUMBER: ${{ steps.pr-info.outputs.pr_number }}
        with:
          script: |
            const prNumber = process.env.PR_NUMBER;
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
            });

            // Find comment from this bot (contains the marker)
            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('*This comment was automatically generated by the E2E Tests workflow.*')
            );

            if (botComment) {
              core.setOutput('comment_id', botComment.id);
              core.setOutput('exists', 'true');
              console.log(`Found existing comment: ${botComment.id}`);
            } else {
              core.setOutput('exists', 'false');
              console.log('No existing comment found');
            }

      - name: Post or update PR comment
        uses: actions/github-script@v7
        env:
          PR_NUMBER: ${{ steps.pr-info.outputs.pr_number }}
        with:
          retries: 3
          script: |
            const fs = require('fs');
            const prNumber = process.env.PR_NUMBER;

            if (!fs.existsSync('comment-body.txt')) {
              console.log('Comment file not found');
              return;
            }

            const commentBody = fs.readFileSync('comment-body.txt', 'utf8');
            const commentExists = '${{ steps.find-comment.outputs.exists }}' === 'true';
            const existingCommentId = '${{ steps.find-comment.outputs.comment_id }}';

            if (commentExists && existingCommentId) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingCommentId,
                body: commentBody
              });
              console.log(`Updated existing comment #${existingCommentId} on PR #${prNumber}`);
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: prNumber,
                body: commentBody
              });
              console.log(`Created new comment on PR #${prNumber}`);
            }

  wait-for-cli:
    name: Wait for CLI Build
    runs-on: ubuntu-latest
    outputs:
      cli_run_id: ${{ steps.wait.outputs.run_id }}
    steps:
      - name: Wait for CLI workflow to complete
        id: wait
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          echo "Waiting for 'Build CLI' workflow to complete..."

          # Get the current commit SHA
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            COMMIT_SHA="${{ github.event.pull_request.head.sha }}"
          else
            COMMIT_SHA="${{ github.sha }}"
          fi
          echo "Looking for CLI build for commit: $COMMIT_SHA"

          # Wait up to 30 minutes for the CLI workflow to complete
          # This accounts for GitHub runner queue delays, not just build time
          MAX_ATTEMPTS=60
          SLEEP_TIME=30
          START_TIME=$(date +%s)
          PREV_STATUS=""

          for i in $(seq 1 $MAX_ATTEMPTS); do
            CURRENT_TIME=$(date +%s)
            ELAPSED=$(( (CURRENT_TIME - START_TIME) / 60 ))
            REMAINING=$(( (MAX_ATTEMPTS - i) * SLEEP_TIME / 60 ))

            echo ""
            echo "Attempt $i/$MAX_ATTEMPTS (${ELAPSED}m elapsed, ~${REMAINING}m remaining)"

            # Find the CLI workflow run for this commit
            RUN_INFO=$(gh api \
              -H "Accept: application/vnd.github+json" \
              "/repos/${{ github.repository }}/actions/workflows/cli.yml/runs?head_sha=$COMMIT_SHA&per_page=1" \
              --jq '.workflow_runs[0] | {id: .id, status: .status, conclusion: .conclusion, created_at: .created_at, run_started_at: .run_started_at}' 2>/dev/null || echo "{}")

            RUN_ID=$(echo "$RUN_INFO" | jq -r '.id // empty')
            STATUS=$(echo "$RUN_INFO" | jq -r '.status // empty')
            CONCLUSION=$(echo "$RUN_INFO" | jq -r '.conclusion // empty')
            CREATED_AT=$(echo "$RUN_INFO" | jq -r '.created_at // empty')
            RUN_STARTED_AT=$(echo "$RUN_INFO" | jq -r '.run_started_at // empty')

            if [ -z "$RUN_ID" ]; then
              echo "  No CLI workflow run found yet, waiting..."
              sleep $SLEEP_TIME
              continue
            fi

            echo "  CLI workflow $RUN_ID: status=$STATUS, conclusion=$CONCLUSION"

            # Detect status transitions
            if [ -n "$PREV_STATUS" ] && [ "$STATUS" != "$PREV_STATUS" ]; then
              echo "  Status changed: $PREV_STATUS → $STATUS"
            fi
            PREV_STATUS="$STATUS"

            # Get queue statistics for context
            QUEUED_COUNT=$(gh api \
              -H "Accept: application/vnd.github+json" \
              "/repos/${{ github.repository }}/actions/workflows/cli.yml/runs?status=queued" \
              --jq '.total_count' 2>/dev/null || echo "?")
            IN_PROGRESS_COUNT=$(gh api \
              -H "Accept: application/vnd.github+json" \
              "/repos/${{ github.repository }}/actions/workflows/cli.yml/runs?status=in_progress" \
              --jq '.total_count' 2>/dev/null || echo "?")
            echo "  Queue status: $QUEUED_COUNT jobs queued, $IN_PROGRESS_COUNT running"

            # Status-specific messaging
            case "$STATUS" in
              queued)
                echo "  Waiting for GitHub runner..."
                ;;
              in_progress)
                if [ -n "$RUN_STARTED_AT" ] && [ "$RUN_STARTED_AT" != "null" ]; then
                  RUN_START_EPOCH=$(date -d "$RUN_STARTED_AT" +%s 2>/dev/null || date -j -f "%Y-%m-%dT%H:%M:%SZ" "$RUN_STARTED_AT" +%s 2>/dev/null || echo "")
                  if [ -n "$RUN_START_EPOCH" ]; then
                    BUILD_DURATION=$(( (CURRENT_TIME - RUN_START_EPOCH) / 60 ))
                    echo "  Building... (started ${BUILD_DURATION}m ago)"
                  else
                    echo "  Building..."
                  fi
                else
                  echo "  Building..."
                fi
                ;;
              completed)
                if [ "$CONCLUSION" = "success" ]; then
                  echo "✓ CLI workflow completed successfully!"
                  echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT

                  # Write job summary
                  {
                    echo "## CLI Build Wait Summary"
                    echo ""
                    echo "- **Total wait time**: ${ELAPSED}m"
                    echo "- **Attempts**: $i/$MAX_ATTEMPTS"
                    echo "- **CLI Run ID**: $RUN_ID"
                    echo "- **Conclusion**: $CONCLUSION"
                  } >> $GITHUB_STEP_SUMMARY

                  exit 0
                elif [ "$CONCLUSION" = "cancelled" ]; then
                  echo "❌ CLI workflow was cancelled (likely by concurrency controls)"
                  echo "  This can happen when a new commit is pushed while the previous build is queued."
                  echo "  The E2E tests for the previous commit will not run."
                  exit 1
                else
                  echo "❌ CLI workflow failed with conclusion: $CONCLUSION"
                  exit 1
                fi
                ;;
            esac

            sleep $SLEEP_TIME
          done

          FINAL_ELAPSED=$(( ($(date +%s) - START_TIME) / 60 ))
          echo ""
          echo "❌ Timed out waiting for CLI workflow after ${FINAL_ELAPSED}m"
          echo "  Last known status: $STATUS"
          echo "  Queue stats at timeout: $QUEUED_COUNT queued, $IN_PROGRESS_COUNT running"

          # Write timeout summary
          {
            echo "## CLI Build Wait - TIMEOUT"
            echo ""
            echo "- **Total wait time**: ${FINAL_ELAPSED}m"
            echo "- **Last status**: $STATUS"
            echo "- **Queue at timeout**: $QUEUED_COUNT queued, $IN_PROGRESS_COUNT running"
            echo ""
            echo "The CLI workflow did not complete within the 30-minute timeout."
            echo "This is usually caused by GitHub runner availability issues."
          } >> $GITHUB_STEP_SUMMARY

          exit 1

  wait-for-pyapp:
    name: Wait for PyApp Build
    runs-on: ubuntu-latest
    outputs:
      pyapp_run_id: ${{ steps.wait.outputs.run_id }}
    steps:
      - name: Wait for PyApp workflow (if triggered)
        id: wait
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          echo "Checking for 'Build Native Binaries (PyApp)' workflow run..."

          # Get the current commit SHA
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            COMMIT_SHA="${{ github.event.pull_request.head.sha }}"
          else
            COMMIT_SHA="${{ github.sha }}"
          fi
          echo "Looking for PyApp build for commit: $COMMIT_SHA"

          # Phase 1: Check if a PyApp workflow run exists for this commit.
          # pyapp.yml only triggers when tools/pyapp/** or the workflow file changes,
          # so there may not be a run for every commit. Wait up to 5 minutes for
          # GitHub to dispatch it (accounting for queue delays).
          DISCOVERY_ATTEMPTS=10
          SLEEP_TIME=30
          RUN_ID=""

          echo ""
          echo "Phase 1: Checking if PyApp workflow was triggered for this commit..."
          for i in $(seq 1 $DISCOVERY_ATTEMPTS); do
            RUN_INFO=$(gh api \
              -H "Accept: application/vnd.github+json" \
              "/repos/${{ github.repository }}/actions/workflows/pyapp.yml/runs?head_sha=$COMMIT_SHA&per_page=1" \
              --jq '.workflow_runs[0] | {id: .id, status: .status, conclusion: .conclusion}' 2>/dev/null || echo "{}")

            RUN_ID=$(echo "$RUN_INFO" | jq -r '.id // empty')

            if [ -n "$RUN_ID" ]; then
              echo "  Found PyApp workflow run: $RUN_ID"
              break
            fi

            echo "  Attempt $i/$DISCOVERY_ATTEMPTS: No PyApp workflow run found, waiting..."
            sleep $SLEEP_TIME
          done

          if [ -z "$RUN_ID" ]; then
            echo ""
            echo "No PyApp workflow run found for this commit."
            echo "Binary tests will use the latest artifacts from the branch."
            echo "run_id=" >> $GITHUB_OUTPUT

            {
              echo "## PyApp Build Wait Summary"
              echo ""
              echo "- **Result**: No PyApp workflow triggered for this commit"
              echo "- **Fallback**: Binary tests will use latest branch artifacts"
            } >> $GITHUB_STEP_SUMMARY

            exit 0
          fi

          # Phase 2: Wait for the PyApp workflow to complete.
          # PyApp builds compile Rust binaries across 4 platforms, so they can
          # take 20-40 minutes. Wait up to 45 minutes total.
          echo ""
          echo "Phase 2: Waiting for PyApp workflow $RUN_ID to complete..."
          MAX_ATTEMPTS=90
          START_TIME=$(date +%s)
          PREV_STATUS=""

          for i in $(seq 1 $MAX_ATTEMPTS); do
            CURRENT_TIME=$(date +%s)
            ELAPSED=$(( (CURRENT_TIME - START_TIME) / 60 ))
            REMAINING=$(( (MAX_ATTEMPTS - i) * SLEEP_TIME / 60 ))

            RUN_INFO=$(gh api \
              -H "Accept: application/vnd.github+json" \
              "/repos/${{ github.repository }}/actions/runs/$RUN_ID" \
              --jq '{status: .status, conclusion: .conclusion}' 2>/dev/null || echo "{}")

            STATUS=$(echo "$RUN_INFO" | jq -r '.status // empty')
            CONCLUSION=$(echo "$RUN_INFO" | jq -r '.conclusion // empty')

            if [ -n "$PREV_STATUS" ] && [ "$STATUS" != "$PREV_STATUS" ]; then
              echo "  Status changed: $PREV_STATUS → $STATUS"
            fi
            PREV_STATUS="$STATUS"

            echo "  Attempt $i: status=$STATUS, conclusion=$CONCLUSION (${ELAPSED}m elapsed, ~${REMAINING}m remaining)"

            if [ "$STATUS" = "completed" ]; then
              if [ "$CONCLUSION" = "success" ]; then
                echo ""
                echo "✓ PyApp workflow completed successfully!"
                echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT

                {
                  echo "## PyApp Build Wait Summary"
                  echo ""
                  echo "- **Total wait time**: ${ELAPSED}m"
                  echo "- **PyApp Run ID**: $RUN_ID"
                  echo "- **Conclusion**: $CONCLUSION"
                } >> $GITHUB_STEP_SUMMARY

                exit 0
              elif [ "$CONCLUSION" = "cancelled" ]; then
                echo ""
                echo "❌ PyApp workflow was cancelled (likely by concurrency controls)"
                exit 1
              else
                echo ""
                echo "❌ PyApp workflow failed with conclusion: $CONCLUSION"
                exit 1
              fi
            fi

            sleep $SLEEP_TIME
          done

          FINAL_ELAPSED=$(( ($(date +%s) - START_TIME) / 60 ))
          echo ""
          echo "❌ Timed out waiting for PyApp workflow after ${FINAL_ELAPSED}m"

          {
            echo "## PyApp Build Wait - TIMEOUT"
            echo ""
            echo "- **Total wait time**: ${FINAL_ELAPSED}m"
            echo "- **Last status**: $STATUS"
            echo ""
            echo "The PyApp workflow did not complete within the 45-minute timeout."
            echo "This is usually caused by GitHub runner availability issues."
          } >> $GITHUB_STEP_SUMMARY

          exit 1

  e2e-test:
    name: E2E Tests (${{ matrix.os }}, ${{ matrix.deploy_mode }})
    needs: [wait-for-cli, wait-for-pyapp]
    runs-on: ${{ matrix.os }}
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix:
        include:
          # Source mode (existing behavior: uv + source)
          - os: ubuntu-latest
            artifact: llamafarm-linux-amd64
            binary: lf
            deploy_mode: source
          - os: macos-latest
            artifact: llamafarm-darwin-arm64
            binary: lf
            deploy_mode: source
          - os: windows-latest
            artifact: llamafarm-windows-amd64.exe
            binary: lf.exe
            deploy_mode: source
          # Binary deploy mode (PyApp standalone binaries)
          - os: ubuntu-latest
            artifact: llamafarm-linux-amd64
            binary: lf
            deploy_mode: binary
            pyapp_os: linux-x86_64
          - os: macos-latest
            artifact: llamafarm-darwin-arm64
            binary: lf
            deploy_mode: binary
            pyapp_os: macos-arm64
          - os: windows-latest
            artifact: llamafarm-windows-amd64.exe
            binary: lf.exe
            deploy_mode: binary
            pyapp_os: windows-x86_64
    env:
      # Use the current branch/ref being tested instead of main
      # For PRs (including fork PRs), use the merge ref which is always accessible
      # For pushes to main, use the branch name directly
      LF_VERSION_REF: ${{ github.event_name == 'pull_request' && format('refs/pull/{0}/merge', github.event.pull_request.number) || github.ref_name }}
      # Use CPU-only PyTorch to avoid downloading 3GB+ of CUDA packages in CI
      UV_EXTRA_INDEX_URL: https://download.pytorch.org/whl/cpu
      # Allow uv to search all indexes for best version match (needed because PyTorch index
      # has old versions of common packages like 'requests' that would block resolution)
      UV_INDEX_STRATEGY: unsafe-best-match
      # Force CPU for GGUF inference to avoid Metal SIGSEGV on macOS CI
      # (Apple Paravirtual GPU in GitHub Actions VMs is unreliable)
      LLAMAFARM_GGUF_FORCE_CPU: 1
      # Disable Metal backend initialization completely (prevents SIGSEGV during llama_context creation)
      GGML_METAL_ENABLE: 0

    steps:
      # ============================================================
      # Setup Phase
      # ============================================================
      - name: Checkout code
        uses: actions/checkout@v4

      # ============================================================
      # Download Pre-built CLI Binary
      # ============================================================
      - name: Download CLI binary artifact
        uses: dawidd6/action-download-artifact@v6
        with:
          workflow: cli.yml
          run_id: ${{ needs.wait-for-cli.outputs.cli_run_id }}
          name: ${{ matrix.artifact }}
          path: cli/
          if_no_artifact_found: fail

      - name: Download PR info artifact
        if: github.event_name == 'pull_request'
        uses: dawidd6/action-download-artifact@v6
        with:
          workflow: cli.yml
          run_id: ${{ needs.wait-for-cli.outputs.cli_run_id }}
          name: pr-info
          path: ./pr-info
          if_no_artifact_found: ignore

      - name: Set PR info
        id: pr-info
        shell: bash
        run: |
          # For PRs, use the PR number directly from the event
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "pr_number=${{ github.event.pull_request.number }}" >> $GITHUB_OUTPUT
            echo "head_sha=${{ github.event.pull_request.head.sha }}" >> $GITHUB_OUTPUT
            echo "✓ PR metadata from event: PR #${{ github.event.pull_request.number }}"
          else
            echo "pr_number=" >> $GITHUB_OUTPUT
            echo "Not a PR event"
          fi

      - name: Rename binary to standard name
        shell: bash
        run: |
          cd cli
          # The artifact contains the binary with the full artifact name
          # Rename it to the short name we want to use
          if [ -f "${{ matrix.artifact }}" ]; then
            mv "${{ matrix.artifact }}" "${{ matrix.binary }}"
            echo "✓ Renamed ${{ matrix.artifact }} to ${{ matrix.binary }}"
          else
            echo "Error: Downloaded artifact not found at cli/${{ matrix.artifact }}"
            ls -la
            exit 1
          fi

      - name: Make CLI binary executable
        if: runner.os != 'Windows'
        shell: bash
        run: chmod +x cli/${{ matrix.binary }}

      - name: Verify CLI binary works
        working-directory: cli
        shell: bash
        run: |
          ./${{ matrix.binary }} version
          echo "✓ CLI binary is functional"

      # ============================================================
      # Setup PyApp Binary Auto-Download (binary deploy mode only)
      # ============================================================
      - name: Verify PyApp artifacts exist
        if: matrix.deploy_mode == 'binary'
        id: check-artifacts
        shell: bash
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          # Determine which PyApp run to use
          if [ -n "${{ needs.wait-for-pyapp.outputs.pyapp_run_id }}" ]; then
            RUN_ID="${{ needs.wait-for-pyapp.outputs.pyapp_run_id }}"
            echo "Using PyApp run from current commit: $RUN_ID"
          else
            # Try to find latest successful PyApp run from this branch
            BRANCH="${{ github.head_ref || github.ref_name }}"
            echo "Looking for latest PyApp run from branch: $BRANCH"

            RUN_ID=$(gh api \
              -H "Accept: application/vnd.github+json" \
              "/repos/${{ github.repository }}/actions/workflows/pyapp.yml/runs?branch=$BRANCH&status=success&per_page=1" \
              --jq '.workflow_runs[0].id // empty' 2>/dev/null || echo "")

            if [ -z "$RUN_ID" ]; then
              echo "::error::No successful PyApp workflow runs found for branch $BRANCH"
              echo "Run the 'Build Native Binaries (PyApp)' workflow first, then re-run this workflow."
              exit 1
            fi
            echo "Using latest PyApp run from branch: $RUN_ID"
          fi

          # Verify all three component artifacts exist for our platform
          PLATFORM="${{ matrix.pyapp_os }}"
          COMPONENTS=("server" "rag" "runtime")
          MISSING_ARTIFACTS=()

          for component in "${COMPONENTS[@]}"; do
            ARTIFACT_NAME="llamafarm-${component}-pyapp-${PLATFORM}"
            ARTIFACT_COUNT=$(gh api \
              -H "Accept: application/vnd.github+json" \
              "/repos/${{ github.repository }}/actions/runs/$RUN_ID/artifacts" \
              --jq ".artifacts[] | select(.name == \"$ARTIFACT_NAME\") | .name" 2>/dev/null | wc -l || echo "0")

            if [ "$ARTIFACT_COUNT" -eq 0 ]; then
              MISSING_ARTIFACTS+=("$ARTIFACT_NAME")
            else
              echo "✓ Found artifact: $ARTIFACT_NAME"
            fi
          done

          if [ ${#MISSING_ARTIFACTS[@]} -gt 0 ]; then
            echo "::error::Missing PyApp artifacts for platform $PLATFORM in run $RUN_ID:"
            for artifact in "${MISSING_ARTIFACTS[@]}"; do
              echo "  - $artifact"
            done
            exit 1
          fi

          echo "✓ All PyApp artifacts verified for platform $PLATFORM (run $RUN_ID)"
          echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT

      - name: Configure binary deploy mode
        if: matrix.deploy_mode == 'binary'
        shell: bash
        run: |
          # Set environment variables for CLI to auto-download binaries from artifacts
          echo "LF_DEPLOY_MODE=binary" >> $GITHUB_ENV
          echo "LF_BINARY_SOURCE=artifact" >> $GITHUB_ENV
          echo "LF_ARTIFACT_RUN_ID=${{ steps.check-artifacts.outputs.run_id }}" >> $GITHUB_ENV
          echo "GITHUB_TOKEN=${{ github.token }}" >> $GITHUB_ENV

          echo "✓ Binary deploy mode configured"
          echo "  LF_DEPLOY_MODE=binary"
          echo "  LF_BINARY_SOURCE=artifact"
          echo "  LF_ARTIFACT_RUN_ID=${{ steps.check-artifacts.outputs.run_id }}"
          echo ""
          echo "The CLI will auto-download binaries from GitHub Actions artifacts on first use."

      # ============================================================
      # Project Initialization
      # ============================================================
      - name: Create test directory
        shell: bash
        run: |
          TEST_DIR="${{ runner.temp }}/e2e-test"
          mkdir -p "$TEST_DIR"
          echo "TEST_DIR=$TEST_DIR" >> $GITHUB_ENV
          echo "✓ Test directory created at $TEST_DIR"

      - name: Initialize project with CLI
        working-directory: cli
        shell: bash
        run: |
          if ! ./${{ matrix.binary }} init --cwd "$TEST_DIR" --debug; then
            echo "❌ Project initialization failed, checking server health..."
            curl -s http://localhost:14345/health | jq '.' || echo "Server not reachable"
            exit 1
          fi
          echo "✓ Project initialized"

      - name: Verify project config was created
        shell: bash
        run: |
          if [ ! -f "$TEST_DIR/llamafarm.yaml" ]; then
            echo "❌ Error: llamafarm.yaml not created"
            exit 1
          fi
          echo "✓ Project configuration exists"
          echo ""
          echo "=== FULL CONFIG AFTER INIT ==="
          cat "$TEST_DIR/llamafarm.yaml"
          echo ""
          echo "=== Checking critical fields ==="
          echo "namespace: $(grep '^namespace:' "$TEST_DIR/llamafarm.yaml" || echo 'MISSING')"
          echo "name: $(grep '^name:' "$TEST_DIR/llamafarm.yaml" || echo 'MISSING')"
          echo "prompts.name: $(grep -A2 '^prompts:' "$TEST_DIR/llamafarm.yaml" | grep 'name:' | head -1 || echo 'MISSING')"

      - name: Switch to tiny models for fast CI testing
        shell: bash
        run: |
          echo "Switching to tiny models for faster CI execution..."
          # Install yq if not available
          if ! command -v yq &> /dev/null; then
            echo "Installing yq..."
            if [ "$RUNNER_OS" = "macOS" ]; then
              brew install yq
            elif [ "$RUNNER_OS" = "Windows" ]; then
              choco install yq -y
            else
              sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
              sudo chmod +x /usr/local/bin/yq
            fi
          fi

          # Update runtime model, quantization, and context window
          yq -i '.runtime.models[0].model = "unsloth/Qwen3-0.6B-GGUF:IQ1_S"' "$TEST_DIR/llamafarm.yaml"
          yq -i '.runtime.models[0].extra_body.n_ctx = 512' "$TEST_DIR/llamafarm.yaml"

          # Update embedding model and dimension
          yq -i '.rag.databases[0].embedding_strategies[0].config.model = "sentence-transformers/all-MiniLM-L6-v2"' "$TEST_DIR/llamafarm.yaml"
          yq -i '.rag.databases[0].embedding_strategies[0].config.dimension = 384' "$TEST_DIR/llamafarm.yaml"

          echo "✓ Model configuration updated"
          echo ""
          echo "=== Updated runtime config ==="
          yq '.runtime.models[0]' "$TEST_DIR/llamafarm.yaml"
          echo ""
          echo "=== Updated embedding config ==="
          yq '.rag.databases[0].embedding_strategies[0].config | pick(["model", "dimension"])' "$TEST_DIR/llamafarm.yaml"

          # Verify the changes took effect
          RUNTIME_MODEL=$(yq '.runtime.models[0].model' "$TEST_DIR/llamafarm.yaml")
          if [ "$RUNTIME_MODEL" != "unsloth/Qwen3-0.6B-GGUF:IQ1_S" ]; then
            echo "❌ Model configuration update failed!"
            echo "Expected: unsloth/Qwen3-0.6B-GGUF:IQ1_S"
            echo "Got: $RUNTIME_MODEL"
            exit 1
          fi
          echo "✓ Model configuration verified"

      - name: Pre-warm models by loading them
        working-directory: cli
        shell: bash
        timeout-minutes: 8
        env:
          PYTHONUNBUFFERED: "1"
          HF_HUB_ENABLE_HF_TRANSFER: "1" # Use faster download method
        run: |
          echo "Pre-warming models (downloading and loading)..."
          echo "Model: unsloth/Qwen3-0.6B-GGUF (IQ1_S quantization)"
          echo "Expected size: ~50-100MB"
          echo ""

          # Use a simple query to trigger model loading with retry logic.
          # The runtime may crash (SIGSEGV) on first load due to llama.cpp
          # native code issues on CI runners; a retry gives it a second chance
          # after the CLI auto-restarts the process.
          MAX_ATTEMPTS=2
          for attempt in $(seq 1 $MAX_ATTEMPTS); do
            echo "Pre-warm attempt $attempt/$MAX_ATTEMPTS..."
            START_TIME=$(date +%s)
            if ./${{ matrix.binary }} --cwd "$TEST_DIR" --debug chat --no-rag "hi" 2>&1 | tee /tmp/prewarm.log; then
              END_TIME=$(date +%s)
              DURATION=$((END_TIME - START_TIME))
              echo ""

              # Check for actual system errors (not model quality issues)
              RESPONSE=$(cat /tmp/prewarm.log)
              if echo "$RESPONSE" | grep -qi "error loading model\|failed to load model\|no backends are loaded"; then
                echo "⚠ Model loading error detected in response (attempt $attempt)"
                if [ "$attempt" -lt "$MAX_ATTEMPTS" ]; then
                  echo "Waiting 10s before retry..."
                  sleep 10
                  continue
                fi
                echo "❌ Model pre-warming failed after $MAX_ATTEMPTS attempts!"
                echo ""
                echo "Full response:"
                echo "$RESPONSE"
                exit 1
              fi

              echo "✓ Models successfully pre-warmed in ${DURATION}s"
              break
            else
              echo ""
              echo "⚠ Pre-warm attempt $attempt failed (exit code $?)"
              if [ "$attempt" -lt "$MAX_ATTEMPTS" ]; then
                echo "Checking server health before retry..."
                curl -s http://localhost:14345/health | jq '.' || echo "Server not reachable"
                echo "Waiting 10s for services to recover..."
                sleep 10
                continue
              fi
              echo "❌ Model pre-warming failed after $MAX_ATTEMPTS attempts!"
              echo "Checking server health..."
              curl -s http://localhost:14345/health | jq '.' || echo "Server not reachable"
              echo ""
              echo "Last 50 lines of output:"
              tail -50 /tmp/prewarm.log
              exit 1
            fi
          done

      # ============================================================
      # Pre-warm Embedding Model
      # ============================================================
      - name: Pre-warm embedding model
        working-directory: cli
        shell: bash
        timeout-minutes: 12
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          echo "Pre-warming embedding model (sentence-transformers/all-MiniLM-L6-v2)..."
          echo "Sending a dummy embedding request to trigger model loading..."

          # The universal runtime may have crashed during chat pre-warming (SIGSEGV
          # in llama.cpp native code). The CLI restarts unhealthy services when you
          # run a command, so we use `lf chat` to trigger a restart before retrying.
          # HTTP 500 is expected while the embedding model is downloading/loading.
          # On Windows binary, a PyTorch assertion error in torch._dynamo causes
          # repeated 500s until the import eventually succeeds.
          MAX_RETRIES=120
          RETRY_DELAY=5
          RESTART_COUNT=0
          MAX_RESTARTS=3
          CONSECUTIVE_DEAD=0

          for i in $(seq 1 $MAX_RETRIES); do
            HTTP_CODE=$(curl -s -o /tmp/embed_response.json -w "%{http_code}" \
              -X POST http://localhost:11540/v1/embeddings \
              -H "Content-Type: application/json" \
              -d '{"model": "sentence-transformers/all-MiniLM-L6-v2", "input": "warm up"}' \
              2>/dev/null) || HTTP_CODE="000"

            if [ "$HTTP_CODE" = "200" ]; then
              echo "✓ Embedding model loaded and responding (HTTP $HTTP_CODE) on attempt $i"
              exit 0
            fi

            # Track consecutive dead responses to trigger restart
            if [ "$HTTP_CODE" = "000" ]; then
              CONSECUTIVE_DEAD=$((CONSECUTIVE_DEAD + 1))
            else
              CONSECUTIVE_DEAD=0
            fi

            # If runtime is dead for 5+ consecutive checks, restart via CLI
            if [ "$CONSECUTIVE_DEAD" -ge 5 ] && [ "$RESTART_COUNT" -lt "$MAX_RESTARTS" ]; then
              RESTART_COUNT=$((RESTART_COUNT + 1))
              echo "  Runtime dead for $CONSECUTIVE_DEAD checks, restarting via CLI (restart $RESTART_COUNT/$MAX_RESTARTS)..."
              ./${{ matrix.binary }} --cwd "$TEST_DIR" --debug chat --no-rag "ping" 2>&1 | tail -5 || true
              CONSECUTIVE_DEAD=0
              echo "  Waiting for runtime to initialize..."
              sleep 20
              continue
            fi

            if [ "$i" -eq "$MAX_RETRIES" ]; then
              echo "❌ Embedding model failed to respond after $MAX_RETRIES attempts (last HTTP $HTTP_CODE)"
              echo "Restarts attempted: $RESTART_COUNT"
              echo "Checking server health..."
              curl -s http://localhost:14345/health | jq '.' 2>/dev/null || echo "Server not reachable"
              cat /tmp/embed_response.json 2>/dev/null || true
              exit 1
            fi

            echo "  Attempt $i/$MAX_RETRIES: HTTP $HTTP_CODE, retrying in ${RETRY_DELAY}s..."
            sleep $RETRY_DELAY
          done

      # ============================================================
      # Verify llama.cpp Binary Download (critical for GGUF models)
      # ============================================================
      - name: Verify llama.cpp binary download
        shell: bash
        run: |
          echo "Verifying llama.cpp binary installation..."

          # Determine cache directory based on OS
          if [ "$RUNNER_OS" = "macOS" ]; then
            LLAMA_CACHE="$HOME/Library/Caches/llamafarm-llama"
            LIB_NAME="libllama.dylib"
          elif [ "$RUNNER_OS" = "Windows" ]; then
            # Convert Windows path to Unix-style for bash compatibility
            LLAMA_CACHE="$(cygpath -u "$LOCALAPPDATA")/llamafarm-llama/cache"
            LIB_NAME="llama.dll"
          else
            LLAMA_CACHE="$HOME/.cache/llamafarm-llama"
            LIB_NAME="libllama.so"
          fi

          # Strip both LF and CR to handle Windows CRLF line endings
          LLAMA_VERSION=$(cat llama-cpp-version.txt | tr -d '\r\n')
          LIB_PATH="$LLAMA_CACHE/$LLAMA_VERSION/$LIB_NAME"

          echo "Expected binary location: $LIB_PATH"
          echo ""
          echo "Cache directory contents:"
          ls -la "$LLAMA_CACHE/$LLAMA_VERSION/" 2>/dev/null || echo "Cache directory does not exist"
          echo ""

          # Check if binary exists (could be a file or symlink)
          if [ ! -e "$LIB_PATH" ]; then
            echo "❌ llama.cpp binary not found at $LIB_PATH"
            exit 1
          fi

          # Check if it's a symlink and follow it
          ACTUAL_PATH="$LIB_PATH"
          if [ -L "$LIB_PATH" ]; then
            echo "Found symlink: $LIB_NAME -> $(readlink "$LIB_PATH")"
            # Follow symlink chain to find actual file
            ACTUAL_PATH=$(readlink -f "$LIB_PATH" 2>/dev/null || python3 -c "import os; print(os.path.realpath('$LIB_PATH'))")
            echo "Resolved to: $ACTUAL_PATH"
          fi

          # Verify the actual file exists and has reasonable size
          if [ ! -f "$ACTUAL_PATH" ]; then
            echo "❌ Resolved path $ACTUAL_PATH is not a file"
            exit 1
          fi

          # Check file size (should be > 1MB for a real library)
          # Use -L flag with stat to follow symlinks
          FILE_SIZE=$(stat -f%z "$ACTUAL_PATH" 2>/dev/null || stat -c%s "$ACTUAL_PATH" 2>/dev/null)
          echo "Binary size: $FILE_SIZE bytes"

          if [ "$FILE_SIZE" -lt 1000000 ]; then
            echo "❌ Binary file is too small ($FILE_SIZE bytes), may be corrupted"
            echo "File contents (if text):"
            head -c 200 "$ACTUAL_PATH" 2>/dev/null || true
            exit 1
          fi

          # On macOS, verify it's a valid Mach-O binary and check dependencies
          if [ "$RUNNER_OS" = "macOS" ]; then
            FILE_TYPE=$(file "$ACTUAL_PATH")
            echo "File type: $FILE_TYPE"
            if echo "$FILE_TYPE" | grep -q "Mach-O"; then
              echo "✓ Valid Mach-O binary"
            else
              echo "⚠ Warning: Not recognized as Mach-O binary"
            fi

            echo ""
            echo "Checking macOS dependencies..."
            CACHE_DIR="$LLAMA_CACHE/$LLAMA_VERSION"
            DYLIB_COUNT=$(find "$CACHE_DIR" -maxdepth 1 -name "*.dylib" 2>/dev/null | wc -l || echo "0")
            echo "Found $DYLIB_COUNT .dylib files in cache:"
            ls -la "$CACHE_DIR"/*.dylib 2>/dev/null || echo "Could not list .dylib files"

            # Check for common dependencies
            for dep in libggml.dylib libggml.0.dylib libggml-base.dylib libggml-cpu.dylib; do
              if [ -e "$CACHE_DIR/$dep" ]; then
                echo "  ✓ $dep found"
              else
                echo "  ⚠ $dep not found"
              fi
            done
          fi

          # On Linux, verify it's a valid ELF binary and check dependencies
          if [ "$RUNNER_OS" = "Linux" ]; then
            FILE_TYPE=$(file "$ACTUAL_PATH")
            echo "File type: $FILE_TYPE"
            if echo "$FILE_TYPE" | grep -q "ELF"; then
              echo "✓ Valid ELF binary"
            else
              echo "⚠ Warning: Not recognized as ELF binary"
            fi

            echo ""
            echo "Checking Linux dependencies..."
            CACHE_DIR="$LLAMA_CACHE/$LLAMA_VERSION"
            SO_COUNT=$(find "$CACHE_DIR" -maxdepth 1 -name "*.so*" 2>/dev/null | wc -l || echo "0")
            echo "Found $SO_COUNT .so files in cache:"
            ls -la "$CACHE_DIR"/*.so* 2>/dev/null || echo "Could not list .so files"

            # Check for common dependencies
            for dep in libggml.so libggml.so.0 libggml-base.so libggml-cpu.so; do
              if [ -e "$CACHE_DIR/$dep" ]; then
                echo "  ✓ $dep found"
              else
                echo "  ⚠ $dep not found"
              fi
            done
          fi

          # On Windows, verify dependencies are present
          if [ "$RUNNER_OS" = "Windows" ]; then
            echo ""
            echo "Checking Windows dependencies..."
            CACHE_DIR="$LLAMA_CACHE/$LLAMA_VERSION"
            DLL_COUNT=$(find "$CACHE_DIR" -maxdepth 1 -name "*.dll" 2>/dev/null | wc -l || ls "$CACHE_DIR"/*.dll 2>/dev/null | wc -l || echo "0")
            echo "Found $DLL_COUNT DLL files in cache:"
            ls -la "$CACHE_DIR"/*.dll 2>/dev/null || dir "$CACHE_DIR\\*.dll" 2>/dev/null || echo "Could not list DLLs"

            # Check for common dependencies
            for dep in ggml.dll ggml-base.dll ggml-cpu.dll; do
              if [ -f "$CACHE_DIR/$dep" ]; then
                echo "  ✓ $dep found"
              fi
            done
          fi

          echo ""
          echo "✓ llama.cpp binary verified successfully"
          echo "  Version: $LLAMA_VERSION"
          echo "  Path: $LIB_PATH"
          if [ "$ACTUAL_PATH" != "$LIB_PATH" ]; then
            echo "  Actual: $ACTUAL_PATH"
          fi
          echo "  Size: $FILE_SIZE bytes"

      # ============================================================
      # Data Ingestion Test
      # ============================================================
      - name: Create dataset
        working-directory: cli
        shell: bash
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          if ! ./${{ matrix.binary }} --cwd "$TEST_DIR" --debug datasets create \
            -s universal_rag \
            -b main_database \
            test_dataset; then
            echo "❌ Dataset creation failed, checking server health..."
            curl -s http://localhost:14345/health | jq '.' || echo "Server not reachable"
            exit 1
          fi
          echo "✓ Dataset created"

      - name: Wait for config sync after dataset create
        shell: bash
        run: sleep 2.5

      - name: Upload sample files
        working-directory: cli
        shell: bash
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          if ! ./${{ matrix.binary }} --cwd "$TEST_DIR" --debug datasets upload test_dataset "../examples/quick_rag/files"; then
            echo "❌ Upload failed for ../examples/quick_rag/files, checking server health..."
            echo "Current directory: $(pwd)"
            curl -s http://localhost:14345/health | jq '.' || echo "Server not reachable"
            exit 1
          fi
          echo "✓ All sample files uploaded"

      - name: Wait for config sync after dataset upload
        shell: bash
        run: sleep 2.5

      - name: Process dataset
        working-directory: cli
        shell: bash
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          sleep 2
          if ! ./${{ matrix.binary }} --cwd "$TEST_DIR" --debug datasets process test_dataset; then
            echo "❌ Dataset processing failed, checking server health..."
            curl -s http://localhost:14345/health | jq '.' || echo "Server not reachable"
            exit 1
          fi
          echo "✓ Dataset processing complete"

      # ============================================================
      # RAG Query Test
      # ============================================================
      - name: Run RAG query
        working-directory: cli
        shell: bash
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          echo "Running RAG query..."
          if ! OUTPUT=$(./${{ matrix.binary }} --cwd "$TEST_DIR" --debug rag query \
            --database main_database \
            --top-k 3 \
            "neural scaling laws" 2>&1); then
            echo "❌ RAG query command failed"
            echo "$OUTPUT"
            echo "Checking server health..."
            curl -s http://localhost:14345/health | jq '.' || echo "Server not reachable"
            exit 1
          fi

          echo "$OUTPUT"

          # Verify output is not empty
          if [ -z "$OUTPUT" ]; then
            echo "❌ Error: RAG query returned empty output"
            curl -s http://localhost:14345/health | jq '.' || echo "Server not reachable"
            exit 1
          fi

          echo "✓ RAG query returned results"

      - name: Validate RAG query results
        working-directory: cli
        shell: bash
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          if ! OUTPUT=$(./${{ matrix.binary }} --cwd "$TEST_DIR" --debug rag query \
            --database main_database \
            --top-k 3 \
            --include-metadata \
            "neural scaling laws" 2>&1); then
            echo "❌ RAG query validation failed"
            echo "$OUTPUT"
            echo "Checking server health..."
            curl -s http://localhost:14345/health | jq '.' || echo "Server not reachable"
            exit 1
          fi

          # Check if output contains expected content
          if echo "$OUTPUT" | grep -q "neural"; then
            echo "✓ RAG query results contain relevant content"
          else
            echo "⚠ Warning: Results may not contain expected content"
            echo "$OUTPUT"
          fi

      # ============================================================
      # Chat Command Test
      # ============================================================
      - name: Run chat command with RAG
        working-directory: cli
        shell: bash
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          echo "Running chat command with RAG..."
          # Retry once if the runtime crashed and needs to restart
          for attempt in 1 2; do
            if OUTPUT=$(./${{ matrix.binary }} --cwd "$TEST_DIR" --debug chat \
              "Summarize neural scaling laws in one sentence" 2>&1); then
              break
            fi
            echo "⚠ Chat attempt $attempt failed, waiting for runtime to recover..."
            sleep 10
            if [ "$attempt" -eq 2 ]; then
              echo "❌ Chat command failed after retries"
              echo "$OUTPUT"
              curl -s http://localhost:14345/health | jq '.' || echo "Server not reachable"
              exit 1
            fi
          done

          echo "Chat response:"
          echo "$OUTPUT"

          # Verify output is not empty and has some substance
          if [ -z "$OUTPUT" ] || [ ${#OUTPUT} -lt 10 ]; then
            echo "❌ Error: Chat response empty or too short"
            curl -s http://localhost:14345/health | jq '.' || echo "Server not reachable"
            exit 1
          fi

          echo "✓ Chat command returned a response"

      - name: Test chat command without RAG
        working-directory: cli
        shell: bash
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          echo "Running chat command without RAG..."
          # Retry once if the runtime crashed and needs to restart
          for attempt in 1 2; do
            if OUTPUT=$(./${{ matrix.binary }} --cwd "$TEST_DIR" --debug chat \
              --no-rag \
              "What is 2 + 2?" 2>&1); then
              break
            fi
            echo "⚠ Chat (no RAG) attempt $attempt failed, waiting for runtime to recover..."
            sleep 10
            if [ "$attempt" -eq 2 ]; then
              echo "❌ Chat command (no RAG) failed after retries"
              echo "$OUTPUT"
              curl -s http://localhost:14345/health | jq '.' || echo "Server not reachable"
              exit 1
            fi
          done

          echo "Chat response:"
          echo "$OUTPUT"

          # Verify output is not empty
          if [ -z "$OUTPUT" ]; then
            echo "❌ Error: Chat command (no RAG) returned empty output"
            curl -s http://localhost:14345/health | jq '.' || echo "Server not reachable"
            exit 1
          fi

          echo "✓ Chat command without RAG works"

      # ============================================================
      # Validation & Summary
      # ============================================================
      - name: List datasets to verify persistence
        working-directory: cli
        shell: bash
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          echo "Listing datasets..."
          if ! ./${{ matrix.binary }} --cwd "$TEST_DIR" --debug datasets list; then
            echo "❌ Dataset listing failed, checking server health..."
            curl -s http://localhost:14345/health | jq '.' || echo "Server not reachable"
            exit 1
          fi
          echo "✓ Dataset listing works"

      - name: Check RAG health
        working-directory: cli
        shell: bash
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          echo "Checking RAG health..."
          ./${{ matrix.binary }} --cwd "$TEST_DIR" --debug rag health || true
          echo "✓ RAG health check complete"

      - name: Verify designer accessibility (binary mode only)
        if: matrix.deploy_mode == 'binary'
        shell: bash
        run: |
          echo "Verifying designer is accessible from server..."

          # Request the root path which should serve the designer
          HTTP_CODE=$(curl -s -o /tmp/designer_response.html -w "%{http_code}" http://localhost:14345/)

          if [ "$HTTP_CODE" != "200" ]; then
            echo "❌ Designer endpoint returned HTTP $HTTP_CODE (expected 200)"
            echo "Response content:"
            cat /tmp/designer_response.html 2>/dev/null || echo "(empty)"
            echo ""
            echo "Server health:"
            curl -s http://localhost:14345/health | jq '.' || echo "Server not reachable"
            exit 1
          fi

          # Check if response contains designer HTML content
          if ! grep -q "<!doctype html>" /tmp/designer_response.html; then
            echo "❌ Designer response doesn't contain expected HTML content"
            echo "Response preview (first 500 chars):"
            head -c 500 /tmp/designer_response.html
            exit 1
          fi

          # Verify it's the designer by checking for React root div
          if ! grep -q 'id="root"' /tmp/designer_response.html; then
            echo "❌ Designer HTML missing React root element"
            echo "Response preview (first 500 chars):"
            head -c 500 /tmp/designer_response.html
            exit 1
          fi

          echo "✓ Designer is accessible and serving expected content"

      - name: E2E test summary
        shell: bash
        run: |
          echo "=========================================="
          echo "End-to-End Test Summary (${{ matrix.os }}, ${{ matrix.deploy_mode }})"
          echo "=========================================="
          echo "✓ Deploy mode: ${{ matrix.deploy_mode }}"
          echo "✓ CLI binary downloaded and verified"
          echo "✓ Project initialized"
          echo "✓ llama.cpp binary downloaded and verified"
          echo "✓ GGUF model loaded successfully"
          echo "✓ Dataset created and processed"
          echo "✓ RAG queries executed"
          echo "✓ Chat commands completed"
          if [ "${{ matrix.deploy_mode }}" = "binary" ]; then
            echo "✓ Designer accessibility verified"
          fi
          echo "=========================================="
          echo "All E2E tests passed!"

      # ============================================================
      # Save Results for Aggregation
      # ============================================================
      - name: Save test result for aggregation
        if: always()
        shell: bash
        run: |
          mkdir -p e2e-results
          echo "${{ job.status }}" > e2e-results/${{ matrix.os }}-${{ matrix.deploy_mode }}.txt
          echo "Saved result: ${{ job.status }} for ${{ matrix.os }} (${{ matrix.deploy_mode }})"

      - name: Upload test result artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-result-${{ matrix.os }}-${{ matrix.deploy_mode }}
          path: e2e-results/
          retention-days: 1

      # ============================================================
      # Cleanup
      # ============================================================
      - name: Cleanup test artifacts
        if: always()
        shell: bash
        run: |
          echo "Cleaning up test directory..."
          rm -rf "$TEST_DIR"
          echo "✓ Cleanup complete"

      - name: Display service logs on failure
        if: failure()
        shell: bash
        run: |
          echo "=========================================="
          echo "Service Logs (${{ matrix.os }})"
          echo "=========================================="

          # Determine home directory (cross-platform)
          if [ "$RUNNER_OS" = "Windows" ]; then
            LF_HOME="$USERPROFILE/.llamafarm"
          else
            LF_HOME="$HOME/.llamafarm"
          fi

          # Display server logs if they exist
          if [ -f "$LF_HOME/logs/server.log" ]; then
            echo "--- Server Logs ---"
            tail -100 "$LF_HOME/logs/server.log"
          else
            echo "No server logs found at $LF_HOME/logs/server.log"
          fi

          echo ""

          # Display RAG logs if they exist
          if [ -f "$LF_HOME/logs/rag.log" ]; then
            echo "--- RAG Worker Logs ---"
            tail -100 "$LF_HOME/logs/rag.log"
          else
            echo "No RAG logs found at $LF_HOME/logs/rag.log"
          fi

          echo ""

          # Display Universal Runtime logs if they exist
          if [ -f "$LF_HOME/logs/universal-runtime.log" ]; then
            echo "--- Universal Runtime Logs ---"
            tail -100 "$LF_HOME/logs/universal-runtime.log"
          else
            echo "No Universal Runtime logs found at $LF_HOME/logs/universal-runtime.log"
          fi

          echo ""

          # Display llama.cpp cache directory status (for debugging binary download issues)
          echo "--- llama.cpp Binary Cache Status ---"
          if [ "$RUNNER_OS" = "macOS" ]; then
            LLAMA_CACHE="$HOME/Library/Caches/llamafarm-llama"
          elif [ "$RUNNER_OS" = "Windows" ]; then
            # Convert Windows path to Unix-style for bash compatibility
            LLAMA_CACHE="$(cygpath -u "$LOCALAPPDATA")/llamafarm-llama/cache"
          else
            LLAMA_CACHE="$HOME/.cache/llamafarm-llama"
          fi

          echo "Cache directory: $LLAMA_CACHE"
          if [ -d "$LLAMA_CACHE" ]; then
            echo "Contents:"
            ls -laR "$LLAMA_CACHE" 2>/dev/null || dir /s "$LLAMA_CACHE" 2>/dev/null || echo "Could not list cache contents"
          else
            echo "Cache directory does not exist - binary may not have been downloaded"
          fi

      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-test-logs-${{ matrix.os }}-${{ matrix.deploy_mode }}
          path: |
            ~/.llamafarm/logs/
            ~/Library/Caches/llamafarm-llama/
            ~/.cache/llamafarm-llama/
          retention-days: 7
          if-no-files-found: warn

  # ============================================================
  # Aggregated PR Comment Job
  # Note: This job is skipped for fork PRs due to GitHub's security model
  # which limits GITHUB_TOKEN to read-only for workflows triggered by forks.
  # ============================================================
  report-results:
    name: Report E2E Results
    needs: e2e-test
    # Only run for non-fork PRs (fork PRs have head.repo.fork = true)
    if: always() && github.event_name == 'pull_request' && github.event.pull_request.head.repo.fork == false
    runs-on: ubuntu-latest
    steps:
      - name: Download all result artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: e2e-result-*
          path: results/
          merge-multiple: false

      - name: Get PR info
        id: pr-info
        run: |
          echo "pr_number=${{ github.event.pull_request.number }}" >> $GITHUB_OUTPUT
          echo "head_sha=${{ github.event.pull_request.head.sha }}" >> $GITHUB_OUTPUT

      - name: Build aggregated results summary
        id: build-summary
        env:
          HEAD_SHA: ${{ steps.pr-info.outputs.head_sha }}
        run: |
          WORKFLOW_RUN_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

          # Initialize counters
          TOTAL=0
          PASSED=0
          FAILED=0
          CANCELLED=0

          # Build results table
          RESULTS_TABLE="| OS | Mode | Status |\n|---|---|---|\n"

          for os in ubuntu-latest macos-latest windows-latest; do
            for mode in source binary; do
              RESULT_FILE="results/e2e-result-${os}-${mode}/${os}-${mode}.txt"
              if [ -f "$RESULT_FILE" ]; then
                STATUS=$(cat "$RESULT_FILE")
                TOTAL=$((TOTAL + 1))
                case "$STATUS" in
                  success)
                    PASSED=$((PASSED + 1))
                    RESULTS_TABLE="${RESULTS_TABLE}| ${os} | ${mode} | :white_check_mark: Passed |\n"
                    ;;
                  failure)
                    FAILED=$((FAILED + 1))
                    RESULTS_TABLE="${RESULTS_TABLE}| ${os} | ${mode} | :x: Failed |\n"
                    ;;
                  cancelled)
                    CANCELLED=$((CANCELLED + 1))
                    RESULTS_TABLE="${RESULTS_TABLE}| ${os} | ${mode} | :warning: Cancelled |\n"
                    ;;
                  *)
                    RESULTS_TABLE="${RESULTS_TABLE}| ${os} | ${mode} | :question: ${STATUS} |\n"
                    ;;
                esac
              else
                echo "Warning: No result file found for ${os}-${mode}"
                RESULTS_TABLE="${RESULTS_TABLE}| ${os} | ${mode} | :grey_question: No result |\n"
              fi
            done
          done

          # Determine overall status
          if [ "$FAILED" -gt 0 ]; then
            STATUS_EMOJI=":x:"
            STATUS_TEXT="**E2E Tests Failed**"
          elif [ "$CANCELLED" -gt 0 ]; then
            STATUS_EMOJI=":warning:"
            STATUS_TEXT="**E2E Tests Cancelled**"
          elif [ "$PASSED" -eq "$TOTAL" ] && [ "$TOTAL" -gt 0 ]; then
            STATUS_EMOJI=":white_check_mark:"
            STATUS_TEXT="**All E2E Tests Passed!**"
          else
            STATUS_EMOJI=":grey_question:"
            STATUS_TEXT="**E2E Tests completed with unknown status**"
          fi

          # Build the comment body and save to file
          {
            echo "## ${STATUS_EMOJI} ${STATUS_TEXT}"
            echo ""
            echo "### Test Results by Platform"
            echo ""
            echo -e "$RESULTS_TABLE"
            echo ""
            echo "### Summary"
            echo ""
            echo "- **Passed**: ${PASSED}/${TOTAL}"
            if [ "$FAILED" -gt 0 ]; then
              echo "- **Failed**: ${FAILED}"
            fi
            if [ "$CANCELLED" -gt 0 ]; then
              echo "- **Cancelled**: ${CANCELLED}"
            fi
            echo "- **Workflow Run**: [View Details](${WORKFLOW_RUN_URL})"
            echo "- **Commit**: \`${HEAD_SHA:0:7}\`"
            echo ""
            echo "---"
            echo "*This comment was automatically generated by the E2E Tests workflow.*"
          } > comment-body.txt

          echo "Comment body prepared:"
          cat comment-body.txt

      - name: Find existing comment
        id: find-comment
        uses: actions/github-script@v7
        env:
          PR_NUMBER: ${{ steps.pr-info.outputs.pr_number }}
        with:
          script: |
            const prNumber = process.env.PR_NUMBER;
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
            });

            // Find comment from this bot (contains the marker)
            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('*This comment was automatically generated by the E2E Tests workflow.*')
            );

            if (botComment) {
              core.setOutput('comment_id', botComment.id);
              core.setOutput('exists', 'true');
              console.log(`Found existing comment: ${botComment.id}`);
            } else {
              core.setOutput('exists', 'false');
              console.log('No existing comment found');
            }

      - name: Post or update PR comment
        uses: actions/github-script@v7
        env:
          PR_NUMBER: ${{ steps.pr-info.outputs.pr_number }}
        with:
          retries: 3
          script: |
            const fs = require('fs');
            const prNumber = process.env.PR_NUMBER;

            if (!fs.existsSync('comment-body.txt')) {
              console.log('Comment file not found');
              return;
            }

            const commentBody = fs.readFileSync('comment-body.txt', 'utf8');
            const commentExists = '${{ steps.find-comment.outputs.exists }}' === 'true';
            const existingCommentId = '${{ steps.find-comment.outputs.comment_id }}';

            if (commentExists && existingCommentId) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingCommentId,
                body: commentBody
              });
              console.log(`Updated existing comment #${existingCommentId} on PR #${prNumber}`);
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: prNumber,
                body: commentBody
              });
              console.log(`Created new comment on PR #${prNumber}`);
            }
