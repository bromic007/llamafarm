[project]
name = "transformers-runtime"
version = "1.0.0"
description = "OpenAI-compatible API for HuggingFace models"
readme = "README.md"
requires-python = ">=3.11,<3.14"
dependencies = [
  "torch>=2.0.0",
  "transformers>=4.35.0",
  "diffusers>=0.25.0",
  "accelerate>=0.25.0",
  "fastapi>=0.104.0",
  "uvicorn[standard]>=0.24.0",
  "pydantic>=2.0.0",
  "pillow>=10.0.0",
  "python-multipart",
  "einops>=0.8.1",
  "openai>=1.0.0",
  "llamafarm-common",
  "structlog>=24.0.0",
]

[dependency-groups]
dev = [
  "ruff>=0.14.3",
  "pytest>=7.0.0",
  "pytest-asyncio>=0.21.0",
  "pytest-cov>=4.0.0",
  "pytest-timeout>=2.1.0",
  "httpx>=0.24.0",
  "numpy>=1.24.0",
]

[project.optional-dependencies]
xformers = ["xformers>=0.0.22"]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["models", "utils", "core"]
py-modules = ["server", "download_model"]

[tool.uv]
# PyTorch variant selection:
# The LlamaFarm CLI automatically detects your hardware (CUDA, Metal, ROCm, or CPU)
# and sets UV_EXTRA_INDEX_URL appropriately to install the optimal PyTorch build.
#
# Note: UV_EXTRA_INDEX_URL is only set for the universal-runtime service, not for
# server/rag/config/common. This prevents dependency resolution issues where the
# PyTorch index contains incomplete/incompatible versions of common packages.
#
# Manual override options:
#   UV_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cpu   # Force CPU-only
#   UV_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu121 # Force CUDA 12.1
#   UV_EXTRA_INDEX_URL=https://download.pytorch.org/whl/rocm6.0 # Force ROCm 6.0
#
# Hardware detection priority:
#   1. CUDA (NVIDIA GPUs) - uses default PyPI with GPU support (~3.4GB)
#   2. Metal (Apple Silicon) - uses default PyPI with Metal support
#   3. ROCm (AMD GPUs) - uses ROCm index
#   4. CPU (fallback) - uses CPU-only index (~100MB)
index-url = "https://pypi.org/simple"

[tool.uv.sources]
llamafarm-common = { path = "../../common", editable = true }

# Ruff configuration is shared across all Python components
# See ../../ruff.toml for the actual configuration

[tool.ruff]
# Import shared configuration
extend = "../../ruff.toml"
