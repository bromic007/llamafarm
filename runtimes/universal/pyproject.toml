[project]
name = "universal-runtime"
version = "1.0.0"
description = "OpenAI-compatible API for HuggingFace models"
readme = "README.md"
requires-python = ">=3.11,<3.14"
dependencies = [
  "transformers>=4.35.0",
  "diffusers>=0.25.0",
  "accelerate>=0.25.0",
  "fastapi>=0.104.0",
  "uvicorn[standard]>=0.24.0",
  "pydantic>=2.0.0",
  "pillow>=10.0.0",
  "python-multipart",
  "einops>=0.8.1",
  "openai>=1.0.0",
  "llamafarm-common",
  "structlog>=24.0.0",
  "psutil>=5.9.0",
  "pyyaml>=6.0",
  "gguf>=0.17.1",
  # hf-transfer is now provided by llamafarm-common
]

[dependency-groups]
dev = [
  "ruff>=0.14.3",
  "pytest>=7.0.0",
  "pytest-asyncio>=0.21.0",
  "pytest-cov>=4.0.0",
  "pytest-timeout>=2.1.0",
  "httpx>=0.24.0",
  "numpy>=1.24.0",
  # For testing: torch and llama-cpp-python will use CPU wheels via UV_EXTRA_INDEX_URL in CI
  "torch>=2.0.0",
  "llama-cpp-python>=0.3.0",
]

[project.optional-dependencies]
# Hardware-accelerated packages installed separately via CLI based on detected hardware
# These are not installed during 'uv sync' to avoid compilation dependencies
# Note: torch and llama-cpp-python are now in dev group for testing
gpu = ["torch>=2.0.0", "llama-cpp-python>=0.3.0"]
# xformers is GPU-only and must be installed manually when needed:
#   uv pip install xformers
# (Removed from extras to avoid lockfile conflicts with CPU-only PyTorch index)

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["models", "utils", "core"]
py-modules = ["server", "download_model"]

[tool.uv]
# PyTorch variant selection:
# The LlamaFarm CLI automatically detects your hardware (CUDA, Metal, ROCm, or CPU)
# and sets UV_EXTRA_INDEX_URL appropriately to install the optimal PyTorch build.
#
# Note: UV_EXTRA_INDEX_URL is only set for the universal-runtime service, not for
# server/rag/config/common. This prevents dependency resolution issues where the
# PyTorch index contains incomplete/incompatible versions of common packages.
#
# Manual override options:
#   UV_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cpu   # Force CPU-only
#   UV_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu121 # Force CUDA 12.1
#   UV_EXTRA_INDEX_URL=https://download.pytorch.org/whl/rocm6.0 # Force ROCm 6.0
#
# Hardware detection priority:
#   1. CUDA (NVIDIA GPUs) - uses default PyPI with GPU support (~3.4GB)
#   2. Metal (Apple Silicon) - uses default PyPI with Metal support
#   3. ROCm (AMD GPUs) - uses ROCm index
#   4. CPU (fallback) - uses CPU-only index (~100MB)
index-url = "https://pypi.org/simple"

[tool.uv.sources]
llamafarm-common = { path = "../../common", editable = true }

# Ruff configuration is shared across all Python components
# See ../../ruff.toml for the actual configuration

[tool.ruff]
# Import shared configuration
extend = "../../ruff.toml"
