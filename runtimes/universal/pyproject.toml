[project]
name = "universal-runtime"
version = "1.0.0"
description = "OpenAI-compatible API for HuggingFace models"
readme = "README.md"
requires-python = ">=3.10,<3.14"
dependencies = [
  "transformers>=4.35.0",
  "diffusers>=0.25.0",
  "accelerate>=0.25.0",
  "fastapi>=0.104.0",
  "uvicorn[standard]>=0.24.0",
  "pydantic>=2.0.0",
  "pillow>=10.0.0",
  "python-multipart>=0.0.22",
  "einops>=0.8.1",
  "openai>=1.0.0",
  "llamafarm-common",
  "llamafarm-llama",
  "structlog>=24.0.0",
  "psutil>=5.9.0",
  "pyyaml>=6.0",
  "gguf>=0.17.1",
  # hf-transfer is now provided by llamafarm-common
  "pymupdf>=1.26.6", # Required for PDF file uploads
  # Note: easyocr moved to optional [ocr-easyocr] extra to avoid forcing torch install
  # Note: scikit-learn is in optional [anomaly] extra (installed via nx sync)
  "protobuf>=6.33.5",
  "sentencepiece>=0.2.1",
  "cachetools>=6.0.0",    # TTL-based model caching
  "jinja2>=3.0.0",        # For tool-aware chat template rendering
  # Anomaly detection
  "scikit-learn>=1.3.0",
  "pyod>=1.1.0",  # PyOD: 15+ outlier detection algorithms (ECOD, HBOS, KNN, etc.)
  "numba>=0.60.0",  # Explicit pin for Python 3.12 compatibility (pyod -> numba -> llvmlite)
  # High-performance data processing for streaming ML
  "polars>=1.0.0",  # Fast DataFrame library for rolling windows and feature engineering
  # Text classification (few-shot learning with SetFit)
  "setfit>=1.1.0",
  "sentence-transformers>=3.0.0",
  # OCR backends
  "surya-ocr>=0.6.0",
  "easyocr>=1.7.0",
  "opencv-python-headless>=4.8.0",
  "pytesseract>=0.3.10",
  "ultralytics>=8.4.14",
  "onnx>=1.12.0,<2.0.0",
  "onnxslim>=0.1.85",
  "onnxruntime>=1.23.2",
]

[dependency-groups]
dev = [
  "ruff>=0.14.3",
  "pytest>=7.0.0",
  "pytest-asyncio>=0.21.0",
  "pytest-cov>=4.0.0",
  "pytest-timeout>=2.1.0",
  "httpx>=0.24.0",
  "numpy>=1.24.0",
  # For testing: torch will use CPU wheels via UV_EXTRA_INDEX_URL in CI
  "torch>=2.6.0",
]

[project.optional-dependencies]
# Hardware-accelerated packages installed separately via CLI based on detected hardware
# These are not installed during 'uv sync' to avoid compilation dependencies
# Note: torch is now in dev group for testing
gpu = ["torch>=2.6.0"]
# xformers is GPU-only and must be installed manually when needed:
#   uv pip install xformers
# (Removed from extras to avoid lockfile conflicts with CPU-only PyTorch index)

# DEPRECATED: OCR, anomaly, and classification dependencies are now in main dependencies.
# These extras are kept for backwards compatibility but are no longer needed.
# All ML features are available by default without installing extras.

# OCR backends - install the one(s) you need
# Usage: uv pip install "universal-runtime[ocr-surya]"
ocr-surya = ["surya-ocr>=0.6.0"]
ocr-easyocr = ["easyocr>=1.7.0", "opencv-python-headless>=4.8.0"]
ocr-paddleocr = ["paddleocr>=2.7.0"]
ocr-tesseract = ["pytesseract>=0.3.10"]
# Install recommended OCR backends (surya + easyocr + tesseract)
ocr = [
  "surya-ocr>=0.6.0",
  "easyocr>=1.7.0",
  "opencv-python-headless>=4.8.0", # Required by easyocr, explicit to avoid version conflicts
  "pytesseract>=0.3.10",
]

# Document understanding models
document = ["transformers>=4.35.0", "sentencepiece>=0.1.99"]

# Anomaly detection (now in main deps, kept for backwards compatibility)
anomaly = ["scikit-learn>=1.3.0"]

# Text classification (now in main deps, kept for backwards compatibility)
classification = ["setfit>=1.1.0", "sentence-transformers>=3.0.0"]

# Speech-to-text with faster-whisper
# Usage: uv pip install "universal-runtime[speech]"
speech = [
  "faster-whisper>=1.0.0",
  "webrtcvad>=2.0.10",     # Voice activity detection for streaming
  "av>=12.0.0",            # PyAV for efficient compressed audio decoding (WebM/Opus)
]

# Text-to-speech with Kokoro
# Usage: uv pip install "universal-runtime[tts]"
tts = [
  "kokoro>=0.9.4",     # High-quality neural TTS (~82M params)
  "misaki>=0.9.0",     # G2P for English (kokoro dependency) - no [en] extra to avoid conflicts
  "spacy>=3.8.11",     # NLP for G2P
  "pydub>=0.25.0",     # Audio format conversion (MP3, FLAC)
  "av>=12.0.0",        # PyAV for Opus/AAC encoding
  "pocket-tts>=1.0.3",
]

# Text-to-speech with Kokoro MLX (Apple Silicon optimized)
# Native MLX implementation for faster inference on M-series chips
# Usage: uv pip install "universal-runtime[tts-mlx]"
tts-mlx = [
  "mlx-audio>=0.3.1",
  "spacy>=3.8.11",    # NLP for G2P (required by mlx-audio's Kokoro)
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.metadata]
# Allow direct URL references for spaCy model in optional dependencies
allow-direct-references = true

[tool.hatch.build.targets.wheel]
packages = ["models", "utils", "core"]
py-modules = ["server", "download_model"]

[tool.uv]
# Ensure dependencies resolve correctly on all target platforms
# This prevents lockfile from containing platform-specific packages that only work on one arch
environments = [
  "sys_platform == 'linux' and platform_machine == 'x86_64'",
  "sys_platform == 'linux' and platform_machine == 'aarch64'",
  "sys_platform == 'darwin' and platform_machine == 'arm64'",
  "sys_platform == 'darwin' and platform_machine == 'x86_64'", # Intel Mac
  "sys_platform == 'win32' and platform_machine == 'AMD64'",
]

# PyTorch variant selection:
# The LlamaFarm CLI automatically detects your hardware (CUDA, Metal, ROCm, or CPU)
# and sets UV_EXTRA_INDEX_URL appropriately to install the optimal PyTorch build.
#
# Note: UV_EXTRA_INDEX_URL is only set for the universal-runtime service, not for
# server/rag/config/common. This prevents dependency resolution issues where the
# PyTorch index contains incomplete/incompatible versions of common packages.
#
# Manual override options:
#   UV_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cpu   # Force CPU-only
#   UV_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu121 # Force CUDA 12.1
#   UV_EXTRA_INDEX_URL=https://download.pytorch.org/whl/rocm6.0 # Force ROCm 6.0
#
# Hardware detection priority:
#   1. CUDA (NVIDIA GPUs) - uses default PyPI with GPU support (~3.4GB)
#   2. Metal (Apple Silicon) - uses default PyPI with Metal support
#   3. ROCm (AMD GPUs) - uses ROCm index
#   4. CPU (fallback) - uses CPU-only index (~100MB)
index-url = "https://pypi.org/simple"

# Override chatterbox-tts strict dependency pins - it works fine with newer versions
override-dependencies = [
  "pillow>=10.0.0",     # Override surya-ocr's pillow<11 cap; >=10.0.0 for PyTorch CPU index compat
  "numpy>=1.24.0,<2.4", # numba (via librosa) requires numpy<2.4
  # PyTorch: Intel Mac only has wheels up to 2.2.2, other platforms can use latest
  # Note: torch >=2.6.0 required to fix CVE (torch.load RCE). Intel Mac is excluded from
  # security override since PyTorch dropped Intel Mac support after 2.2.2.
  "torch>=2.6.0; sys_platform != 'darwin' or platform_machine != 'x86_64'",
  "torchaudio>=2.6.0; sys_platform != 'darwin' or platform_machine != 'x86_64'",
  "transformers>=4.35.0",                                                                 # chatterbox pins ==4.46.3
]

[tool.uv.sources]
llamafarm-common = { path = "../../common", editable = true }
llamafarm-llama = { path = "../../packages/llamafarm-llama", editable = true }

# Ruff configuration is shared across all Python components
# See ../../ruff.toml for the actual configuration

[tool.ruff]
# Import shared configuration
extend = "../../ruff.toml"
