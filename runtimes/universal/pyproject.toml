[project]
name = "universal-runtime"
version = "1.0.0"
description = "OpenAI-compatible API for HuggingFace models"
readme = "README.md"
requires-python = ">=3.11,<3.14"
dependencies = [
  "transformers>=4.35.0",
  "diffusers>=0.25.0",
  "accelerate>=0.25.0",
  "fastapi>=0.104.0",
  "uvicorn[standard]>=0.24.0",
  "pydantic>=2.0.0",
  "pillow>=10.0.0",
  "python-multipart",
  "einops>=0.8.1",
  "openai>=1.0.0",
  "llamafarm-common",
  "llamafarm-llama",
  "structlog>=24.0.0",
  "psutil>=5.9.0",
  "pyyaml>=6.0",
  "gguf>=0.17.1",
  # hf-transfer is now provided by llamafarm-common
  "pymupdf>=1.26.6", # Required for PDF file uploads
  # Note: easyocr moved to optional [ocr-easyocr] extra to avoid forcing torch install
  # Note: scikit-learn is in optional [anomaly] extra (installed via nx sync)
  "protobuf>=6.33.1",
  "sentencepiece>=0.2.1",
]

[dependency-groups]
dev = [
  "ruff>=0.14.3",
  "pytest>=7.0.0",
  "pytest-asyncio>=0.21.0",
  "pytest-cov>=4.0.0",
  "pytest-timeout>=2.1.0",
  "httpx>=0.24.0",
  "numpy>=1.24.0",
  # For testing: torch will use CPU wheels via UV_EXTRA_INDEX_URL in CI
  "torch>=2.0.0",
]

[project.optional-dependencies]
# Hardware-accelerated packages installed separately via CLI based on detected hardware
# These are not installed during 'uv sync' to avoid compilation dependencies
# Note: torch is now in dev group for testing
gpu = ["torch>=2.0.0"]
# xformers is GPU-only and must be installed manually when needed:
#   uv pip install xformers
# (Removed from extras to avoid lockfile conflicts with CPU-only PyTorch index)

# OCR backends - install the one(s) you need
# Usage: uv pip install "universal-runtime[ocr-surya]"
ocr-surya = ["surya-ocr>=0.6.0"]
ocr-easyocr = ["easyocr>=1.7.0", "opencv-python-headless>=4.8.0"]
ocr-paddleocr = ["paddleocr>=2.7.0"]
ocr-tesseract = ["pytesseract>=0.3.10"]
# Install recommended OCR backends (surya + easyocr + tesseract)
ocr = [
  "surya-ocr>=0.6.0",
  "easyocr>=1.7.0",
  "opencv-python-headless>=4.8.0", # Required by easyocr, explicit to avoid version conflicts
  "pytesseract>=0.3.10",
]

# Document understanding models
document = ["transformers>=4.35.0", "sentencepiece>=0.1.99"]

# Anomaly detection
anomaly = ["scikit-learn>=1.3.0"]

# Text classification (few-shot learning with SetFit)
classification = ["setfit>=1.1.0", "sentence-transformers>=3.0.0"]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["models", "utils", "core"]
py-modules = ["server", "download_model"]

[tool.uv]
# PyTorch variant selection:
# The LlamaFarm CLI automatically detects your hardware (CUDA, Metal, ROCm, or CPU)
# and sets UV_EXTRA_INDEX_URL appropriately to install the optimal PyTorch build.
#
# Note: UV_EXTRA_INDEX_URL is only set for the universal-runtime service, not for
# server/rag/config/common. This prevents dependency resolution issues where the
# PyTorch index contains incomplete/incompatible versions of common packages.
#
# Manual override options:
#   UV_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cpu   # Force CPU-only
#   UV_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu121 # Force CUDA 12.1
#   UV_EXTRA_INDEX_URL=https://download.pytorch.org/whl/rocm6.0 # Force ROCm 6.0
#
# Hardware detection priority:
#   1. CUDA (NVIDIA GPUs) - uses default PyPI with GPU support (~3.4GB)
#   2. Metal (Apple Silicon) - uses default PyPI with Metal support
#   3. ROCm (AMD GPUs) - uses ROCm index
#   4. CPU (fallback) - uses CPU-only index (~100MB)
index-url = "https://pypi.org/simple"

[tool.uv.sources]
llamafarm-common = { path = "../../common", editable = true }
llamafarm-llama = { path = "../../packages/llamafarm-llama", editable = true }

# Ruff configuration is shared across all Python components
# See ../../ruff.toml for the actual configuration

[tool.ruff]
# Import shared configuration
extend = "../../ruff.toml"
