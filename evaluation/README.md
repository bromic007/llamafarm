# Evaluation (Work in Progress)

This directory is a placeholder for the future LlamaFarm model evaluation suite (benchmarks, metrics, reporting). The files currently describe target architecture but are not wired into the CLI or server yet.

## Current Status
- No automated evaluation harness is shipped with the project.
- Documentation under this folder is aspirational and will be replaced when the feature is implemented.

## What To Do Today
- Track roadmap items/issues tagged with `evaluation` for progress.
- Use your preferred external evaluation tools (e.g., llm-comparison frameworks, manual benchmarks) until official support lands.

## Contributing
Interested in helping build the evaluation stack? Open a discussion or issue to coordinate with maintainers before investing significant effort.
